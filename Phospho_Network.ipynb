{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational phospho-proteomic network inference pipeline\n",
    "####  by Matt Macgilvary\n",
    "\n",
    "###### This pipeline turns a list of S. cerevesiae phospho-peptides that exhibit stress responsive abundance changes, as measured by mass spectrometry, into a hierarchical signaling network, connecting upstream kinases and phosphatases to their downstream targets. Our computational pipeline is based on the premise that kinases and phosphatases recognize target substrates through specific amino acid sequences at the phosphorylated residue, called phosphorylation motifs. This pipeline groups phospho-peptides with similar abundance changes and the same phosphorylation motif into modules. Modules are partitioned into smaller groups, called submodules, based on differences in phospho- peptide abundance in mutant strain(s) (sources). Candidate submodule regulators, called shared interactors, are identified through enrichment analysis using a protein interaction network in yeast (Chasman et al., 2014). Shared interactor-submodule pairs serve as inputs for a previously developed Integer Programming (IP) approach that connects the sources to their downstream target submodules (Chasman et al., 2014).\n",
    "\n",
    "###### Please see our bioRxiv preprint for additional information:\n",
    "    Network inference reveals novel connections in pathways regulating growth and defense in the yeast salt response.   Matthew E. MacGilvray+, Evgenia Shishkova+, Deborah Chasman, Michael Place, Anthony Gitter, Joshua J. Coon, Audrey P. Gasch. bioRxiv 2017. doi:10.1101/176230\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "### The user should define differentially changing phospho-peptides in the \"WT\" or \"Parent\" strain using their own criteria (eg; fold-change, p-value, etc.), followed by grouping/clustering phospho-peptides based on similar directionality of abundance change.\n",
    "\n",
    "\n",
    "# Identify motifs by Calling Motifx.py\n",
    "    \n",
    "    This automates submitting jobs to the Motif-x Website (http://motif-x.med.harvard.edu/)\n",
    "\n",
    "### Expected Input  : A single Plain text file listing excel files to process, one excel file name per line.\n",
    "\n",
    "    text file:\n",
    "    data_sheet1.xlsx\n",
    "    data_sheet2.xlsx\n",
    "\n",
    "\tThe Excel file format:\n",
    "\tPpep\tGroup\tLocalized_Sequence\tMotif_X_Input_Peptide\n",
    "\tYGL076C_T8_S11\tInduced\tAAEKILtPEsQLKK\tAAEKILT*PES*QLKK\n",
    "\n",
    "\tColumn order is unimportant, column names must match above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: motifx_sample.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [13:36<00:00, 271.93s/it]\n"
     ]
    }
   ],
   "source": [
    "%run -i 'Motifx.py' -f 'inputfiles' -u 'reference/orf_trans_all.20150113.fasta'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep input for  Identify Modules and Submodules \n",
    "\n",
    " Look in your directory for a file called *-Motifx-results.txt,  This will be modified for use as input in the next step.\n",
    " The input file, which is in .csv format, must use the following format:\n",
    "\n",
    "### Column headers\n",
    "    Ppep, Cluster, Motif, Peptide, FirstMutantNamePhenotype, SecondMutantNamePhenotype, ThirdMutantNamePhenotype\n",
    "### Ppep = phospho-peptide, YORF followed by the phosphorylated residue(s). \n",
    "    Examples - YLR113W_S115, YLR113W_S115_T179\n",
    "### Cluster - either 'Induced' or 'Repressed'\n",
    "### Motif - Identified by Motif-X\n",
    "### Peptide - 13 aa long phospho-peptide returned by Motif-X. The middle residue is the phosphorylated amino acid.\n",
    "### Example Phenotype mutants, these are the last 3 columns and are gene names.\n",
    "\n",
    " FirstMutantNamePhenotype -  'hog1', the name of a gene for which we interogated a deletion strain by phospho-proteomics\n",
    " SecondMutantNamePhenotype - 'pde2' , the name of a gene for which we interogated a deletion strain by phospho-proteomics\n",
    " ThirdMutantNamePhenotype - 'cdc14' , the name of a gene for which we interogated a deletion strain by phospho-proteomics\n",
    "\n",
    "### Example  input file generated from the Motifx output:\n",
    "\n",
    "Ppep,Cluster,Motif,Peptide,hog1,pde2,cdc14\n",
    "YLR319C_T169,Induced,......TP.....,EGTREGTPLSSRK,Induced_Defective,,Repressed_Amplified\n",
    "YJL070C_T195,Induced,......TP.....,TGTGAATPHRHGY,Induced_Defective,,Repressed_Amplified\n",
    "YAL035W_T390,Induced,......TP.....,AATPAATPTPSSA,Induced_Defective,,Repressed_Amplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This can be used to generate your file automatically.\n",
    "# Replace these names w/ your mutants of interest\n",
    "# replace output.write gene names w/ your gene names of interest\n",
    "mutant1 = 'Induced_Defective'\n",
    "mutant2 = ''\n",
    "mutant3 = 'Repressed_Amplified'\n",
    "\n",
    "with open('idModules.csv', 'w') as output:       # open identify_modules_and_submodules_inputfile for writing\n",
    "    output.write('Ppep,Cluster,Motif,Peptide,hog1,pde2,cdc14\\n')        # replace gene names here\n",
    "    with open('motifx_sample-Motifx-results.txt') as f:      \n",
    "        for line in f:\n",
    "            row = line.rstrip().split(',')\n",
    "            row.append(row.pop(1))\n",
    "            row.append(mutant1)\n",
    "            row.append(mutant2)\n",
    "            row.append(mutant3)\n",
    "            output.write(','.join(row) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run  Identify_Modules_and_Submodules step.\n",
    "\n",
    " This script identifies co-regulated groups of phospho-peptides using the following approach:\n",
    " \n",
    "  1) First, the script identifies 'modules', which are groups of phospho-peptides that exhibit the same directionality in stress-dependent abundance change (ie, increased 'Induced', or decreased 'Repressed') and the same motif.\n",
    " The module nomenclature is as follows: Induced/Repressed- motif (ex: Induced..RK.s....).\n",
    " \n",
    "  2) Next, the script partitions modules into 'submodules' based on their phospho-peptide constituents dependency on a protein(s) for stress-dependent abundance changes (ie, phospho-peptides that exhibit increased 'amplified' or decreased 'defective' abundance in a deletion strain compared to the 'WT' or  'Parental' type strain). These phenotypes are user defined. If two or more mutant phenotypes are recorded for a phospho-peptide then it's placed into two separate subModules (one for each mutant phenotype). If there was not a mutant phenotype at a user defined threshold then the phenotype is 'No-Phenotype'\n",
    " \n",
    " The submodule nomenclature is as follows: module name-mutant phenotype/No-Phenotype (ex: Induced..RK.s....Mutant_Defective).\n",
    "\n",
    " Possible submodule phenotypes: Induced-Defective, Induced-Amplified, Repressed-Defective, Repressed-Amplified, Induced-No-Phenotype, Repressed-No-Phenotype \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import required python libraries\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from scipy.stats import hypergeom\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "Data=pd.read_csv('idModules.csv') # Define path to input file\n",
    "\n",
    "def Slicedataframe():\n",
    "    '''Define a function that slices the input dataframe into independent dataframes based on the Cluster names. Next, slice these dataframes based on the presence of the same motif, generating 'modules' '''  \n",
    "    ClusterLST=Data['Cluster'].unique().tolist()                            # generate a list of unique Cluster names (ie, 'Induced' and 'Repressed')\n",
    "    lst=[]                                                                 \n",
    "    DF=Data.copy()                                                         \n",
    "    for cluster in ClusterLST:                                              # Select the first 'cluster' on the list \n",
    "        DF2=DF.loc[DF['Cluster']== cluster]                                 # Create a new dataframe by selecting only those rows that contain the selected 'cluster' in the 'Cluster' column \n",
    "        MotifLST=DF2['Motif'].unique().tolist()                             # From the newly created dataframe, place each instance of a unique motif into a list\n",
    "        cleanedMotifLST = [x for x in MotifLST if str(x) != 'nan']          # drop the string 'nan' from the list. 'nan' occurs for Ppeps that did not have an identified Motif from Motif-X. \n",
    "        for motif in cleanedMotifLST:                                       # Select a motif in the list\n",
    "            DF3=DF2.loc[DF['Motif']== motif]                                # Filter the dataframe, selecting only those rows that contain 'motif' in the Motif column\n",
    "            DF3['freq'] = DF3.groupby('Motif')['Motif'].transform('count')  # Produce a new column, called 'freq' that contains the number of rows, and thus phospho-peptides, that contain a given motif.\n",
    "            lst.append(DF3) \n",
    "        \n",
    "    return lst\n",
    "\n",
    "SlicedDF_lst=Slicedataframe()\n",
    "\n",
    "def ConcatenateDFs():\n",
    "    ''' Define a function that appends the dataframes in the SlicedDF_list together. '''\n",
    "    EmptyDF = pd.DataFrame()                                                # create an empty dataframe\n",
    "    for df in SlicedDF_lst:                                                \n",
    "        df=df.copy() \n",
    "        EmptyDF=EmptyDF.append(df)                                          # append to the empty DF the dataframe selected and overwrite the empty dataframe\n",
    "    return EmptyDF\n",
    "\n",
    "Final_DF=ConcatenateDFs()\n",
    "FinalDFV2=Final_DF.fillna(0)                                                #  fill any NaN values with '0'\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "def Module_Motif_NoMutantPhenotypeExists(df):\n",
    "    ''' Define a function that assigns no-phenotype submodules'''\n",
    "    if (df['hog1']==0) & (df['pde2']==0) & (df['cdc14']==0):\n",
    "        return 'No_Phenotype_Exists'\n",
    "    \n",
    "FinalDFV2['Phenotype']=FinalDFV2.apply(Module_Motif_NoMutantPhenotypeExists, axis=1) \n",
    "FinalDFV2=FinalDFV2.loc[FinalDFV2['Phenotype']=='No_Phenotype_Exists']        # Select all rows for which \"No_Phenotype_Exists\" in the 'Phenotype' column.\n",
    "FinalDFV2['subModule']=FinalDFV2.Cluster.map(str) + \"_\" + FinalDFV2.Motif + \"_\" + FinalDFV2.Phenotype                                   # create a new column, called submodule, that contains the concatenated strings in the 'Cluster', 'Motif', and 'Phenotype' columns.\n",
    "\n",
    "# CHANGE GENE NAMES HERE\n",
    "FinalDF=Final_DF.dropna(subset = ['hog1', 'pde2', 'cdc14'], how='all')        # Remove rows that have NaN in all 3 columns representing mutant phenotpes. This steps removes theNo-phenotype submodules which were creat                                                                               # ed above. \n",
    "FinalDF=FinalDF.fillna(0)                                                     # fill any NaN that remain with '0'\n",
    "lstCols=['hog1', 'pde2', 'cdc14']                                             # make a list that contains the column headers for the 3 mutants. \n",
    "\n",
    "\n",
    "\n",
    "def DefineMutantContribution(row):\n",
    "    ''' Define a function that identifies for each phospho-peptide if it has a phenotype in more than one mutant strain'''\n",
    "    dictData={} \n",
    "    for colname in lstCols:    \n",
    "        if not row[colname]==0:                                                # if value is not equal to zero, there is a mutant phenotype (ex; Induced_defective)\n",
    "            dictData[colname]=row[colname]  \n",
    "    if len(dictData.keys())==0: return 0  \n",
    "    else:\n",
    "        return \":\".join(dictData.keys())\n",
    "    \n",
    "FinalDF['Contribution']=FinalDF.apply(lambda x: DefineMutantContribution(x), axis=1) \n",
    "\n",
    "\n",
    "\n",
    "def DefinePhenotypeFromMutants(row):\n",
    "    ''' Define a function that captures the mutant phenotype for Ppeps with multiple phenotypes and places it within a column'''\n",
    "    dictData={}  \n",
    "    for colname in lstCols:   \n",
    "        if not row[colname]==0:\n",
    "            dictData[colname]=row[colname] \n",
    "    if len(dictData.keys())==0: return 0 \n",
    "    else:\n",
    "        return \":\".join(dictData.values()) \n",
    "    \n",
    "FinalDF['Phenotype']=FinalDF.apply(lambda x: DefinePhenotypeFromMutants(x), axis=1)\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' Determine all Ppeps that have 2 or more mutant phenotypes (Pde2/Hog1/Cdc14 phenotypes), then the script produces a new column with individual subModule names for \n",
    "the hog1 phenotype''' \n",
    "\n",
    "FinalDF_multiplePhenotypes=FinalDF[FinalDF['Contribution'].str.contains(\":\")]     # Select 'contribution column rows that contain \":\", which means the Ppep has two mutant phenotypes since this is a separator between gene names\n",
    "FinalDF_multiplePhenotypes_hog1=FinalDF_multiplePhenotypes[FinalDF_multiplePhenotypes['Contribution'].str.contains(\"hog1\")] \n",
    "FinalDF_multiplePhenotypes_hog1['Hog1']='hog1' \n",
    "FinalDF_multiplePhenotypes_hog1['subModule']=FinalDF_multiplePhenotypes_hog1.Cluster.map(str) + \"_\" + FinalDF_multiplePhenotypes_hog1.Motif + \"_\" + FinalDF_multiplePhenotypes_hog1.Hog1 + \"_\" + FinalDF_multiplePhenotypes_hog1.hog1\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' Define all Ppeps that have 2 or more mutant phenotypes (Pde2/Hog1/Cdc14 phenotypes), then the script produces a new column with individual subModule names for \n",
    "the pde2 phenotype''' \n",
    "FinalDF_multiplePhenotypes_pde2=FinalDF_multiplePhenotypes[FinalDF_multiplePhenotypes['Contribution'].str.contains(\"pde2\")]\n",
    "FinalDF_multiplePhenotypes_pde2['Pde2']='pde2'\n",
    "FinalDF_multiplePhenotypes_pde2['subModule']=FinalDF_multiplePhenotypes_pde2.Cluster.map(str) + \"_\" + FinalDF_multiplePhenotypes_pde2.Motif + \"_\" + FinalDF_multiplePhenotypes_pde2.Pde2 + \"_\" + FinalDF_multiplePhenotypes_pde2.pde2\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' Define all Ppeps that have 2 or more mutant phenotypes (Pde2/Hog1/Cdc14 phenotypes), then the script produces a new column with individual subModule names for \n",
    "the cdc14 phenotype'''\n",
    "\n",
    "FinalDF_multiplePhenotypes_cdc14=FinalDF_multiplePhenotypes[FinalDF_multiplePhenotypes['Contribution'].str.contains(\"cdc14\")]\n",
    "FinalDF_multiplePhenotypes_cdc14['Cdc14']='cdc14'\n",
    "FinalDF_multiplePhenotypes_cdc14['subModule']=FinalDF_multiplePhenotypes_cdc14.Cluster.map(str) + \"_\" + FinalDF_multiplePhenotypes_cdc14.Motif + \"_\" + FinalDF_multiplePhenotypes_cdc14.Cdc14 + \"_\" + FinalDF_multiplePhenotypes_cdc14.cdc14\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "'''This section of code appends the above mutant dataframes together (ie, FinalDF_multiplePhenotypes_cdc14, etc.) (contained \":\"). The result is Ppeps with phenotypes in more than one strain are listed on multiple lines rather than a single line'''\n",
    "\n",
    "FinalDF_mutants=FinalDF_multiplePhenotypes_cdc14.append(FinalDF_multiplePhenotypes_hog1) \n",
    "FinalDF_mutants_Final=FinalDF_mutants.append(FinalDF_multiplePhenotypes_pde2)\n",
    "\n",
    "FinalDF_mutants_Final=FinalDF_mutants_Final[['Ppep','Cluster','Motif','Peptide','hog1','pde2','cdc14','freq','Contribution','Phenotype','subModule']] # Only retain these columns \n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Drop from the original dataframe rows containing Ppeps with multiple mutant phenotypes. \n",
    "FinalDF_minus_multiPhenotypePpeps=FinalDF[FinalDF.Contribution.str.contains(\":\")==False] # Removing all rows that contain \":\", and thus are phospho-peptides with multiple mutant phenotypes\n",
    "\n",
    "\n",
    "# Generate the final submodule names\n",
    "FinalDF_minus_multiPhenotypePpeps['subModule']=FinalDF_minus_multiPhenotypePpeps.Cluster.map(str) + \"_\" + FinalDF_minus_multiPhenotypePpeps.Motif + \"_\" + FinalDF_minus_multiPhenotypePpeps.Contribution + \"_\" + FinalDF_minus_multiPhenotypePpeps.Phenotype\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Ppeps_with_PhenotypesDF=FinalDF_minus_multiPhenotypePpeps.append(FinalDF_mutants_Final)  # Appending together the dataframes that originally had single mutant phenotypes, and the dataframe that started with multiple mutant Phentoypes, but now contains single listings for each Ppep-mutant phenotype\n",
    "\n",
    "\n",
    "\n",
    "# Remove any submodule that only has a single Ppep constituent, since by default a submodule must contain 2 Ppeps. \n",
    "Ppeps_with_PhenotypesDF_subModules=Ppeps_with_PhenotypesDF[Ppeps_with_PhenotypesDF.duplicated(['subModule'], keep='last') | Ppeps_with_PhenotypesDF.duplicated(['subModule'])]  # only retain duplicates, get rid of single entries \n",
    "\n",
    "\n",
    "\n",
    "# Append to the dataframe with phenotype subModules, all No-Phenotype submodules\n",
    "Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF=Ppeps_with_PhenotypesDF_subModules.append(FinalDFV2) # append to dataframe\n",
    "Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF=Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF[['Ppep', 'Cluster', 'Motif', 'Peptide', 'hog1', 'pde2', 'cdc14', 'freq', 'Contribution', 'Phenotype', 'subModule']]\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' Create a column with the 'Module' name '''\n",
    "Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF['Module']=Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF.Cluster.map(str) + \"_\" + Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF.Motif \n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# define a function that will write out a dataframe as a tab separated file\n",
    "def Dataframe_to_Tsv (dataframe, NewFileName):\n",
    "    path =\"/home/mplace/projects/forMatt/Phospho_Network/\"\n",
    "    dataframe.to_csv (path+NewFileName,sep='\\t')\n",
    "\n",
    "Dataframe_to_Tsv(Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF, 'Modules_pPep.csv') \n",
    "# The above file contains all modules and subModules with and without mutant phenotypes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Shared Interactors \n",
    "\n",
    "''' This script identifies proteins enriched for interactions with Submodule constituent proteins, based on known interactions in the background network. We call these proteins 'Shared Interactors'. The background network is a protein\n",
    "interaction network curated in yeast under mostly nutrient replete conditions that contains 4638 proteins and ~ 25,000 interactions, including directed (ex; kinase-substrate), and \n",
    "non-directed. \n",
    "\n",
    "Proteins enriched for interactions with Submodule proteins at a 5% FDR, determined by a hypergeometric test and BH correction, are considered shared interactors.\n",
    "\n",
    "Shared Interactors represent numerous functional classes, including kinases and phosphatases. Kinase and phosphatase shared interactors represent potential Submodule regulators.\n",
    " \n",
    "HyperG function:\n",
    "distrib=hypergeom(N,M,n)\n",
    "distrib.pmf(m)\n",
    "\n",
    "N - population size (4638 unique proteins in Background network file - phospho_v4_bgnet_siflike_withdirections_Matt_Modified.csv)\n",
    "\n",
    "M - total number of successes  (# of interactions for a given protein. ie. Protein A has 200 known interactions in the background network).\n",
    "\n",
    "n - the number of trials (also called sample size) -  ie. (Number of proteins that reside within a submdoule)\n",
    "\n",
    "m - the number of successes - for example: Protein A, a shared interactor, has 35 interactions with proteins in Submodule B. \n",
    "\n",
    "input file : \n",
    "Submodule,ORF\n",
    "Induced_......TP....._cdc14_Repressed_Amplified,YLR319C\n",
    "Induced_......TP....._cdc14_Repressed_Amplified,YJL070C\n",
    "\n",
    "#### NOTE: ORF names have need to have the '-' removed,  YER074W-A becomes YER074A.\n",
    "\n",
    "The other input files are provided.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# create the input file based on the output of the previous step.\n",
    "with open('Submodule_constituents.csv', 'w') as out:\n",
    "    out.write('Submodule,ORF\\n')\n",
    "    with open('Modules_pPep.csv') as f:\n",
    "        f.readline()                            # skip header\n",
    "        for line in f:\n",
    "            data = line.rstrip().split('\\t')\n",
    "            name = data[1].split('_')[0]   \n",
    "            name = re.sub('-', '', name)\n",
    "            row = data[-2] + ',' + name + '\\n'\n",
    "            out.write(row)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Submodule_DF   = pd.read_csv(current_dir + '/Submodule_constituents.csv')                                                                       # File that contains Submodule names and their protein constituents\n",
    "BgNet          = pd.read_csv(current_dir + '/SI_Identification_Input_Files/Background_Network.csv')                                                                                   # Background network of protein interactions\n",
    "Num_Prot_Inter = pd.read_csv(current_dir + '/SI_Identification_Input_Files/Number_Interactions_Each_Protein.csv')                                              # Number of protein interactions for each protein in the background network\n",
    "Annotation_DF  = pd.read_csv(current_dir + '/SI_Identification_Input_Files/Annotation.csv')                                                   # Yeast protein annotation file\n",
    " \n",
    "Submodule_List=Submodule_DF['Submodule'].unique().tolist()                                                                                                  # Send the Submodules to a list, but filter out duplicates, which there will be many, since the Submodules will have been found in many proteins.\n",
    "\n",
    "dicOrfs={}\n",
    "for Submodule in Submodule_List:                                                                                                                            # Key (Submodule), Value (Yeast ORFs that are Submodule constituents). Filter ORFs found twice to single occurence (important for enrichment analysis)\n",
    "    dicOrfs[Submodule]=(Submodule_DF.loc[Submodule_DF['Submodule'] == Submodule])['ORF'].unique().tolist()\n",
    "        \n",
    "\n",
    "dicOrfsCounts={}  \n",
    "for k,v in dicOrfs.items():  \n",
    "    if k not in dicOrfsCounts:  \n",
    "        value=len(v)            \n",
    "        dicOrfsCounts[k]=value\n",
    "        \n",
    "df_Submodule_Size=pd.DataFrame(list(dicOrfsCounts.items()),                                                                                                  # convert dict to dataframe.\n",
    "                      columns=['Submodule','n'])\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#def DF_to_TSV(dataframe, NewFileName): \n",
    "#    ''' Define a function that writes out a dataframe as TSV'''\n",
    "#    path ='/Users/mmacgilvray18/Desktop/' \n",
    "#    dataframe.to_csv (path+NewFileName,sep='\\t')\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def SliceDataframe():\n",
    "    ''' For each Submodule identify all proteins that interact with the Submodule proteins in the backgroudn network '''\n",
    "    lst = []\n",
    "    for key in dicOrfs.keys():                                                                                                                             #Select the key, which is a Submodule, from the dict\n",
    "        CurrentDF=BgNet.copy() \n",
    "        x=CurrentDF[CurrentDF['Protein1'].isin(dicOrfs[key])].rename(columns={'Protein1':'Submodule_Containing_Proteins', 'Protein2':'Possible_Shared_Interactors'})                              #Create a new dataframe that is a slice of the salt background network, and only contains proteins that were passed in \"dicOrfs[key]\". At the same time, rename the columns                                \n",
    "        x['Submodule']=key \n",
    "        lst.append(x)\n",
    "        \n",
    "    return lst\n",
    "\n",
    "Sliced_dataframe_list= SliceDataframe()\n",
    "    \n",
    "      \n",
    "def Add_n():    \n",
    "    ''' Function adds 'n', the number of proteins in the Submodule, to each dataframe'''\n",
    "    lst= []\n",
    "    for df in Sliced_dataframe_list:\n",
    "        NewDF=df.merge(df_Submodule_Size)\n",
    "        lst.append(NewDF)\n",
    "        \n",
    "    return lst\n",
    "\n",
    "Sliced_dataframe_list= Add_n()\n",
    "\n",
    "def Identify_Shared_Interactors():\n",
    "    ''' Function identifies proteins that interact with at least 2 protein constituents of each submodule'''\n",
    "    \n",
    "    lst=[] \n",
    "    for df in Sliced_dataframe_list: \n",
    "        NewDF=df.copy()\n",
    "        NewDF2=NewDF[NewDF.duplicated(['Possible_Shared_Interactors'], keep = 'last')| NewDF.duplicated(['Possible_Shared_Interactors'])]                  # Only retain proteins that interact with at least 2 submodule protein constituents\n",
    "        x=NewDF2.sort_values(by='Possible_Shared_Interactors', ascending=True) \n",
    "        lst.append(x)\n",
    "       \n",
    "    return lst\n",
    "\n",
    "Shared_Interactors_lst=Identify_Shared_Interactors()\n",
    "\n",
    "\n",
    "def AppendDFs_that_Contain_AllSharedInteractors_and_their_targets():\n",
    "    ''' Function appends all submodules and their shared interactors together into a single file'''\n",
    "    EmptyDF = pd.DataFrame() \n",
    "    for df in Shared_Interactors_lst:  \n",
    "        df=df.copy() \n",
    "        EmptyDF=EmptyDF.append(df)\n",
    "    return EmptyDF\n",
    "\n",
    "SI_andTargets=AppendDFs_that_Contain_AllSharedInteractors_and_their_targets()\n",
    "\n",
    "\n",
    "SI_andTargets_FINAL=pd.merge(left=SI_andTargets, right=Annotation_DF, how='left',\n",
    "                              left_on='Possible_Shared_Interactors', right_on='systematic_name_dash_removed')                                               # complete a merge so I can get the dashes back in the names, which are not included in the background network\n",
    "del SI_andTargets_FINAL['Possible_Shared_Interactors']                                                                                                      # drop because  lacks the dashes which are needed for the correct naming convention\n",
    "del SI_andTargets_FINAL['systematic_name_dash_removed']                                                                                                     # drop because carried over from the merge\n",
    "del SI_andTargets_FINAL['Directed']\n",
    "\n",
    "SI_andTargets_FINAL.columns = ['Submodule_Containing_Proteins', 'Interaction', 'Submodule', 'n','Possible_Shared_Interactors']                        # rename columns\n",
    "\n",
    "\n",
    "myDF = pd.DataFrame(SI_andTargets_FINAL)\n",
    "# output name for shared interactors\n",
    "filename = 'SI_Identification_SubmoduleS__SIs_and_Targets_FDR.csv'\n",
    "myDF.to_csv(filename, index=False, encoding='utf-8' )              # All interactions between SIs and their submodule constituent proteins. No enrichment at this step.\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' Preparing dataframe for Hypergeometric test'''\n",
    "\n",
    "def Add_N_and_m():\n",
    "    ''' Function adds 'N' and calculates 'm' values, which are inputs for the hypergeometric test, to the datframe'''\n",
    "    lst=[]\n",
    "    for df in Shared_Interactors_lst:\n",
    "        NewDF=df.copy()\n",
    "        NewDF['N'] = 4638                                                                                                                                   # of proteins in the background network\n",
    "        NewDF['m'] = NewDF.groupby('Possible_Shared_Interactors')['Possible_Shared_Interactors'].transform('count')\n",
    "        lst.append(NewDF)\n",
    "    \n",
    "    return lst\n",
    "\n",
    "Dataframes_list_with_n_N_m=Add_N_and_m()\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def Drop_dups():\n",
    "    ''' For each dataframe, which contains a single submodule, it's protein constituents, and shared interactors, drop duplicate entries for identified SI proteins\n",
    "    . This leaves a single entry for each shared interactor protein. '''\n",
    "    lst=[]\n",
    "    for df in Dataframes_list_with_n_N_m:\n",
    "        NewDF=df.copy()\n",
    "        Final_DF=NewDF.drop_duplicates('Possible_Shared_Interactors')\n",
    "        Final_DF=Final_DF.rename(columns={'Possible_Shared_Interactors':'Shared_Interactor'})\n",
    "        lst.append(Final_DF)\n",
    "        \n",
    "    return lst\n",
    "\n",
    "Drop_Dups_lst=Drop_dups()\n",
    "\n",
    "\n",
    "def Return_M():\n",
    "    ''' Function identifies 'M' (the total number of interactions for each Shared Interactor protein in the background network) and adds that number\n",
    "    to the dataframe'''\n",
    "    lst=[]\n",
    "    for df in Drop_Dups_lst:\n",
    "        NewDF=df.copy()\n",
    "        NewDF2=df.copy()\n",
    "        NewDF_lst=NewDF['Shared_Interactor'].tolist()                                                                                                            # place all proteins in the 'Shared_Interactor' column in a list \n",
    "        Shared_Interactors=Num_Prot_Inter[Num_Prot_Inter['Protein'].isin(NewDF_lst)].rename(columns={'Protein':'Shared_Interactor', 'Total':'M'})\n",
    "        Shared_Interactor_merge=Shared_Interactors.merge(NewDF2, on='Shared_Interactor')\n",
    "        Shared_Interactor_merge=Shared_Interactor_merge.sort_values(by='Shared_Interactor', ascending=True)\n",
    "        lst.append(Shared_Interactor_merge)\n",
    "        \n",
    "    return lst\n",
    "\n",
    "Return_M_lst=Return_M()\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------\n",
    "def hyper(N,M,n,m): \n",
    "    ''' Function defines the parameters for a hypergeometric test that returns a p-value representing the chances of identifying >= x, where x is the number of successes '''  \n",
    "    frozendist=hypergeom(N,M,n)\n",
    "    ms=np.arange(m, min(n+1, M+1))\n",
    "    rv=0;\n",
    "    for single_m in ms: rv=rv+frozendist.pmf(single_m)\n",
    "    return rv\n",
    "\n",
    "def run_hyper():\n",
    "    ''' Function calls the hypergeometric function above  on each shared interactor for each submodule'''\n",
    "    lst=[]\n",
    "    for df in Return_M_lst:\n",
    "        if not df.empty:\n",
    "            NewDF=df.copy()\n",
    "            NewDF['p-value'] = NewDF.apply(lambda row: hyper(row['N'], row['M'], row['n'], row['m']), axis=1)\n",
    "            lst.append(NewDF)\n",
    "        \n",
    "    return lst \n",
    "\n",
    "run_hyper_lst=run_hyper()\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def AppendDFs():\n",
    "    ''' Append DFs for each submodule and it's SIs together into a single DF'''   \n",
    "    EmptyDF = pd.DataFrame() #\n",
    "    for df in run_hyper_lst: \n",
    "        df=df.copy() \n",
    "        EmptyDF=EmptyDF.append(df)\n",
    "    return EmptyDF\n",
    "\n",
    "Final=AppendDFs()\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' Prepping for Benjamini Hochberg procedure. Below code is ranking p-values from 1 to n based on lowest to highest p-value score'''\n",
    "\n",
    "Final=Final.sort_values(by=['p-value'],ascending=[True])                                                                                              # Sort p-values from lowest to highest\n",
    "Final_resetIndex=Final.reset_index()                                                                                                        # Reset the index after the sort\n",
    "Final_resetIndex.index +=1                                                                                                                  # start numbering at 1 for index\n",
    "       \n",
    "NewDF=Final_resetIndex\n",
    "NewDF_Allp_values=Final_resetIndex\n",
    "NewDF=NewDF[['p-value']]                                                                                                                    # select only the p-value column of the dataframe \n",
    "NewDF_dropdups=NewDF.drop_duplicates('p-value')                                                                                             # drop duplicate p-values\n",
    "NewDF_dropdups=NewDF_dropdups.reset_index()                                                                                                 # reset the index\n",
    "NewDF_dropdups.index +=1                                                                                                                    # start numbering at 1 for index\n",
    "NewDF_dropdups['Rank(i)'] = NewDF_dropdups.index                                                                                            # #Add a rank column that will be filled with index values. \n",
    "NewDF_dropdups=NewDF_dropdups.drop('index', 1)                                                                                              # Drop the additional column 'index' that is not sorted.\n",
    "NewDF_merge=NewDF_Allp_values.merge(NewDF_dropdups, on='p-value')                                                                           # create a new dataframe that is a merge of the dataframe with all p-values, and the dataframe with unique p-values and their ranks. \n",
    "NewDF_merge=NewDF_merge.drop('index',1)                                                                                                     # drop the index that was added from the merge. This leaves all p-values ordered from lowest to highest with their ranking.\n",
    "\n",
    "'''Add parameters necessary for completing Benjamini-Hochberg procedure '''\n",
    "\n",
    "NewDF=NewDF_merge\n",
    "NewDF['m_(number_of_tests)']=(len(NewDF))                                                                                                   # Add 'm (number of tests)' column \n",
    "NewDF['Q_(FDR)']=0.05                                                                                                                       # Add Q (FDR) column. This can be changed manually.\n",
    "NewDF['(i/m)Q']=((NewDF['Rank(i)']/NewDF['m_(number_of_tests)'])*NewDF['Q_(FDR)'])                                                          # add the (i/m)Q column \n",
    "NewDF['BH_significant']=NewDF.apply(lambda x: 1 if x['p-value']<x['(i/m)Q'] else 0, axis=1)                                                 # Identify which proteins are  significant. \n",
    "NewDF=pd.merge(left=NewDF, right=Annotation_DF, how='left', left_on='Shared_Interactor', right_on='systematic_name_dash_removed')           # complete a merge to recover dashed version of YORFs\n",
    "del NewDF['Shared_Interactor'] \n",
    "del NewDF['systematic_name_dash_removed']\n",
    "del NewDF['Directed']\n",
    "NewDF.columns = ['M','Submodule_Containing_Proteins', 'Interaction', 'Submodule', 'n','N','m','p-value','Rank(i)', 'm_(number_of_tests)', 'Q_(FDR)','(i/m)Q','BH_significant', 'Shared_Interactor'] # rename columns\n",
    "\n",
    "myDF = pd.DataFrame(NewDF)\n",
    "filename = 'Network_Submodule_Nodes_background_Network.csv'\n",
    "myDF.to_csv(filename, index=False, encoding='utf-8' )       # Write out final file with enriched shared interactors for each submodule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Shared Interactors Inputs Outputs\n",
    "\n",
    "The function of this script is as follows: For each SI and it's connections with submodule protein constituents, determine if the SI acts upon the submodule (that is, the Shared Interactor has at least 1 directional interaction, or ppi interaction, with a submodule protein), or if the submodule acts upon the SI (that is, all interactions between the SI and submodule proteins have the 'Reverse' designation', indicating that the submodule proteins act upon the SI).\n",
    "\n",
    "-If all of the interactions are reversed, then the script will define the relationship between the SI and the submodule as \"Output\"\n",
    "\n",
    "-If there is at least one interaction that is directed from SI towards submodule, or is a ppi, the relationship between the SI and the submodule is defined as \"Input\"\n",
    "\n",
    "This script takes an input file that contains the following:\n",
    "\n",
    "- All enriched Shared Interactors (SIs) (according to HyperG) and their connections to submodules.\n",
    "- All known protein interactions for each SI (ppi, kinase-substrate, etc)\n",
    "- Many of these interactions are directed (kinase-substrate, metabolic pathway, etc). PPI are not a directed interaction.\n",
    "\n",
    "Input: plain csv text file\n",
    "\n",
    "Csv format:\n",
    "SI_submodule,Shared_Interactor,SI_name,Motif_Containing_Proteins,submodule_Name\n",
    ",Interaction_Directionality\n",
    "\n",
    "YLR164C_Repressed_..RR.s.No_Phenotype_Exists,YLR164C,Tpk1,YDR207C,\n",
    "Repressed_..RR.s.No_Phenotype_Exists, kinase_substrate\n",
    "\n",
    "Column order is unimportant, column names must match above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import yeast_Gene_name_to_ORF as yg          # used to get standard name\n",
    "\n",
    "# create input files \n",
    "# SI_Identification_SubmoduleS__SIs_and_Targets_FDR.csv - input from ID shared interactors\n",
    "submod = dict()\n",
    "with open('SI_Identification_SubmoduleS__SIs_and_Targets_FDR.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        row = line.rstrip().split(',')\n",
    "        submod[row[0] + '_' + row[2]] = row\n",
    "\n",
    "f.close()\n",
    "\n",
    "# get network information\n",
    "with open('classify_sharedInteractors_input.csv', 'w') as out:\n",
    "    header = '%s,%s,%s,%s,%s,%s\\n' %('SI_submodule','Shared_Interactor','SI_name','Motif_Containing_Proteins','submodule_Name'\n",
    ",'Interaction_Directionality')\n",
    "    out.write(header)\n",
    "    with open('Network_Submodule_Nodes_background_Network.csv') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('M'):\n",
    "                continue\n",
    "            row = line.rstrip().split(',')\n",
    "            if int(row[12]) != 1:\n",
    "                continue\n",
    "            name = row[1] + '_' + row[3]\n",
    "            if name in submod:\n",
    "                if row[1].endswith(('A','B')):\n",
    "                    tmp = list(row[1])\n",
    "                    tmp.insert(-1,'-')\n",
    "                    row[1] = \"\".join(tmp)\n",
    "                n = re.sub('-', '', row[1])\n",
    "                ln = name + ',' + n + ',' + yg.sc_orfToGene[row[1]] + ',' + row[-1] + ',' + row[3] + ',' + row[2] + '\\n'\n",
    "                out.write(ln)\n",
    "\n",
    "Input_df=pd.read_csv('classify_sharedInteractors_input.csv')\n",
    "\n",
    "def Split_based_on_SI_submodule_Column():\n",
    "    ''' Function splits the input DF into independent DFs based on the SI-submodule column pairs. Thus, each SI and it's submodule protein interactions are\n",
    "    in independent dataframes '''\n",
    "    DF_lst =[]\n",
    "    for SI_submodule in Input_df['SI_submodule'].unique():\n",
    "        DF=Input_df.loc[Input_df['SI_submodule']==SI_submodule]\n",
    "        DF_lst.append(DF)\n",
    "    return DF_lst\n",
    "\n",
    "DF_lst=Split_based_on_SI_submodule_Column()\n",
    "\n",
    "def Count_Instances_of_Reverse_Interaction():\n",
    "    ''' Function counts, for each DF, and thus each SI-submodule pair, how many of the interactions are 'reversed', or facing from submodule TOWARDS SI. \n",
    "        It also counts the length of the dataframe, and then subtracts the the length of the dataframe from the counts. If the resultant value is 0, then all of the interactions \n",
    "        were reversed '''\n",
    "    DF_Counts_lst=[]\n",
    "    for df in DF_lst:\n",
    "        df=df.copy()\n",
    "        df['Counts']=df.Interaction_Directionality.str.contains('Reversed').sum()                                                               # Count the number of interactions that are \"Reversed\"\n",
    "        x=len(df)\n",
    "        df['Length']=x\n",
    "        df['Counts_Length']=df['Counts']-df['Length']\n",
    "        \n",
    "        DF_Counts_lst.append(df)\n",
    "    return DF_Counts_lst\n",
    "\n",
    "DF_Counts_lst=Count_Instances_of_Reverse_Interaction()\n",
    "\n",
    "def Only_Reverse_Interactions_Move_to_Outgoing_Columns():\n",
    "    '''Function assigns 'Input' and 'Output' classifications based on the 'Counts_Length' column in the dataframe. '0' values are 'outputs', all other's are 'inputs' '''\n",
    "    df_Modified_Outgoing_lst=[]\n",
    "    for df in DF_Counts_lst:\n",
    "        for value in df['Counts_Length'].unique():\n",
    "           \n",
    "            if value == 0:\n",
    "                df['Shared_Interactor_submodule_Relationship']= 'Output'\n",
    "                df_Modified_Outgoing_lst.append(df)\n",
    "            else:\n",
    "                df['Shared_Interactor_submodule_Relationship']= 'Input'\n",
    "                df_Modified_Outgoing_lst.append(df)\n",
    "           \n",
    "    return df_Modified_Outgoing_lst\n",
    "            \n",
    "df_Modified_Outgoing_lst=Only_Reverse_Interactions_Move_to_Outgoing_Columns()\n",
    "\n",
    "\n",
    "def AppendDFs(): \n",
    "    '''Function appends all dataframes back together '''\n",
    "    EmptyDF = pd.DataFrame()\n",
    "    for df in df_Modified_Outgoing_lst: \n",
    "        df=df.copy() \n",
    "        EmptyDF=EmptyDF.append(df) \n",
    "    return EmptyDF\n",
    "\n",
    "Final=AppendDFs()    \n",
    "\n",
    "Final_Keep_Columns_Needed_For_SIF=Final[['SI_submodule', 'Shared_Interactor', 'submodule_Name', 'Shared_Interactor_submodule_Relationship']]  \n",
    "Final_Keep_Columns_Needed_For_SIF=Final_Keep_Columns_Needed_For_SIF.drop_duplicates('SI_submodule')                                                                     # Dropping duplicates entries, which are created because for each SI-submodule interaction there are numerous interactions with protein constituent. Only want a single interaction, input or output, for each SI and it's submodule. \n",
    "\n",
    "# create a new dataframe and write results to file\n",
    "myDF = pd.DataFrame(Final_Keep_Columns_Needed_For_SIF)\n",
    "filename = 'SIs_submodule_Relationships_Define_ClassA_Network.csv'\n",
    "myDF.to_csv(filename, index=False, encoding='utf-8',sep='\\t') \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Fasta file\n",
    "\n",
    "Input is file from Identify_Modules_and_Submodules step, 'Modules_pPep.csv'.  This file is parsed to produce\n",
    "\n",
    "All peptide sequences should be the same length (13 amino acids).\n",
    "\n",
    "Module constituents should be used here, not submodules. \n",
    "Fasta Files for each module will be created in a dir called: FastaFiles_Modules/\n",
    "\n",
    "The output Fasta format files are named with their module designation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# open & parse file\n",
    "with open('pwm_input.csv', 'w') as out:\n",
    "    header = 'Module,Name,Sequence\\n'\n",
    "    out.write(header)\n",
    "    with open('Modules_pPep.csv','r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('\\tPpep'):\n",
    "                continue\n",
    "            dat = line.split('\\t')\n",
    "            outrow = '%s_%s,%s,%s\\n' %(dat[2],dat[3], dat[1], dat[4])\n",
    "            out.write(outrow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "Input_df=pd.read_csv('pwm_input.csv')\n",
    "\n",
    "def Split_Into_SeparateDFs():\n",
    "    ''' Function splits the input dataframe, based on the module name, into independent dataframes for each module'''\n",
    "    df_lst=[]\n",
    "    for Module in Input_df['Module'].unique():\n",
    "        DF=Input_df.loc[Input_df['Module']==Module]\n",
    "        df_lst.append(DF)\n",
    "        \n",
    "    return df_lst\n",
    "\n",
    "df_lst=Split_Into_SeparateDFs()\n",
    "#print (df_lst)\n",
    "\n",
    "if not os.path.exists('FastaFiles_Modules'):\n",
    "    os.mkdir('FastaFiles_Modules')\n",
    "\n",
    "def CreateIndividualFastaFiles():\n",
    "    '''Function creates individual fasta files for each module nd writes them out to a user defined directory'''\n",
    "    for df in df_lst:                \n",
    "        Module_lst=df[\"Module\"].tolist()    \n",
    "        for name in Module_lst:          \n",
    "        # open a new file that contains the module name. USER can Change directory here.\n",
    "            ofile= open(\"FastaFiles_Modules/\"+name+\".fasta\", \"w\") \n",
    "        \n",
    "            df_lstName=df['Name'].tolist()             # send the module names to a list\n",
    "            df_lstSeq=df['Sequence'].tolist()          # send the peptide sequences to a list \n",
    "            \n",
    "            for i in range(len(df_lstSeq)):                    \n",
    "                \n",
    "                ofile.write(\">\" + df_lstName[i] + \"\\n\" + df_lstSeq[i] + \"\\n\")                                            # create a fasta file where the peptide name will be followed by the peptide sequence, on a new line\n",
    "           \n",
    "        df_lstName=[]                                                                                                    # empty each of the lists for the next iteration\n",
    "        df_lstSeq=[]\n",
    "        Module_lst=[]\n",
    "        ofile.close           \n",
    "        \n",
    "    return \n",
    "      \n",
    "CreateIndividualFastaFiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create PWMs from Module Fasta\n",
    "\n",
    "Generate PWMs for each module, using the module Fasta files. Module PWMs\n",
    "can then be compared to PWMs for 63 known kinase recognition motifs (Mok et al.,\n",
    "2010).\n",
    "\n",
    "Input: A directory containing files in Fasta format.\n",
    "\n",
    "Script uses BioPython to generate position weight matrices from a directory containing Fasta files for\n",
    "each modules phospho-peptides. \n",
    "\n",
    "### Note: \n",
    "Duplicate amino acid sequences should be removed from the Fasta files before running this script, if they exist, to prevent overweighting the matrix\n",
    "\n",
    "\n",
    "    \n",
    "output file should look like:\n",
    "\n",
    "    Motif,AA,0,1,2,3,4,5,6,7,8,9,10,11,12\n",
    "    Induced_...R.NS......,A:,0.044444444444444446,0.044444444444444446,0.044444444444444446, etc...\n",
    "    Induced_...R.NS......,C:,0.022222222222222223,0.022222222222222223,0.022222222222222223, etc...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file: FastaFiles_Modules/Induced_...R.NS.......fasta \n",
      "processing file: FastaFiles_Modules/Induced_....R.S.......fasta \n",
      "processing file: FastaFiles_Modules/Induced_......SP......fasta \n",
      "processing file: FastaFiles_Modules/Induced_......TP......fasta \n",
      "processing file: FastaFiles_Modules/Induced_...R..S.......fasta \n",
      "processing file: FastaFiles_Modules/Induced_......S..E....fasta \n",
      "processing file: FastaFiles_Modules/Induced_......SR..S...fasta \n",
      "processing file: FastaFiles_Modules/Induced_.....NS.......fasta \n",
      "processing file: FastaFiles_Modules/Induced_...RR.S.......fasta \n",
      "processing file: FastaFiles_Modules/Induced_......SP.R....fasta \n",
      "processing file: FastaFiles_Modules/Induced_......S..S....fasta \n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate sequences from each fasta file\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "clean = dict()\n",
    "\n",
    "for fasta in glob.glob('FastaFiles_Modules/*'):\n",
    "    print('processing file: %s ' %(fasta))\n",
    "    for seq_record in SeqIO.parse(fasta, 'fasta'):   # create Seq objects\n",
    "        s = str(seq_record.seq)\n",
    "        if s not in clean:\n",
    "            clean[s] = seq_record                    # only keep unique sequences\n",
    "            \n",
    "    out_handle = open('tmp.fasta', 'w')              \n",
    "    \n",
    "    for k,v in clean.items():                        # write unique sequences to tmp file  \n",
    "        SeqIO.write(v, out_handle, 'fasta')\n",
    "    out_handle.close()\n",
    "            \n",
    "    shutil.move('tmp.fasta', fasta)                  # overwrite original fasta file    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import Bio\n",
    "import os\n",
    "import re\n",
    "from Bio.Seq import Seq\n",
    "from Bio import motifs\n",
    "from Bio.Alphabet import IUPAC\n",
    "\n",
    "alphabet = IUPAC.protein           # use protein alphabet\n",
    "instances = []\n",
    "# list of amino acids used to print the position weight matrix\n",
    "AminoList = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y' ]\n",
    "# column numbers for printing pwm, length of peptide, assumed to be 13, if different change last value\n",
    "pep_Header = ','.join([str(i) for i in range(0,13)])     \n",
    "\n",
    "# user defined directory containing Fasta files\n",
    "#os.chdir(\"/home/mplace/projects/forMatt/Phospho_Network/\")  \n",
    "\n",
    "def CreatePWM():\n",
    "    ''' Function creates PWMs for each Module '''\n",
    "    instances = []\n",
    "    with open('position_weight_matrix.txt', 'w') as out:\n",
    "        out.write('Motif,AA,%s\\n' %(pep_Header))                   \n",
    "        for x in os.listdir('FastaFiles_Modules/'):                 # Iterate through the Fasta files in the directory\n",
    "            if x.endswith('.fasta'):\n",
    "                with open('FastaFiles_Modules/' + x, \"r\") as f:\n",
    "                    for line in f:\n",
    "                        if line.startswith('>'):                                       \n",
    "                            continue\n",
    "                        line = line.rstrip()                                                 \n",
    "                        instances.append(Seq(line, IUPAC.protein))  # add amino acid sequence to instances\n",
    "                    m = motifs.create(instances)\n",
    "                    pwm = m.counts.normalize(pseudocounts = 1)      # Add a +1 pseudocount\n",
    "                    instances = []\n",
    "                    name = re.sub('.fasta', '', x)                  # use file name for 1st column          \n",
    "                    for aa in AminoList :\n",
    "                        score = [ str(i) for i in pwm[aa]]\n",
    "                        score = ','.join(score)\n",
    "                        out.write('%s,%s:,%s\\n' %(name,aa,score))\n",
    "    out.close()\n",
    "                    \n",
    "CreatePWM()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Kullback-Leibler Module to Each Kinase\n",
    "\n",
    "Purpose:  \n",
    "\n",
    "To quantify similarity between the Mok et. al. kinase PWMs and the module\n",
    "PWMs. Script employs a previously described quantitative motif comparison method\n",
    "called Kullback-Leibler divergence (KLD) (Thijs et al., 2002, Gupta et al., 2007).\n",
    "KLD generates a similarity measure by comparing the Kullback-Leiber distance, or\n",
    "information content, for each amino acid at each position between a query and\n",
    "comparison PWM. The more alike two PWMs are, the closer to zero the score approaches.\n",
    "\n",
    "    KLD(X,Y) = 1/2 (E Xalog(Xa/Ya) + E Yalog(Ya/Xa))\n",
    "\n",
    "Where ‘X’ represents a query PWM position and ‘Y’ a comparison PWM position.\n",
    "Xa indicates the probability of a given amino acid a ε A in X. \n",
    "The symbol ‘A’ represents the length of the motif alphabet, which is 20, \n",
    "representing each of the naturally occurring amino acids. \n",
    "\n",
    "\n",
    "\n",
    "Input:\n",
    "A plain text .csv file that contains all module position weight matrices. Each\n",
    "module PWM should have 20 rows, representing each of the 20 naturally occurring\n",
    "amino acids. They are in a column called \"AA\" which stands for amino acid. There\n",
    "should also be 13 columns, labeled 0-12 (representing the 13 amino acid sequence length\n",
    "of the phospho-peptides used to build the position weight matrix) that contain the\n",
    "frequency of each amino acid at each position.\n",
    "\n",
    "Csv file format\n",
    "Motif,AA,0,1,2,3,4,5,6,7,8,9,10,11,12\n",
    "Induced_...sP.,P:,0.05,0.05,0.03, 0.05,0.05,0.03,0.05,0.05,0.03, 0.05,0.05,0.03\n",
    "\n",
    "In addition, a directory that contains the Mok et al kinase PWMs. They have the identical\n",
    "format as above. They have been pre-generated and are available for download on Github.\n",
    "The repository is titled, \"Mok_kinase_PWMs\"\n",
    "\n",
    "Required Parameters: Pandas must be installed on your machine.\n",
    "\n",
    "Output: A directory containing plain text .csv files named after each module (ie.\n",
    "Induced_...sP..txt). Within the .csv files are 63 KLD scores representing how well the\n",
    "63 Mok et al kinases match the module motif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "\n",
    "Compare_To=pd.read_csv('position_weight_matrix.txt')   # input module pwm                                                                                    # The PWMs for the Modules.\n",
    "\n",
    "def DF_to_TSV(dataframe, NewFileName): \n",
    "    ''' Function writes out dataframes as TSV files'''\n",
    "    #path ='' \n",
    "    dataframe.to_csv (NewFileName,sep='\\t')  \n",
    "\n",
    "def SplitCompareTOMotifs_df():\n",
    "    ''' Function splits the Compare_To DF by Motif, which is listed in the \"Motif\" column, \n",
    "        and puts the new dataframes into a list\n",
    "    '''\n",
    "    DF_CompareTo_lst =[]\n",
    "    for Motif in Compare_To['Motif'].unique():\n",
    "        DF=Compare_To.loc[Compare_To['Motif']==Motif]\n",
    "        DF_CompareTo_lst.append(DF)\n",
    "    return DF_CompareTo_lst\n",
    "\n",
    "DF_CompareTo_lst=SplitCompareTOMotifs_df()\n",
    "\n",
    "def SplitInput_df_byMotif():\n",
    "    ''' Split the Input dataframe by Motif and create indpendent dataframes'''\n",
    "    DF_Input_lst =[]\n",
    "    for Motif in Input['Motif'].unique():\n",
    "        DF=Input.loc[Input['Motif']==Motif]\n",
    "        DF_Input_lst.append(DF)\n",
    "    return DF_Input_lst\n",
    "\n",
    "def Copy(df):\n",
    "    ''' Function makes a copy of a dataframe.  '''\n",
    "    df=df.copy()\n",
    "    return df\n",
    "\n",
    "def mergeInputMotifFile_withDF_CompareTo(df_Input,df_CompareTo):\n",
    "    ''' Merge the query and comparison PWMs so that KLD can be calculated by comparing column values'''\n",
    "    df_merged=df_Input.merge(df_CompareTo, on='AA')\n",
    "  \n",
    "    return df_merged\n",
    "\n",
    "def Calculate_log_x_y(df):\n",
    "        ''' Function takes the log2 value of the amino acid frequency at each position of the query/comparison motifs'''\n",
    "        df['0_log(x/y)'] = df.apply(lambda x: math.log(x['0_x'],2) - math.log(x['0_y'],2), axis=1)\n",
    "        df['1_log(x/y)'] = df.apply(lambda x: math.log(x['1_x'],2) - math.log(x['1_y'],2), axis=1)\n",
    "        df['2_log(x/y)'] = df.apply(lambda x: math.log(x['2_x'],2) - math.log(x['2_y'],2), axis=1)\n",
    "        df['3_log(x/y)'] = df.apply(lambda x: math.log(x['3_x'],2) - math.log(x['3_y'],2), axis=1)\n",
    "        df['4_log(x/y)'] = df.apply(lambda x: math.log(x['4_x'],2) - math.log(x['4_y'],2), axis=1)\n",
    "        df['5_log(x/y)'] = df.apply(lambda x: math.log(x['5_x'],2) - math.log(x['5_y'],2), axis=1)\n",
    "        df['6_log(x/y)'] = df.apply(lambda x: math.log(x['6_x'],2) - math.log(x['6_y'],2), axis=1)\n",
    "        df['7_log(x/y)'] = df.apply(lambda x: math.log(x['7_x'],2) - math.log(x['7_y'],2), axis=1)\n",
    "        df['8_log(x/y)'] = df.apply(lambda x: math.log(x['8_x'],2) - math.log(x['8_y'],2), axis=1)\n",
    "        df['9_log(x/y)'] = df.apply(lambda x: math.log(x['9_x'],2) - math.log(x['9_y'],2), axis=1)\n",
    "        df['10_log(x/y)'] = df.apply(lambda x: math.log(x['10_x'],2) - math.log(x['10_y'],2), axis=1)\n",
    "        df['11_log(x/y)'] = df.apply(lambda x: math.log(x['11_x'],2) - math.log(x['11_y'],2), axis=1)\n",
    "        df['12_log(x/y)'] = df.apply(lambda x: math.log(x['12_x'],2) - math.log(x['12_y'],2), axis=1)\n",
    "        return df\n",
    "\n",
    "\n",
    "def Calculate_log_y_x(df):\n",
    "        ''' Function takes the log2 value of the amino acid frequency at each position of the comparison/query motifs'''\n",
    "        df['0_log(y/x)'] = df.apply(lambda x: math.log(x['0_y'],2) - math.log(x['0_x'],2), axis=1)\n",
    "        df['1_log(y/x)'] = df.apply(lambda x: math.log(x['1_y'],2) - math.log(x['1_x'],2), axis=1)\n",
    "        df['2_log(y/x)'] = df.apply(lambda x: math.log(x['2_y'],2) - math.log(x['2_x'],2), axis=1)\n",
    "        df['3_log(y/x)'] = df.apply(lambda x: math.log(x['3_y'],2) - math.log(x['3_x'],2), axis=1)\n",
    "        df['4_log(y/x)'] = df.apply(lambda x: math.log(x['4_y'],2) - math.log(x['4_x'],2), axis=1)\n",
    "        df['5_log(y/x)'] = df.apply(lambda x: math.log(x['5_y'],2) - math.log(x['5_x'],2), axis=1)\n",
    "        df['6_log(y/x)'] = df.apply(lambda x: math.log(x['6_y'],2) - math.log(x['6_x'],2), axis=1)\n",
    "        df['7_log(y/x)'] = df.apply(lambda x: math.log(x['7_y'],2) - math.log(x['7_x'],2), axis=1)\n",
    "        df['8_log(y/x)'] = df.apply(lambda x: math.log(x['8_y'],2) - math.log(x['8_x'],2), axis=1)\n",
    "        df['9_log(y/x)'] = df.apply(lambda x: math.log(x['9_y'],2) - math.log(x['9_x'],2), axis=1)\n",
    "        df['10_log(y/x)'] = df.apply(lambda x: math.log(x['10_y'],2) - math.log(x['10_x'],2), axis=1)\n",
    "        df['11_log(y/x)'] = df.apply(lambda x: math.log(x['11_y'],2) - math.log(x['11_x'],2), axis=1)\n",
    "        df['12_log(y/x)'] = df.apply(lambda x: math.log(x['12_y'],2) - math.log(x['12_x'],2), axis=1)\n",
    "        return df\n",
    "\n",
    "def Calculate_Faax_times_log_x_y(df):\n",
    "    ''' Function multiplies the frequency of an amino acid (Faax) \"Xa\" at a specific position in the query motif against the log(Xa/Ya) for that amino acid\n",
    "     It is calculating this part of the function  \"Xalog(Xa/Ya)\" '''\n",
    "\n",
    "    df['F(aax)*0_log(x/y)']=df['0_x']*df['0_log(x/y)']\n",
    "    df['F(aax)*1_log(x/y)']=df['1_x']*df['1_log(x/y)']\n",
    "    df['F(aax)*2_log(x/y)']=df['2_x']*df['2_log(x/y)']\n",
    "    df['F(aax)*3_log(x/y)']=df['3_x']*df['3_log(x/y)']\n",
    "    df['F(aax)*4_log(x/y)']=df['4_x']*df['4_log(x/y)']\n",
    "    df['F(aax)*5_log(x/y)']=df['5_x']*df['5_log(x/y)']\n",
    "    df['F(aax)*6_log(x/y)']=df['6_x']*df['6_log(x/y)']\n",
    "    df['F(aax)*7_log(x/y)']=df['7_x']*df['7_log(x/y)']\n",
    "    df['F(aax)*8_log(x/y)']=df['8_x']*df['8_log(x/y)']\n",
    "    df['F(aax)*9_log(x/y)']=df['9_x']*df['9_log(x/y)']\n",
    "    df['F(aax)*10_log(x/y)']=df['10_x']*df['10_log(x/y)']\n",
    "    df['F(aax)*11_log(x/y)']=df['11_x']*df['11_log(x/y)']\n",
    "    df['F(aax)*12_log(x/y)']=df['12_x']*df['12_log(x/y)']\n",
    "    return df\n",
    "\n",
    "def Calculate_Faay_times_log_y_x(df):\n",
    "    ''' Function multiplies the frequency of an amino acid (Faay) \"Ya\" at a specific position in the query motif against the log(Ya/Xa) for that amino acid\n",
    "     It is calculating this part of the function  \"Yalog(Ya/Xa)\" '''\n",
    "    df['F(aay)*0_log(y/x)']=df['0_y']*df['0_log(y/x)']\n",
    "    df['F(aay)*1_log(y/x)']=df['1_y']*df['1_log(y/x)']\n",
    "    df['F(aay)*2_log(y/x)']=df['2_y']*df['2_log(y/x)']\n",
    "    df['F(aay)*3_log(y/x)']=df['3_y']*df['3_log(y/x)']\n",
    "    df['F(aay)*4_log(y/x)']=df['4_y']*df['4_log(y/x)']\n",
    "    df['F(aay)*5_log(y/x)']=df['5_y']*df['5_log(y/x)']\n",
    "    df['F(aay)*6_log(y/x)']=df['6_y']*df['6_log(y/x)']\n",
    "    df['F(aay)*7_log(y/x)']=df['7_y']*df['7_log(y/x)']\n",
    "    df['F(aay)*8_log(y/x)']=df['8_y']*df['8_log(y/x)']\n",
    "    df['F(aay)*9_log(y/x)']=df['9_y']*df['9_log(y/x)']\n",
    "    df['F(aay)*10_log(y/x)']=df['10_y']*df['10_log(y/x)']\n",
    "    df['F(aay)*11_log(y/x)']=df['11_y']*df['11_log(y/x)']\n",
    "    df['F(aay)*12_log(y/x)']=df['12_y']*df['12_log(y/x)']\n",
    "    return df\n",
    "\n",
    "def Column_SUM(df):\n",
    "    ''' Function sums the values calculated by the previous two functions for each position, or column, in the PWMs  '''\n",
    "    df['sum_0']=sum(df['F(aax)*0_log(x/y)'])+sum(df['F(aay)*0_log(y/x)'])\n",
    "    df['sum_1']=sum(df['F(aax)*1_log(x/y)'])+sum(df['F(aay)*1_log(y/x)'])\n",
    "    df['sum_2']=sum(df['F(aax)*2_log(x/y)'])+sum(df['F(aay)*2_log(y/x)'])\n",
    "    df['sum_3']=sum(df['F(aax)*3_log(x/y)'])+sum(df['F(aay)*3_log(y/x)'])\n",
    "    df['sum_4']=sum(df['F(aax)*4_log(x/y)'])+sum(df['F(aay)*4_log(y/x)'])\n",
    "    df['sum_5']=sum(df['F(aax)*5_log(x/y)'])+sum(df['F(aay)*5_log(y/x)'])\n",
    "    df['sum_6']=sum(df['F(aax)*6_log(x/y)'])+sum(df['F(aay)*6_log(y/x)'])\n",
    "    df['sum_7']=sum(df['F(aax)*7_log(x/y)'])+sum(df['F(aay)*7_log(y/x)'])\n",
    "    df['sum_8']=sum(df['F(aax)*8_log(x/y)'])+sum(df['F(aay)*8_log(y/x)'])\n",
    "    df['sum_9']=sum(df['F(aax)*9_log(x/y)'])+sum(df['F(aay)*9_log(y/x)'])\n",
    "    df['sum_10']=sum(df['F(aax)*10_log(x/y)'])+sum(df['F(aay)*10_log(y/x)'])\n",
    "    df['sum_11']=sum(df['F(aax)*11_log(x/y)'])+sum(df['F(aay)*11_log(y/x)'])\n",
    "    df['sum_12']=(sum(df['F(aax)*12_log(x/y)'])+sum(df['F(aay)*12_log(y/x)']))\n",
    "    return df\n",
    "\n",
    "def TotalScore(df):\n",
    "    ''' Function calculates the total score by summing the summed values for each position in the PWM (13 positions)'''\n",
    "    df['FinalScore']=df['sum_0']+df['sum_1']+df['sum_2']+df['sum_3']+df['sum_4']+df['sum_5']+df['sum_6']+df['sum_7']+df['sum_8']+df['sum_9']+df['sum_10']+df['sum_11']+df['sum_12']\n",
    "    Lst=df['FinalScore'].unique()\n",
    "    n=Lst[0]\n",
    "    return n\n",
    "\n",
    "# Import the Mok Kinases PWM .csv files individually and create dataframes\n",
    "path=r\"Mok_kinase_PWMs/\"\n",
    "filenames = glob.glob(path + \"*.csv\")\n",
    "\n",
    "dfs_lst = []\n",
    "for filename in filenames:\n",
    "    dfs_lst.append(pd.read_csv(filename, sep=\",\"))\n",
    "    \n",
    "ITER_NUM=1                                          # One iteration of the below function. \n",
    "dict_Final={}\n",
    "for df2 in dfs_lst:                                 # This is the dataframe that has Module PWMs\n",
    "    \n",
    "    subModule_name=df2['Motif'].unique()\n",
    "    for df in DF_CompareTo_lst:                     # select one of the compare to dataframes (Mok Kinase PWM)\n",
    "        Kinase_name=[]\n",
    "        Kinase_name=df['Motif'].unique()\n",
    " \n",
    "        for iteration in range (ITER_NUM):                                      # for the first iteration \n",
    "\n",
    "            Copied=Copy(df2)                                                    # Copy Dataframe\n",
    "            # Create a merged version of the dataframe for each Kinase PWM and each Module PWM\n",
    "            df_merged=mergeInputMotifFile_withDF_CompareTo(Copied, df)  \n",
    "\n",
    "            DF_1=Calculate_log_x_y(df_merged)                                   \n",
    "            DF_2=Calculate_log_y_x(DF_1)\n",
    "            DF_3=Calculate_Faax_times_log_x_y(DF_2)\n",
    "            DF_4=Calculate_Faay_times_log_y_x(DF_3)\n",
    "            DF_5=Column_SUM(DF_4)\n",
    "        \n",
    "            n=TotalScore(DF_5)                                                  \n",
    "            #print (n)\n",
    "            test_tup = (n, subModule_name[0])\n",
    "            if Kinase_name[0] in dict_Final:\n",
    "                dict_Final[Kinase_name[0]].append(test_tup)\n",
    "            else:\n",
    "                lst=[]\n",
    "                dict_Final[Kinase_name[0]] = lst\n",
    "                dict_Final[Kinase_name[0]].append(test_tup)\n",
    "\n",
    "\n",
    "# write out the final dictionary to a folder where each key and value pair is an independent csv file. \n",
    "if not os.path.exists('ClassA_NoShuffle_KL'):\n",
    "    os.mkdir('ClassA_NoShuffle_KL')\n",
    "    \n",
    "path=r\"ClassA_NoShuffle_KL/\"              # this is the path to the folder where the output files will be housed\n",
    "\n",
    "for k, v in dict_Final.items():           # select each key and value pair in the dict \n",
    "    newFile=path+ k +'.csv'               # create newFile, that will have the path and name (the key, which is the kinase) associated with it\n",
    "    #print (newFile)\n",
    "    with open(newFile, 'w') as output:  \n",
    "        output.write(k)\n",
    "        output.write(\"\\n\")\n",
    "        for x in v:\n",
    "           \n",
    "            output.write(str(x))\n",
    "            output.write(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
