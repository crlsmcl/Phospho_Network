{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational phospho-proteomic network inference pipeline\n",
    "####  by Matt Macgilvary\n",
    "\n",
    "###### This pipeline turns a list of S. cerevesiae phospho-peptides that exhibit stress responsive abundance changes, as measured by mass spectrometry, into a hierarchical signaling network, connecting upstream kinases and phosphatases to their downstream targets. Our computational pipeline is based on the premise that kinases and phosphatases recognize target substrates through specific amino acid sequences at the phosphorylated residue, called phosphorylation motifs. This pipeline groups phospho-peptides with similar abundance changes and the same phosphorylation motif into modules. Modules are partitioned into smaller groups, called submodules, based on differences in phospho- peptide abundance in mutant strain(s) (sources). Candidate submodule regulators, called shared interactors, are identified through enrichment analysis using a protein interaction network in yeast (Chasman et al., 2014). Shared interactor-submodule pairs serve as inputs for a previously developed Integer Programming (IP) approach that connects the sources to their downstream target submodules (Chasman et al., 2014).\n",
    "\n",
    "###### Please see our bioRxiv preprint for additional information:\n",
    "    Network inference reveals novel connections in pathways regulating growth and defense in the yeast salt response.   Matthew E. MacGilvray+, Evgenia Shishkova+, Deborah Chasman, Michael Place, Anthony Gitter, Joshua J. Coon, Audrey P. Gasch. bioRxiv 2017. doi:10.1101/176230\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "### The user should define differentially changing phospho-peptides in the \"WT\" or \"Parent\" strain using their own criteria (eg; fold-change, p-value, etc.), followed by grouping/clustering phospho-peptides based on similar directionality of abundance change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify motifs by Calling Motifx.py\n",
    "    \n",
    "    This automates submitting jobs to the Motif-x Website (http://motif-x.med.harvard.edu/)\n",
    "    \n",
    "### Input  : A single Plain text file (called inputfiles in the next cell) listing excel files to process, one excel file name per line.\n",
    "\n",
    "    text file:\n",
    "    data_sheet1.xlsx\n",
    "    data_sheet2.xlsx\n",
    "\n",
    "\tThe Excel file format:\n",
    "\tPpep\tGroup\tLocalized_Sequence\tMotif_X_Input_Peptide\n",
    "\tYGL076C_T8_S11\tInduced\tAAEKILtPEsQLKK\tAAEKILT*PES*QLKK\n",
    "\n",
    "\tColumn order is unimportant, column names must match above.\n",
    "    \n",
    "### Output:\n",
    "A final text table named after the input file containing all the motifs matched to a gene.<br>\n",
    "        \n",
    "Given an input file named,  motifx_sample.xlsx the final results file will be:<br>\n",
    "    \n",
    " > motifx_sample-Motifx-results.txt\n",
    "         \n",
    "The rest of the results are put in a directory.  For instance if your input file is called\n",
    "motifx_sample.xlsx, 3 directories will be created one for each central character.  These \n",
    "contain the LOGO pngs and the original html results page.<br>         \n",
    " > motifx_sample_T<br>\n",
    " > motifx_sample_S<br>\n",
    " > motifx_sample_Y<br>\n",
    "                 \n",
    "__NOTE: if the output file exists, it will not be overwritten and the program will exit.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run -i 'Motifx.py' -f 'inputfiles' -u 'reference/orf_trans_all.20150113.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# required python libraries\n",
    "import Bio\n",
    "import glob\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from scipy.stats import hypergeom\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio import motifs\n",
    "from Bio.Alphabet import IUPAC\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "print(current_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify motifs\n",
    "\n",
    "The motifx results file need to be classified by the user.  For example the output from the motifx results looks like:\n",
    ">YNR051C_T336,SGNNASTPSSSPE,InputFileName,......TP.....<br>\n",
    ">YAL035W_T388,AATPAATPTPSSA,InputFileName,......TP.....<br>\n",
    ">YML008C_T373,KPENAETPSQTSQ,InputFileName,......TP.....<br>\n",
    ">YKL204W_T391,KESRSSTPNAESQ,InputFileName,......TP.....<br>\n",
    "\n",
    "The user must add the grouping. Example: \n",
    "\n",
    "Ppep,Cluster,Motif,Peptide,ire1,mkk1_2\n",
    "YGR240C_S895,Induced,......SP.....,NKKNEASPNTDAK,Induced_Amplified,Induced_Defective\n",
    "YMR005W_S80,Induced,...K..SP.....,VLPKNVSPTTNLR,Induced_Amplified,Induced_Defective\n",
    "YPL242C_S7,Induced,......SP.....,MTAYSGSPSKPGN,Induced_Amplified, \n",
    "\n",
    "Where ire1 and mkk1_2 correspond to gene names and Induced_Amplified,Induced_Defective are grouping provided by user.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Identify_Modules_and_Submodules step.\n",
    "\n",
    "This script identifies co-regulated groups of phospho-peptides using the following approach:\n",
    "\n",
    "1) First, the script identifies 'modules', which are groups of phospho-peptides that exhibit the same directionality in stress-dependent abundance change (ie, increased 'Induced', or decreased 'Repressed') and the same motif. The module nomenclature is as follows: Induced/Repressed- motif (ex: Induced..RK.s....).\n",
    "\n",
    "2) Next, the script partitions modules into 'submodules' based on their phospho-peptide constituents dependency on a protein(s) for stress-dependent abundance changes (ie, phospho-peptides that exhibit increased 'amplified' or decreased 'defective' abundance in a deletion strain compared to the 'WT' or 'Parental' type strain). These phenotypes are user defined. If two or more mutant phenotypes are recorded for a phospho-peptide then it's placed into two separate subModules (one for each mutant phenotype). If there was not a mutant phenotype at a user defined threshold then the phenotype is 'No-Phenotype'\n",
    "\n",
    "The submodule nomenclature is as follows: module name-mutant phenotype/No-Phenotype (ex: Induced..RK.s....Mutant_Defective).\n",
    "\n",
    "Possible submodule phenotypes: Induced-Defective, Induced-Amplified, Repressed-Defective, Repressed-Amplified, Induced-No-Phenotype, Repressed-No-Phenotype\n",
    "\n",
    "idModules.csv file looks like:\n",
    "\n",
    "> Ppep,Cluster,Motif,Peptide,ire1,mkk1_2<br>\n",
    "> YGR240C_S895,Induced,......SP.....,NKKNEASPNTDAK,Induced_Amplified,Induced_Defective<br>\n",
    "> YMR005W_S80,Induced,...K..SP.....,VLPKNVSPTTNLR,Induced_Amplified,Induced_Defective<br>\n",
    "> YPL242C_S7,Induced,......SP.....,MTAYSGSPSKPGN,Induced_Amplified, <br>\n",
    "\n",
    "\n",
    "# How do you classify Induced_Amplified etc...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "Data=pd.read_csv('idModules.csv') # Define path to input file\n",
    "\n",
    "def Slicedataframe():\n",
    "    '''Define a function that slices the input dataframe into independent dataframes based on the Cluster names. Next, slice these dataframes based on the presence of the same motif, generating 'modules' '''  \n",
    "    ClusterLST=Data['Cluster'].unique().tolist()                            # generate a list of unique Cluster names (ie, 'Induced' and 'Repressed')\n",
    "    lst=[]                                                                 \n",
    "    DF=Data.copy()                                                         \n",
    "    for cluster in ClusterLST:                                              # Select the first 'cluster' on the list \n",
    "        # Create a new dataframe by selecting only those rows that contain the selected 'cluster' in the 'Cluster' column \n",
    "        DF2=DF.loc[DF['Cluster']== cluster]                                 \n",
    "        # From the newly created dataframe, place each instance of a unique motif into a list\n",
    "        MotifLST=DF2['Motif'].unique().tolist()                             \n",
    "        cleanedMotifLST = [x for x in MotifLST if str(x) != 'nan']          # drop the string 'nan' from the list. 'nan' occurs for Ppeps that did not have an identified Motif from Motif-X. \n",
    "        for motif in cleanedMotifLST:                                       # Select a motif in the list\n",
    "            DF3=DF2.loc[DF['Motif']== motif]                                # Filter the dataframe, selecting only those rows that contain 'motif' in the Motif column\n",
    "            DF3['freq'] = DF3.groupby('Motif')['Motif'].transform('count')  # Produce a new column, called 'freq' that contains the number of rows, and thus phospho-peptides, that contain a given motif.\n",
    "            lst.append(DF3) \n",
    "    return lst\n",
    "\n",
    "SlicedDF_lst=Slicedataframe()\n",
    "\n",
    "def ConcatenateDFs():\n",
    "    ''' Define a function that appends the dataframes in the SlicedDF_list together. '''\n",
    "    EmptyDF = pd.DataFrame()                                                # create an empty dataframe\n",
    "    for df in SlicedDF_lst:                                                \n",
    "        df=df.copy() \n",
    "        EmptyDF=EmptyDF.append(df)                                          # append to the empty DF the dataframe selected and overwrite the empty dataframe\n",
    "    return EmptyDF\n",
    "\n",
    "Final_DF=ConcatenateDFs()\n",
    "FinalDFV2=Final_DF.fillna(0)                                                #  fill any NaN values with '0'\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "def Module_Motif_NoMutantPhenotypeExists(df):\n",
    "    ''' Define a function that assigns no-phenotype submodules'''\n",
    "    if(df['ire1'] ==0) & (df['mkk1_2']==0):\n",
    "        return 'No_Phenotype_Exists'\n",
    "    \n",
    "FinalDFV2['Phenotype']=FinalDFV2.apply(Module_Motif_NoMutantPhenotypeExists, axis=1) \n",
    "FinalDFV2=FinalDFV2.loc[FinalDFV2['Phenotype']=='No_Phenotype_Exists']        # Select all rows for which \"No_Phenotype_Exists\" in the 'Phenotype' column.\n",
    "FinalDFV2['subModule']=FinalDFV2.Cluster.map(str) + \"_\" + FinalDFV2.Motif + \"_\" + FinalDFV2.Phenotype                                   # create a new column, called submodule, that contains the concatenated strings in the 'Cluster', 'Motif', and 'Phenotype' columns.\n",
    "\n",
    "# CHANGE GENE NAMES HERE\n",
    "FinalDF=Final_DF.dropna(subset = ['ire1', 'mkk1_2'], how='all')        # Remove rows that have NaN in all 3 columns representing mutant phenotpes. This steps removes theNo-phenotype submodules which were creat                                                                               # ed above. \n",
    "FinalDF=FinalDF.fillna(0)                                                     # fill any NaN that remain with '0'\n",
    "lstCols=['ire1', 'mkk1_2']                                             # make a list that contains the column headers for the 3 mutants. \n",
    "\n",
    "\n",
    "\n",
    "def DefineMutantContribution(row):\n",
    "    ''' Define a function that identifies for each phospho-peptide if it has a phenotype in more than one mutant strain'''\n",
    "    dictData={} \n",
    "    for colname in lstCols:    \n",
    "        if not row[colname]==0:                                                # if value is not equal to zero, there is a mutant phenotype (ex; Induced_defective)\n",
    "            dictData[colname]=row[colname]  \n",
    "    if len(dictData.keys())==0: return 0  \n",
    "    else:\n",
    "        return \":\".join(dictData.keys())\n",
    "    \n",
    "FinalDF['Contribution']=FinalDF.apply(lambda x: DefineMutantContribution(x), axis=1) \n",
    "\n",
    "\n",
    "\n",
    "def DefinePhenotypeFromMutants(row):\n",
    "    ''' Define a function that captures the mutant phenotype for Ppeps with multiple phenotypes and places it within a column'''\n",
    "    dictData={}  \n",
    "    for colname in lstCols:   \n",
    "        if not row[colname]==0:\n",
    "            dictData[colname]=row[colname] \n",
    "    if len(dictData.keys())==0: return 0 \n",
    "    else:\n",
    "        return \":\".join(dictData.values()) \n",
    "    \n",
    "FinalDF['Phenotype']=FinalDF.apply(lambda x: DefinePhenotypeFromMutants(x), axis=1)\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' Determine all Ppeps that have 2 or more mutant phenotypes (ire1/mkk1_2 phenotypes), then the script produces a new column with individual subModule names for \n",
    "the ire1 phenotype''' \n",
    "\n",
    "FinalDF_multiplePhenotypes=FinalDF[FinalDF['Contribution'].str.contains(\":\")]     # Select 'contribution column rows that contain \":\", which means the Ppep has two mutant phenotypes since this is a separator between gene names\n",
    "FinalDF_multiplePhenotypes_ire1=FinalDF_multiplePhenotypes[FinalDF_multiplePhenotypes['Contribution'].str.contains(\"ire1\")] \n",
    "FinalDF_multiplePhenotypes_ire1['Ire1']='ire1' \n",
    "FinalDF_multiplePhenotypes_ire1['subModule']=FinalDF_multiplePhenotypes_ire1.Cluster.map(str) + \"_\" + FinalDF_multiplePhenotypes_ire1.Motif + \"_\" + FinalDF_multiplePhenotypes_ire1.Ire1 + \"_\" + FinalDF_multiplePhenotypes_ire1.ire1\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' Define all Ppeps that have 2 or more mutant phenotypes (ire1/mkk1_2 phenotypes), then the script produces a new column with individual subModule names for \n",
    "the mkk1_2 phenotype''' \n",
    "FinalDF_multiplePhenotypes_mkk1_2=FinalDF_multiplePhenotypes[FinalDF_multiplePhenotypes['Contribution'].str.contains(\"mkk1_2\")]\n",
    "FinalDF_multiplePhenotypes_mkk1_2['Mkk1_2']='mkk1_2'\n",
    "FinalDF_multiplePhenotypes_mkk1_2['subModule']=FinalDF_multiplePhenotypes_mkk1_2.Cluster.map(str) + \"_\" + FinalDF_multiplePhenotypes_mkk1_2.Motif + \"_\" + FinalDF_multiplePhenotypes_mkk1_2.Mkk1_2 + \"_\" + FinalDF_multiplePhenotypes_mkk1_2.mkk1_2\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' Define all Ppeps that have 2 or more mutant phenotypes (ire1/mkk1_2 phenotypes), then the script produces a new column with individual subModule names for \n",
    "the cdc14 phenotype'''\n",
    "\n",
    "#FinalDF_multiplePhenotypes_cdc14=FinalDF_multiplePhenotypes[FinalDF_multiplePhenotypes['Contribution'].str.contains(\"cdc14\")]\n",
    "#FinalDF_multiplePhenotypes_cdc14['Cdc14']='cdc14'\n",
    "#FinalDF_multiplePhenotypes_cdc14['subModule']=FinalDF_multiplePhenotypes_cdc14.Cluster.map(str) + \"_\" + FinalDF_multiplePhenotypes_cdc14.Motif + \"_\" + FinalDF_multiplePhenotypes_cdc14.Cdc14 + \"_\" + FinalDF_multiplePhenotypes_cdc14.cdc14\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "'''This section of code appends the above mutant dataframes together (ie, FinalDF_multiplePhenotypes_cdc14, etc.) (contained \":\"). The result is Ppeps with phenotypes in more than one strain are listed on multiple lines rather than a single line'''\n",
    "\n",
    "FinalDF_mutants=FinalDF_multiplePhenotypes_ire1.append(FinalDF_multiplePhenotypes_ire1) \n",
    "FinalDF_mutants_Final=FinalDF_mutants.append(FinalDF_multiplePhenotypes_mkk1_2)\n",
    "\n",
    "# if you have more than 2 gene names add them here after Peptide\n",
    "FinalDF_mutants_Final=FinalDF_mutants_Final[['Ppep','Cluster','Motif','Peptide','ire1','mkk1_2','freq','Contribution','Phenotype','subModule']] # Only retain these columns \n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Drop from the original dataframe rows containing Ppeps with multiple mutant phenotypes. \n",
    "FinalDF_minus_multiPhenotypePpeps=FinalDF[FinalDF.Contribution.str.contains(\":\")==False] # Removing all rows that contain \":\", and thus are phospho-peptides with multiple mutant phenotypes\n",
    "\n",
    "\n",
    "# Generate the final submodule names\n",
    "FinalDF_minus_multiPhenotypePpeps['subModule']=FinalDF_minus_multiPhenotypePpeps.Cluster.map(str) + \"_\" + FinalDF_minus_multiPhenotypePpeps.Motif + \"_\" + FinalDF_minus_multiPhenotypePpeps.Contribution + \"_\" + FinalDF_minus_multiPhenotypePpeps.Phenotype\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Ppeps_with_PhenotypesDF=FinalDF_minus_multiPhenotypePpeps.append(FinalDF_mutants_Final)  # Appending together the dataframes that originally had single mutant phenotypes, and the dataframe that started with multiple mutant Phentoypes, but now contains single listings for each Ppep-mutant phenotype\n",
    "\n",
    "\n",
    "\n",
    "# Remove any submodule that only has a single Ppep constituent, since by default a submodule must contain 2 Ppeps. \n",
    "Ppeps_with_PhenotypesDF_subModules=Ppeps_with_PhenotypesDF[Ppeps_with_PhenotypesDF.duplicated(['subModule'], keep='last') | Ppeps_with_PhenotypesDF.duplicated(['subModule'])]  # only retain duplicates, get rid of single entries \n",
    "\n",
    "\n",
    "\n",
    "# Append to the dataframe with phenotype subModules, all No-Phenotype submodules\n",
    "Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF=Ppeps_with_PhenotypesDF_subModules.append(FinalDFV2) # append to dataframe\n",
    "Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF=Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF[['Ppep', 'Cluster', 'Motif', 'Peptide', 'ire1', 'mkk1_2', 'freq', 'Contribution', 'Phenotype', 'subModule']]\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' Create a column with the 'Module' name '''\n",
    "Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF['Module']=Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF.Cluster.map(str) + \"_\" + Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF.Motif \n",
    "\n",
    "# Make a column concatenating Ppep and subModule and drop the duplicates based on that column\n",
    "Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF['Name'] = Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF['Ppep'] + \"_\" + Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF['subModule']\n",
    "Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF.drop_duplicates(subset='Name', keep='first', inplace=True)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# define a function that will write out a dataframe as a tab separated file\n",
    "def Dataframe_to_Tsv (dataframe, NewFileName):\n",
    "    dataframe.to_csv (NewFileName,sep=',', index=False)\n",
    "\n",
    "Dataframe_to_Tsv(Ppeps_with_Phenotypes_subModules_and_noPhenotypes_DF, 'Modules_pPep.csv') \n",
    "# The above file contains all modules and subModules with and without mutant phenotypes. \n",
    "\n",
    "print('Done')\n",
    "# OUTPUT: Modules_pPep.csv\n",
    "#   for the case of 2 genes ire1, mkk1_2, header looks like\n",
    "#   Ppep    Cluster Motif   Peptide ire1    mkk1_2  freq    Contribution    Phenotype       subModule       Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep for Identify Shared Interactors \n",
    "\n",
    "input file : \n",
    "Submodule,ORF<br>\n",
    "Induced_......TP....._cdc14_Repressed_Amplified,YLR319C<br>\n",
    "Induced_......TP....._cdc14_Repressed_Amplified,YJL070C<br>\n",
    "\n",
    "#### NOTE: ORF names have need to have the '-' removed,  YER074W-A becomes YER074A.\n",
    "\n",
    "The other input files are provided.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "##  CHANGE INPUT FILE HERE   ##\n",
    "inputFile = 'Modules_pPep.csv'\n",
    "\n",
    "# create the input file based on the output of the previous step.\n",
    "with open('Submodule_constituents.csv', 'w') as out:\n",
    "    out.write('Submodule,ORF\\n')\n",
    "    with open(inputFile,'r') as f:\n",
    "        f.readline()                            # skip header\n",
    "        for line in f:\n",
    "            data = line.rstrip().split(',')    # CHECK THE FILE DELIMITER \n",
    "            name = data[0].split('_')[0]   \n",
    "            name = re.sub('-', '', name)\n",
    "            row  = data[9] + ',' + name + '\\n'\n",
    "            \n",
    "            out.write(row)\n",
    "out.close()\n",
    "print('Done')\n",
    "# OUTPUT: Submodule_constituents.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Shared Interactors \n",
    "\n",
    "This script identifies proteins enriched for interactions with Submodule constituent proteins, based on known interactions in the background network. We call these proteins 'Shared Interactors'. The background network is a protein\n",
    "interaction network curated in yeast under mostly nutrient replete conditions that contains 4638 proteins and ~ 25,000 interactions, including directed (ex; kinase-substrate), and \n",
    "non-directed. \n",
    "\n",
    "Proteins enriched for interactions with Submodule proteins at a 5% FDR, determined by a hypergeometric test and BH correction, are considered shared interactors.\n",
    "\n",
    "Shared Interactors represent numerous functional classes, including kinases and phosphatases. Kinase and phosphatase shared interactors represent potential Submodule regulators.\n",
    " \n",
    "HyperG function:\n",
    "distrib=hypergeom(N,M,n)\n",
    "distrib.pmf(m)\n",
    "\n",
    "* N - population size (4638 unique proteins in Background network file - phospho_v4_bgnet_siflike_withdirections_Matt_Modified.csv)\n",
    "\n",
    "* M - total number of successes  (# of interactions for a given protein. ie. Protein A has 200 known interactions in the background network).\n",
    "\n",
    "* n - the number of trials (also called sample size) -  ie. (Number of proteins that reside within a submdoule)\n",
    "\n",
    "* m - the number of successes - for example: Protein A, a shared interactor, has 35 interactions with proteins in Submodule B. \n",
    " \n",
    " \n",
    " Final shared interactor file:   __Final_enriched.csv__  , this contains the significant Shared Interactors based on the\n",
    " BH_significance test.\n",
    " \n",
    " A list of all shared interactors can be found:  __Network_Submodule_Nodes_background_Network.csv__\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Submodule_DF   = pd.read_csv(current_dir + '/Submodule_constituents.csv')                                                                       # File that contains Submodule names and their protein constituents\n",
    "BgNet          = pd.read_csv(current_dir + '/SI_Identification_Input_Files/Background_Network.csv')                                                                                   # Background network of protein interactions\n",
    "Num_Prot_Inter = pd.read_csv(current_dir + '/SI_Identification_Input_Files/Number_Interactions_Each_Protein.csv')                                              # Number of protein interactions for each protein in the background network\n",
    "Annotation_DF  = pd.read_csv(current_dir + '/SI_Identification_Input_Files/Annotation.csv')                                                   # Yeast protein annotation file\n",
    " \n",
    "# Send the Submodules to a list, but filter out duplicates, which there will be many, since the Submodules will have been found in many proteins.\n",
    "Submodule_List=Submodule_DF['Submodule'].unique().tolist()          \n",
    "\n",
    "\n",
    "dicOrfs={}\n",
    "for Submodule in Submodule_List:                                                                                                                            # Key (Submodule), Value (Yeast ORFs that are Submodule constituents). Filter ORFs found twice to single occurence (important for enrichment analysis)\n",
    "    dicOrfs[Submodule]=(Submodule_DF.loc[Submodule_DF['Submodule'] == Submodule])['ORF'].unique().tolist()\n",
    "        \n",
    "\n",
    "dicOrfsCounts={}  \n",
    "for k,v in dicOrfs.items():  \n",
    "    if k not in dicOrfsCounts:  \n",
    "        value=len(v)            \n",
    "        dicOrfsCounts[k]=value\n",
    "        \n",
    "df_Submodule_Size=pd.DataFrame(list(dicOrfsCounts.items()),                                                                                                  # convert dict to dataframe.\n",
    "                      columns=['Submodule','n'])\n",
    "\n",
    "def SliceDataframe():\n",
    "    ''' For each Submodule identify all proteins that interact with the Submodule proteins in the backgroudn network '''\n",
    "    lst = []\n",
    "    for key in dicOrfs.keys():                                                                                                                             #Select the key, which is a Submodule, from the dict\n",
    "        CurrentDF=BgNet.copy() \n",
    "        x=CurrentDF[CurrentDF['Protein1'].isin(dicOrfs[key])].rename(columns={'Protein1':'Submodule_Containing_Proteins', 'Protein2':'Possible_Shared_Interactors'})                              #Create a new dataframe that is a slice of the salt background network, and only contains proteins that were passed in \"dicOrfs[key]\". At the same time, rename the columns                                \n",
    "        x['Submodule']=key \n",
    "        lst.append(x)\n",
    "        \n",
    "    return lst\n",
    "\n",
    "Sliced_dataframe_list= SliceDataframe()\n",
    "      \n",
    "def Add_n():    \n",
    "    ''' Function adds 'n', the number of proteins in the Submodule, to each dataframe'''\n",
    "    lst= []\n",
    "    for df in Sliced_dataframe_list:\n",
    "        NewDF=df.merge(df_Submodule_Size)\n",
    "        lst.append(NewDF)\n",
    "        \n",
    "    return lst\n",
    "\n",
    "Sliced_dataframe_list= Add_n()\n",
    "\n",
    "def Identify_Shared_Interactors():\n",
    "    ''' Function identifies proteins that interact with at least 2 protein constituents of each submodule'''\n",
    "    \n",
    "    lst=[] \n",
    "    for df in Sliced_dataframe_list: \n",
    "        NewDF=df.copy()\n",
    "        NewDF2=NewDF[NewDF.duplicated(['Possible_Shared_Interactors'], keep = 'last')| NewDF.duplicated(['Possible_Shared_Interactors'])]                  # Only retain proteins that interact with at least 2 submodule protein constituents\n",
    "        x=NewDF2.sort_values(by='Possible_Shared_Interactors', ascending=True) \n",
    "        lst.append(x)\n",
    "       \n",
    "    return lst\n",
    "\n",
    "Shared_Interactors_lst=Identify_Shared_Interactors()\n",
    "\n",
    "def AppendDFs_that_Contain_AllSharedInteractors_and_their_targets():\n",
    "    ''' Function appends all submodules and their shared interactors together into a single file'''\n",
    "    EmptyDF = pd.DataFrame() \n",
    "    for df in Shared_Interactors_lst:  \n",
    "        df=df.copy() \n",
    "        EmptyDF=EmptyDF.append(df)\n",
    "    return EmptyDF\n",
    "\n",
    "SI_andTargets=AppendDFs_that_Contain_AllSharedInteractors_and_their_targets()\n",
    "\n",
    "SI_andTargets_FINAL=pd.merge(left=SI_andTargets, right=Annotation_DF, how='left',\n",
    "                              left_on='Possible_Shared_Interactors', right_on='systematic_name_dash_removed')                                               # complete a merge so I can get the dashes back in the names, which are not included in the background network\n",
    "del SI_andTargets_FINAL['Possible_Shared_Interactors']                                                                                                      # drop because  lacks the dashes which are needed for the correct naming convention\n",
    "del SI_andTargets_FINAL['systematic_name_dash_removed']                                                                                                     # drop because carried over from the merge\n",
    "del SI_andTargets_FINAL['Directed']\n",
    "\n",
    "SI_andTargets_FINAL.columns = ['Submodule_Containing_Proteins', 'Interaction', 'Submodule', 'n','Possible_Shared_Interactors']                        # rename columns\n",
    "\n",
    "myDF = pd.DataFrame(SI_andTargets_FINAL)\n",
    "# OUTPUT NAME FOR SHARED INTERACTORS\n",
    "filename = 'SI_Identification_SubmoduleS__SIs_and_Targets_FDR.csv'\n",
    "myDF.to_csv(filename, index=False, encoding='utf-8' )              # All interactions between SIs and their submodule constituent proteins. No enrichment at this step.\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' Preparing dataframe for Hypergeometric test'''\n",
    "\n",
    "def Add_N_and_m():\n",
    "    ''' Function adds 'N' and calculates 'm' values, which are inputs for the hypergeometric test, to the datframe'''\n",
    "    lst=[]\n",
    "    for df in Shared_Interactors_lst:\n",
    "        NewDF=df.copy()\n",
    "        NewDF['N'] = 4638          # THIS IS THE LENGTH OF THE DATA FRAME, *******************************                                                                                                                         # of proteins in the background network\n",
    "        NewDF['m'] = NewDF.groupby('Possible_Shared_Interactors')['Possible_Shared_Interactors'].transform('count')\n",
    "        lst.append(NewDF)\n",
    "    \n",
    "    return lst\n",
    "\n",
    "Dataframes_list_with_n_N_m=Add_N_and_m()\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def Drop_dups():\n",
    "    ''' For each dataframe, which contains a single submodule, it's protein constituents, and shared interactors, drop duplicate entries for identified SI proteins\n",
    "    . This leaves a single entry for each shared interactor protein. '''\n",
    "    lst=[]\n",
    "    for df in Dataframes_list_with_n_N_m:\n",
    "        NewDF=df.copy()\n",
    "        Final_DF=NewDF.drop_duplicates('Possible_Shared_Interactors')\n",
    "        Final_DF=Final_DF.rename(columns={'Possible_Shared_Interactors':'Shared_Interactor'})\n",
    "        lst.append(Final_DF)\n",
    "        \n",
    "    return lst\n",
    "\n",
    "Drop_Dups_lst=Drop_dups()\n",
    "\n",
    "\n",
    "def Return_M():\n",
    "    ''' Function identifies 'M' (the total number of interactions for each Shared Interactor protein in the background network) and adds that number\n",
    "    to the dataframe'''\n",
    "    lst=[]\n",
    "    for df in Drop_Dups_lst:\n",
    "        NewDF=df.copy()\n",
    "        NewDF2=df.copy()\n",
    "        NewDF_lst=NewDF['Shared_Interactor'].tolist()                                                                                                            # place all proteins in the 'Shared_Interactor' column in a list \n",
    "        Shared_Interactors=Num_Prot_Inter[Num_Prot_Inter['Protein'].isin(NewDF_lst)].rename(columns={'Protein':'Shared_Interactor', 'Total':'M'})\n",
    "        Shared_Interactor_merge=Shared_Interactors.merge(NewDF2, on='Shared_Interactor')\n",
    "        Shared_Interactor_merge=Shared_Interactor_merge.sort_values(by='Shared_Interactor', ascending=True)\n",
    "        lst.append(Shared_Interactor_merge)\n",
    "        \n",
    "    return lst\n",
    "\n",
    "Return_M_lst=Return_M()\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------\n",
    "def hyper(N,M,n,m): \n",
    "    ''' Function defines the parameters for a hypergeometric test that returns a p-value representing the chances of identifying >= x, where x is the number of successes '''  \n",
    "    frozendist=hypergeom(N,M,n)\n",
    "    ms=np.arange(m, min(n+1, M+1))\n",
    "    rv=0;\n",
    "    for single_m in ms: rv=rv+frozendist.pmf(single_m)\n",
    "    return rv\n",
    "\n",
    "def run_hyper():\n",
    "    ''' Function calls the hypergeometric function above  on each shared interactor for each submodule'''\n",
    "    lst=[]\n",
    "    for df in Return_M_lst:\n",
    "        if not df.empty:\n",
    "            NewDF=df.copy()\n",
    "            NewDF['p-value'] = NewDF.apply(lambda row: hyper(row['N'], row['M'], row['n'], row['m']), axis=1)\n",
    "            lst.append(NewDF)\n",
    "        \n",
    "    return lst \n",
    "\n",
    "run_hyper_lst=run_hyper()\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def AppendDFs():\n",
    "    ''' Append DFs for each submodule and it's SIs together into a single DF'''   \n",
    "    EmptyDF = pd.DataFrame() #\n",
    "    for df in run_hyper_lst: \n",
    "        df=df.copy() \n",
    "        EmptyDF=EmptyDF.append(df)\n",
    "    return EmptyDF\n",
    "\n",
    "Final=AppendDFs()\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' Prepping for Benjamini Hochberg procedure. Below code is ranking p-values from 1 to n based on lowest to highest p-value score'''\n",
    "\n",
    "Final=Final.sort_values(by=['p-value'],ascending=[True])                                                                                              # Sort p-values from lowest to highest\n",
    "Final_resetIndex=Final.reset_index()                                                                                                        # Reset the index after the sort\n",
    "Final_resetIndex.index +=1                                                                                                                  # start numbering at 1 for index\n",
    "       \n",
    "NewDF=Final_resetIndex\n",
    "NewDF_Allp_values=Final_resetIndex\n",
    "NewDF=NewDF[['p-value']]                                                                                                                    # select only the p-value column of the dataframe \n",
    "NewDF_dropdups=NewDF.drop_duplicates('p-value')                                                                                             # drop duplicate p-values\n",
    "NewDF_dropdups=NewDF_dropdups.reset_index()                                                                                                 # reset the index\n",
    "NewDF_dropdups.index +=1                                                                                                                    # start numbering at 1 for index\n",
    "NewDF_dropdups['Rank(i)'] = NewDF_dropdups.index                                                                                            # #Add a rank column that will be filled with index values. \n",
    "NewDF_dropdups=NewDF_dropdups.drop('index', 1)                                                                                              # Drop the additional column 'index' that is not sorted.\n",
    "NewDF_merge=NewDF_Allp_values.merge(NewDF_dropdups, on='p-value')                                                                           # create a new dataframe that is a merge of the dataframe with all p-values, and the dataframe with unique p-values and their ranks. \n",
    "NewDF_merge=NewDF_merge.drop('index',1)                                                                                                     # drop the index that was added from the merge. This leaves all p-values ordered from lowest to highest with their ranking.\n",
    "\n",
    "'''Add parameters necessary for completing Benjamini-Hochberg procedure '''\n",
    "\n",
    "NewDF=NewDF_merge\n",
    "NewDF['m_(number_of_tests)']=(len(NewDF))                                                                                                   # Add 'm (number of tests)' column \n",
    "NewDF['Q_(FDR)']=0.05      # THIS IS THE FDR VALUE, USER CAN CHANGE **************************************                                                                                                                 # Add Q (FDR) column. This can be changed manually.\n",
    "NewDF['(i/m)Q']=((NewDF['Rank(i)']/NewDF['m_(number_of_tests)'])*NewDF['Q_(FDR)'])                                                          # add the (i/m)Q column \n",
    "NewDF['BH_significant']=NewDF.apply(lambda x: 1 if x['p-value']<x['(i/m)Q'] else 0, axis=1)                                                 # Identify which proteins are  significant. \n",
    "NewDF=pd.merge(left=NewDF, right=Annotation_DF, how='left', left_on='Shared_Interactor', right_on='systematic_name_dash_removed')           # complete a merge to recover dashed version of YORFs\n",
    "del NewDF['Shared_Interactor'] \n",
    "del NewDF['systematic_name_dash_removed']\n",
    "del NewDF['Directed']\n",
    "NewDF.columns = ['M','Submodule_Containing_Proteins', 'Interaction', 'Submodule', 'n','N','m','p-value','Rank(i)', 'm_(number_of_tests)', 'Q_(FDR)','(i/m)Q','BH_significant', 'Shared_Interactor'] # rename columns\n",
    "\n",
    "myDF = pd.DataFrame(NewDF)\n",
    "filename = 'Network_Submodule_Nodes_background_Network.csv'\n",
    "myDF.to_csv(filename, index=False, encoding='utf-8' )       # Write out final file with enriched shared interactors for each submodule\n",
    "\n",
    "\n",
    "# FILTER FOR THE FINAL Shared Interactors.\n",
    "# Open and parse Network_Submodule_Nodes_background_Network.csv \n",
    "# Only keep the identified Shared Interactors about the first zero that appears in the BH_Significant column.\n",
    "with open('Final_enriched.csv', 'w') as outfile, open('Network_Submodule_Nodes_background_Network.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('M,'):\n",
    "            outfile.write(line)\n",
    "            continue\n",
    "        dat = line.split(',')\n",
    "        if dat[12] == '0':\n",
    "            break\n",
    "        else:\n",
    "            outfile.write(line)\n",
    "\n",
    "f.close()\n",
    "outfile.close()\n",
    "\n",
    "\n",
    "print('Done')\n",
    "# OUTPUT: SI_Identification_SubmoduleS__SIs_and_Targets_FDR.csv, \n",
    "#         Network_Submodule_Nodes_background_Network.csv\n",
    "#         Final_enriched.csv\n",
    "\n",
    "\n",
    "# MY file SI_Identification_SubmoduleS__SIs_and_Targets_FDR.csv is a perfect matche for matt's ouput file: \n",
    "#  SI_Identification_T120_DTT_Sept2017_T120_Possible_SIs_and_Targets_Dashes_Removed_4638_proteins_BOTH_Reps_Ppeps_NORMALIZED.csv\n",
    "#\n",
    "\n",
    "# My file Network_Submodule_Nodes_background_Network.csv is a good match ( I think exact but only checked a few ) with\n",
    "# Matt's file: /5_Identify_SIs/Output/SIs_DTT_Network_Sept2017_T120_BOTH_Reps_FDR_0.05_BH_done_At_Once_sig_and_not_Dashes_Removed_4638_Nodes_background_Network_NORMALIZED.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify Shared Interactors Inputs Outputs\n",
    "\n",
    "For each SI and it's connections with submodule protein constituents, determine if the SI acts upon the submodule (that is, the Shared Interactor has at least 1 directional interaction, or ppi interaction, with a submodule protein), or if the submodule acts upon the SI (that is, all interactions between the SI and submodule proteins have the 'Reverse' designation', indicating that the submodule proteins act upon the SI).\n",
    "\n",
    "- If all of the interactions are reversed, then the script will define the relationship between the SI and the submodule as \"Output\"\n",
    "\n",
    "- If there is at least one interaction that is directed from SI towards submodule, or is a ppi, the relationship between the SI and the submodule is defined as \"Input\"\n",
    "\n",
    "This script takes an input file that contains the following:\n",
    "\n",
    "- All enriched Shared Interactors (SIs) (according to HyperG) and their connections to submodules.\n",
    "- All known protein interactions for each SI (ppi, kinase-substrate, etc)\n",
    "- Many of these interactions are directed (kinase-substrate, metabolic pathway, etc). PPI are not a directed interaction.\n",
    "\n",
    "Input: plain csv text file\n",
    "\n",
    "Csv format:\n",
    "SI_submodule,Shared_Interactor,SI_name,Motif_Containing_Proteins,submodule_Name\n",
    ",Interaction_Directionality\n",
    "\n",
    "YLR164C_Repressed_..RR.s.No_Phenotype_Exists,YLR164C,Tpk1,YDR207C,\n",
    "Repressed_..RR.s.No_Phenotype_Exists, kinase_substrate\n",
    "\n",
    "Column order is unimportant, column names must match above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import yeast_Gene_name_to_ORF as yg          # use SI_Identification_SubmoduleS__SIs_and_Targets_FDR.csv'd to get standard name\n",
    "\n",
    "# create input files \n",
    "# SI_Identification_SubmoduleS__SIs_and_Targets_FDR.csv - input from ID shared interactors\n",
    "submod = dict()\n",
    "with open('SI_Identification_SubmoduleS__SIs_and_Targets_FDR.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        row = line.rstrip().split(',')\n",
    "        submod[row[0] + '_' + row[2]] = row\n",
    "\n",
    "f.close()\n",
    "\n",
    "# get network information\n",
    "with open('classify_sharedInteractors_input.csv', 'w') as out:\n",
    "    header = '%s,%s,%s,%s,%s,%s\\n' %('SI_submodule','Shared_Interactor','SI_name','Motif_Containing_Proteins','submodule_Name'\n",
    ",'Interaction_Directionality')\n",
    "    out.write(header)\n",
    "    with open('Final_enriched.csv') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('M'):\n",
    "                continue\n",
    "            row = line.rstrip().split(',')\n",
    "            if int(row[12]) != 1:\n",
    "                continue\n",
    "            name = row[1] + '_' + row[3]\n",
    "            if name in submod:\n",
    "                if row[1].endswith(('A','B')):\n",
    "                    tmp = list(row[1])\n",
    "                    tmp.insert(-1,'-')\n",
    "                    row[1] = \"\".join(tmp)\n",
    "                n = re.sub('-', '', row[1])\n",
    "                ln = name + ',' + n + ',' + yg.sc_orfToGene[row[1]] + ',' + row[-1] + ',' + row[3] + ',' + row[2] + '\\n'\n",
    "                out.write(ln)\n",
    "\n",
    "Input_df=pd.read_csv('classify_sharedInteractors_input.csv')\n",
    "\n",
    "def Split_based_on_SI_submodule_Column():\n",
    "    ''' Function splits the input DF into independent DFs based on the SI-submodule column pairs. Thus, each SI and it's submodule protein interactions are\n",
    "    in independent dataframes '''\n",
    "    DF_lst =[]\n",
    "    for SI_submodule in Input_df['SI_submodule'].unique():\n",
    "        DF=Input_df.loc[Input_df['SI_submodule']==SI_submodule]\n",
    "        DF_lst.append(DF)\n",
    "    return DF_lst\n",
    "\n",
    "DF_lst=Split_based_on_SI_submodule_Column()\n",
    "\n",
    "def Count_Instances_of_Reverse_Interaction():\n",
    "    ''' Function counts, for each DF, and thus each SI-submodule pair, how many of the interactions are 'reversed', or facing from submodule TOWARDS SI. \n",
    "        It also counts the length of the dataframe, and then subtracts the the length of the dataframe from the counts. If the resultant value is 0, then all of the interactions \n",
    "        were reversed '''\n",
    "    DF_Counts_lst=[]\n",
    "    for df in DF_lst:\n",
    "        df=df.copy()\n",
    "        df['Counts']=df.Interaction_Directionality.str.contains('Reversed').sum()                                                               # Count the number of interactions that are \"Reversed\"\n",
    "        x=len(df)\n",
    "        df['Length']=x\n",
    "        df['Counts_Length']=df['Counts']-df['Length']\n",
    "        \n",
    "        DF_Counts_lst.append(df)\n",
    "    return DF_Counts_lst\n",
    "\n",
    "DF_Counts_lst=Count_Instances_of_Reverse_Interaction()\n",
    "\n",
    "def Only_Reverse_Interactions_Move_to_Outgoing_Columns():\n",
    "    '''Function assigns 'Input' and 'Output' classifications based on the 'Counts_Length' column in the dataframe. '0' values are 'outputs', all other's are 'inputs' '''\n",
    "    df_Modified_Outgoing_lst=[]\n",
    "    for df in DF_Counts_lst:\n",
    "        for value in df['Counts_Length'].unique():\n",
    "           \n",
    "            if value == 0:\n",
    "                df['Shared_Interactor_submodule_Relationship']= 'Output'\n",
    "                df_Modified_Outgoing_lst.append(df)\n",
    "            else:\n",
    "                df['Shared_Interactor_submodule_Relationship']= 'Input'\n",
    "                df_Modified_Outgoing_lst.append(df)\n",
    "           \n",
    "    return df_Modified_Outgoing_lst\n",
    "            \n",
    "df_Modified_Outgoing_lst=Only_Reverse_Interactions_Move_to_Outgoing_Columns()\n",
    "\n",
    "\n",
    "def AppendDFs(): \n",
    "    '''Function appends all dataframes back together '''\n",
    "    EmptyDF = pd.DataFrame()\n",
    "    for df in df_Modified_Outgoing_lst: \n",
    "        df=df.copy() \n",
    "        EmptyDF=EmptyDF.append(df) \n",
    "    return EmptyDF\n",
    "\n",
    "Final=AppendDFs()    \n",
    "\n",
    "Final_Keep_Columns_Needed_For_SIF=Final[['SI_submodule', 'Shared_Interactor', 'submodule_Name', 'Shared_Interactor_submodule_Relationship']]  \n",
    "Final_Keep_Columns_Needed_For_SIF=Final_Keep_Columns_Needed_For_SIF.drop_duplicates('SI_submodule')                                                                     # Dropping duplicates entries, which are created because for each SI-submodule interaction there are numerous interactions with protein constituent. Only want a single interaction, input or output, for each SI and it's submodule. \n",
    "\n",
    "# create a new dataframe and write results to file\n",
    "myDF = pd.DataFrame(Final_Keep_Columns_Needed_For_SIF)\n",
    "filename = 'SIs_submodule_Relationships_Define_ClassA_Network.csv'\n",
    "myDF.to_csv(filename, index=False, encoding='utf-8',sep='\\t') \n",
    "    \n",
    "print('Done')\n",
    "# OUTPUT:  classify_sharedInteractors_input.csv  -- DROP columns Motif_Containing Proteins Interaction_Directionality\n",
    "#          SIs_submodule_Relationships_Define_ClassA_Network.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Fasta file\n",
    "\n",
    "This file is parsed to produce a file which looks like:\n",
    "\n",
    "    Module,Name,Sequence\n",
    "    Group_1_......SP.....,YJL082W_S187,KNSSSPSPSEKSQ\n",
    "\n",
    "All peptide sequences should be the same length (13 amino acids).\n",
    "\n",
    "Module constituents should be used here, not submodules. \n",
    "Fasta Files for each module will be created in a dir called: FastaFiles_Modules/\n",
    "\n",
    "The output Fasta format files are named with their module designation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open & parse file\n",
    "with open('pwm_input.csv', 'w') as out:\n",
    "    header = 'Module,Name,Sequence\\n'\n",
    "    out.write(header)\n",
    "    with open('Modules_pPep.csv','r') as f:\n",
    "        f.readline() \n",
    "        for line in f:\n",
    "            dat = line.rstrip().split(',')      # CHECK DELIMITER usually , or tab\n",
    "            outrow = '%s,%s,%s\\n' %(dat[10],dat[0], dat[3])\n",
    "            out.write(outrow)\n",
    "            \n",
    "Input_df=pd.read_csv(current_dir + '/pwm_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Split_Into_SeparateDFs():\n",
    "    ''' Function splits the input dataframe, based on the module name, into independent dataframes for each module'''\n",
    "    df_lst=[]\n",
    "    for Module in Input_df['Module'].unique():\n",
    "        DF=Input_df.loc[Input_df['Module']==Module]\n",
    "        df_lst.append(DF)\n",
    "        \n",
    "    return df_lst\n",
    "\n",
    "df_lst=Split_Into_SeparateDFs()\n",
    "#print (df_lst)\n",
    "\n",
    "if not os.path.exists( current_dir + '/FastaFiles_Modules'):\n",
    "    os.mkdir(current_dir + '/FastaFiles_Modules')\n",
    "\n",
    "def CreateIndividualFastaFiles():\n",
    "    '''Function creates individual fasta files for each module nd writes them out to a user defined directory'''\n",
    "    for df in df_lst:                \n",
    "        Module_lst=df[\"Module\"].tolist()    \n",
    "        for name in Module_lst:          \n",
    "        # open a new file that contains the module name. USER can Change directory here.\n",
    "            ofile= open(current_dir + \"/FastaFiles_Modules/\"+name+\".fasta\", \"w\") \n",
    "        \n",
    "            df_lstName=df['Name'].tolist()             # send the module names to a list\n",
    "            df_lstSeq=df['Sequence'].tolist()          # send the peptide sequences to a list \n",
    "            \n",
    "            for i in range(len(df_lstSeq)):                    \n",
    "                \n",
    "                ofile.write(\">\" + df_lstName[i] + \"\\n\" + df_lstSeq[i] + \"\\n\")                                            # create a fasta file where the peptide name will be followed by the peptide sequence, on a new line\n",
    "           \n",
    "        df_lstName=[]                                                                                                    # empty each of the lists for the next iteration\n",
    "        df_lstSeq=[]\n",
    "        Module_lst=[]\n",
    "        ofile.close           \n",
    "        \n",
    "    return \n",
    "      \n",
    "CreateIndividualFastaFiles()\n",
    "\n",
    "# OUTPUT: pwm_input.csv\n",
    "#         FastaFiles_Modules/*.fasta \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create PWMs from Module Fasta\n",
    "\n",
    "Generate PWMs for each module, using the module Fasta files. Module PWMs\n",
    "can then be compared to PWMs for 63 known kinase recognition motifs (Mok et al.,\n",
    "2010).\n",
    "\n",
    "## Always run on the Module level, i.e. Induced_sp, not Induced_sp mutant phenotype\n",
    "\n",
    "Input: A directory containing files in Fasta format.\n",
    "\n",
    "Script uses BioPython to generate position weight matrices from a directory containing Fasta files for\n",
    "each modules phospho-peptides. \n",
    "\n",
    "### Note: \n",
    "Duplicate amino acid sequences should be removed from the Fasta files before running this script, if they exist, to prevent overweighting the matrix. No value can be zero in the pwm, if the script fails check.\n",
    "\n",
    "\n",
    "    \n",
    "output file should look like:\n",
    "\n",
    "    Motif,AA,0,1,2,3,4,5,6,7,8,9,10,11,12\n",
    "    Induced_...R.NS......,A:,0.044444444444444446,0.044444444444444446,0.044444444444444446, etc...\n",
    "    Induced_...R.NS......,C:,0.022222222222222223,0.022222222222222223,0.022222222222222223, etc...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove duplicate sequences from each fasta file\n",
    "\n",
    "for fasta in glob.glob('FastaFiles_Modules/*'):\n",
    "    print('processing file: %s ' %(fasta))\n",
    "    clean = {}\n",
    "    for seq_record in SeqIO.parse(fasta, 'fasta'):   # create Seq objects\n",
    "        s = str(seq_record.seq)\n",
    "        if s not in clean:\n",
    "            clean[s] = seq_record                    # only keep unique sequences\n",
    "            \n",
    "\n",
    "            \n",
    "    out_handle = open('tmp.fasta', 'w')              \n",
    "    \n",
    "    for k,v in clean.items():                        # write unique sequences to tmp file  \n",
    "        SeqIO.write(v, out_handle, 'fasta')\n",
    "    out_handle.close()\n",
    "            \n",
    "    shutil.move('tmp.fasta', fasta)                  # overwrite original fasta file    \n",
    "\n",
    "print('Done')\n",
    "# OUTPUT: FastaFiles_Modules/*.fasta  file have duplicates removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alphabet = IUPAC.protein           # use protein alphabet\n",
    "instances = []\n",
    "# list of amino acids used to print the position weight matrix\n",
    "AminoList = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y' ]\n",
    "# column numbers for printing pwm, length of peptide, assumed to be 13, if different change last value\n",
    "pep_Header = ','.join([str(i) for i in range(0,13)])     \n",
    "\n",
    "# user defined directory containing Fasta files\n",
    "#os.chdir(\"/home/mplace/projects/forMatt/Phospho_Network/\")  \n",
    "\n",
    "def CreatePWM():\n",
    "    ''' Function creates PWMs for each Module '''\n",
    "    instances = []\n",
    "    with open('position_weight_matrix.txt', 'w') as out:\n",
    "        out.write('Motif,AA,%s\\n' %(pep_Header))                   \n",
    "        for x in os.listdir('FastaFiles_Modules/'):                 # Iterate through the Fasta files in the directory\n",
    "            if x.endswith('.fasta'):\n",
    "                with open('FastaFiles_Modules/' + x, \"r\") as f:\n",
    "                    for line in f:\n",
    "                        if line.startswith('>'):                                       \n",
    "                            continue\n",
    "                        line = line.rstrip()                                                 \n",
    "                        instances.append(Seq(line, IUPAC.protein))  # add amino acid sequence to instances\n",
    "                    m = motifs.create(instances)\n",
    "                    pwm = m.counts.normalize(pseudocounts = 1)      # Add a +1 pseudocount\n",
    "                    instances = []\n",
    "                    name = re.sub('.fasta', '', x)                  # use file name for 1st column          \n",
    "                    for aa in AminoList :\n",
    "                        score = [ str(i) for i in pwm[aa]]\n",
    "                        score = ','.join(score)\n",
    "                        out.write('%s,%s:,%s\\n' %(name,aa,score))\n",
    "    out.close()\n",
    "                    \n",
    "CreatePWM()\n",
    "\n",
    "print('Done')\n",
    "# OUTPUT: position_weight_matrix.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Kullback-Leibler Module to Each Kinase\n",
    "\n",
    "Purpose:  \n",
    "\n",
    "To quantify similarity between the Mok et. al. kinase PWMs and the module\n",
    "PWMs. Script employs a previously described quantitative motif comparison method\n",
    "called Kullback-Leibler divergence (KLD) (Thijs et al., 2002, Gupta et al., 2007).\n",
    "KLD generates a similarity measure by comparing the Kullback-Leiber distance, or\n",
    "information content, for each amino acid at each position between a query and\n",
    "comparison PWM. The more alike two PWMs are, the closer to zero the score approaches.\n",
    "\n",
    "    KLD(X,Y) = 1/2 (E Xalog(Xa/Ya) + E Yalog(Ya/Xa))\n",
    "\n",
    "Where ‘X’ represents a query PWM position and ‘Y’ a comparison PWM position.\n",
    "Xa indicates the probability of a given amino acid a ε A in X. \n",
    "The symbol ‘A’ represents the length of the motif alphabet, which is 20, \n",
    "representing each of the naturally occurring amino acids. \n",
    "\n",
    "\n",
    "\n",
    "Input:\n",
    "A plain text .csv file that contains all module position weight matrices. Each\n",
    "module PWM should have 20 rows, representing each of the 20 naturally occurring\n",
    "amino acids. They are in a column called \"AA\" which stands for amino acid. There\n",
    "should also be 13 columns, labeled 0-12 (representing the 13 amino acid sequence length\n",
    "of the phospho-peptides used to build the position weight matrix) that contain the\n",
    "frequency of each amino acid at each position.\n",
    "\n",
    "Csv file format\n",
    "Motif,AA,0,1,2,3,4,5,6,7,8,9,10,11,12\n",
    "Induced_...sP.,P:,0.05,0.05,0.03, 0.05,0.05,0.03,0.05,0.05,0.03, 0.05,0.05,0.03\n",
    "\n",
    "In addition, a directory that contains the Mok et al kinase PWMs is required. They have the identical\n",
    "format as above. They have been pre-generated and are available for download on Github.\n",
    "The directory is titled, \"Mok_kinase_PWMs\"\n",
    "\n",
    "Required Parameters: Pandas must be installed on your machine.\n",
    "\n",
    "Output: A directory containing plain text .csv files named after each module (ie.\n",
    "Induced_...sP..txt). Within the .csv files are 63 KLD scores representing how well the\n",
    "63 Mok et al kinases match the module motif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Compare_To=pd.read_csv('position_weight_matrix.txt')   # input module pwm                                                                                    # The PWMs for the Modules.\n",
    "\n",
    "def DF_to_TSV(dataframe, NewFileName): \n",
    "    ''' Function writes out dataframes as TSV files'''\n",
    "    dataframe.to_csv (NewFileName,sep='\\t')  \n",
    "\n",
    "def SplitCompareTOMotifs_df():\n",
    "    ''' Function splits the Compare_To DF by Motif, which is listed in the \"Motif\" column, \n",
    "        and puts the new dataframes into a list\n",
    "    '''\n",
    "    DF_CompareTo_lst =[]\n",
    "    for Motif in Compare_To['Motif'].unique():\n",
    "        DF=Compare_To.loc[Compare_To['Motif']==Motif]\n",
    "        DF_CompareTo_lst.append(DF)\n",
    "    return DF_CompareTo_lst\n",
    "\n",
    "DF_CompareTo_lst=SplitCompareTOMotifs_df()\n",
    "\n",
    "def SplitInput_df_byMotif():\n",
    "    ''' Split the Input dataframe by Motif and create indpendent dataframes'''\n",
    "    DF_Input_lst =[]\n",
    "    for Motif in Input['Motif'].unique():\n",
    "        DF=Input.loc[Input['Motif']==Motif]\n",
    "        DF_Input_lst.append(DF)\n",
    "    return DF_Input_lst\n",
    "\n",
    "def Copy(df):\n",
    "    ''' Function makes a copy of a dataframe.  '''\n",
    "    df=df.copy()\n",
    "    return df\n",
    "\n",
    "def mergeInputMotifFile_withDF_CompareTo(df_Input,df_CompareTo):\n",
    "    ''' Merge the query and comparison PWMs so that KLD can be calculated by comparing column values'''\n",
    "    df_merged=df_Input.merge(df_CompareTo, on='AA')\n",
    "  \n",
    "    return df_merged\n",
    "\n",
    "def Calculate_log_x_y(df):\n",
    "        ''' Function takes the log2 value of the amino acid frequency at each position of the query/comparison motifs'''\n",
    "        df['0_log(x/y)'] = df.apply(lambda x: math.log(x['0_x'],2) - math.log(x['0_y'],2), axis=1)\n",
    "        df['1_log(x/y)'] = df.apply(lambda x: math.log(x['1_x'],2) - math.log(x['1_y'],2), axis=1)\n",
    "        df['2_log(x/y)'] = df.apply(lambda x: math.log(x['2_x'],2) - math.log(x['2_y'],2), axis=1)\n",
    "        df['3_log(x/y)'] = df.apply(lambda x: math.log(x['3_x'],2) - math.log(x['3_y'],2), axis=1)\n",
    "        df['4_log(x/y)'] = df.apply(lambda x: math.log(x['4_x'],2) - math.log(x['4_y'],2), axis=1)\n",
    "        df['5_log(x/y)'] = df.apply(lambda x: math.log(x['5_x'],2) - math.log(x['5_y'],2), axis=1)\n",
    "        df['6_log(x/y)'] = df.apply(lambda x: math.log(x['6_x'],2) - math.log(x['6_y'],2), axis=1)\n",
    "        df['7_log(x/y)'] = df.apply(lambda x: math.log(x['7_x'],2) - math.log(x['7_y'],2), axis=1)\n",
    "        df['8_log(x/y)'] = df.apply(lambda x: math.log(x['8_x'],2) - math.log(x['8_y'],2), axis=1)\n",
    "        df['9_log(x/y)'] = df.apply(lambda x: math.log(x['9_x'],2) - math.log(x['9_y'],2), axis=1)\n",
    "        df['10_log(x/y)'] = df.apply(lambda x: math.log(x['10_x'],2) - math.log(x['10_y'],2), axis=1)\n",
    "        df['11_log(x/y)'] = df.apply(lambda x: math.log(x['11_x'],2) - math.log(x['11_y'],2), axis=1)\n",
    "        df['12_log(x/y)'] = df.apply(lambda x: math.log(x['12_x'],2) - math.log(x['12_y'],2), axis=1)\n",
    "        return df\n",
    "\n",
    "\n",
    "def Calculate_log_y_x(df):\n",
    "        ''' Function takes the log2 value of the amino acid frequency at each position of the comparison/query motifs'''\n",
    "        df['0_log(y/x)'] = df.apply(lambda x: math.log(x['0_y'],2) - math.log(x['0_x'],2), axis=1)\n",
    "        df['1_log(y/x)'] = df.apply(lambda x: math.log(x['1_y'],2) - math.log(x['1_x'],2), axis=1)\n",
    "        df['2_log(y/x)'] = df.apply(lambda x: math.log(x['2_y'],2) - math.log(x['2_x'],2), axis=1)\n",
    "        df['3_log(y/x)'] = df.apply(lambda x: math.log(x['3_y'],2) - math.log(x['3_x'],2), axis=1)\n",
    "        df['4_log(y/x)'] = df.apply(lambda x: math.log(x['4_y'],2) - math.log(x['4_x'],2), axis=1)\n",
    "        df['5_log(y/x)'] = df.apply(lambda x: math.log(x['5_y'],2) - math.log(x['5_x'],2), axis=1)\n",
    "        df['6_log(y/x)'] = df.apply(lambda x: math.log(x['6_y'],2) - math.log(x['6_x'],2), axis=1)\n",
    "        df['7_log(y/x)'] = df.apply(lambda x: math.log(x['7_y'],2) - math.log(x['7_x'],2), axis=1)\n",
    "        df['8_log(y/x)'] = df.apply(lambda x: math.log(x['8_y'],2) - math.log(x['8_x'],2), axis=1)\n",
    "        df['9_log(y/x)'] = df.apply(lambda x: math.log(x['9_y'],2) - math.log(x['9_x'],2), axis=1)\n",
    "        df['10_log(y/x)'] = df.apply(lambda x: math.log(x['10_y'],2) - math.log(x['10_x'],2), axis=1)\n",
    "        df['11_log(y/x)'] = df.apply(lambda x: math.log(x['11_y'],2) - math.log(x['11_x'],2), axis=1)\n",
    "        df['12_log(y/x)'] = df.apply(lambda x: math.log(x['12_y'],2) - math.log(x['12_x'],2), axis=1)\n",
    "        return df\n",
    "\n",
    "def Calculate_Faax_times_log_x_y(df):\n",
    "    ''' Function multiplies the frequency of an amino acid (Faax) \"Xa\" at a specific position in the \n",
    "        query motif against the log(Xa/Ya) for that amino acid. It is calculating this part \n",
    "        of the function  \"Xalog(Xa/Ya)\" '''\n",
    "\n",
    "    df['F(aax)*0_log(x/y)']=df['0_x']*df['0_log(x/y)']\n",
    "    df['F(aax)*1_log(x/y)']=df['1_x']*df['1_log(x/y)']\n",
    "    df['F(aax)*2_log(x/y)']=df['2_x']*df['2_log(x/y)']\n",
    "    df['F(aax)*3_log(x/y)']=df['3_x']*df['3_log(x/y)']\n",
    "    df['F(aax)*4_log(x/y)']=df['4_x']*df['4_log(x/y)']\n",
    "    df['F(aax)*5_log(x/y)']=df['5_x']*df['5_log(x/y)']\n",
    "    df['F(aax)*6_log(x/y)']=df['6_x']*df['6_log(x/y)']\n",
    "    df['F(aax)*7_log(x/y)']=df['7_x']*df['7_log(x/y)']\n",
    "    df['F(aax)*8_log(x/y)']=df['8_x']*df['8_log(x/y)']\n",
    "    df['F(aax)*9_log(x/y)']=df['9_x']*df['9_log(x/y)']\n",
    "    df['F(aax)*10_log(x/y)']=df['10_x']*df['10_log(x/y)']\n",
    "    df['F(aax)*11_log(x/y)']=df['11_x']*df['11_log(x/y)']\n",
    "    df['F(aax)*12_log(x/y)']=df['12_x']*df['12_log(x/y)']\n",
    "    return df\n",
    "\n",
    "def Calculate_Faay_times_log_y_x(df):\n",
    "    ''' Function multiplies the frequency of an amino acid (Faay) \"Ya\" at a specific position in \n",
    "        the query motif against the log(Ya/Xa) for that amino acid. It is calculating this part\n",
    "        of the function  \"Yalog(Ya/Xa)\" '''\n",
    "    \n",
    "    df['F(aay)*0_log(y/x)']=df['0_y']*df['0_log(y/x)']\n",
    "    df['F(aay)*1_log(y/x)']=df['1_y']*df['1_log(y/x)']\n",
    "    df['F(aay)*2_log(y/x)']=df['2_y']*df['2_log(y/x)']\n",
    "    df['F(aay)*3_log(y/x)']=df['3_y']*df['3_log(y/x)']\n",
    "    df['F(aay)*4_log(y/x)']=df['4_y']*df['4_log(y/x)']\n",
    "    df['F(aay)*5_log(y/x)']=df['5_y']*df['5_log(y/x)']\n",
    "    df['F(aay)*6_log(y/x)']=df['6_y']*df['6_log(y/x)']\n",
    "    df['F(aay)*7_log(y/x)']=df['7_y']*df['7_log(y/x)']\n",
    "    df['F(aay)*8_log(y/x)']=df['8_y']*df['8_log(y/x)']\n",
    "    df['F(aay)*9_log(y/x)']=df['9_y']*df['9_log(y/x)']\n",
    "    df['F(aay)*10_log(y/x)']=df['10_y']*df['10_log(y/x)']\n",
    "    df['F(aay)*11_log(y/x)']=df['11_y']*df['11_log(y/x)']\n",
    "    df['F(aay)*12_log(y/x)']=df['12_y']*df['12_log(y/x)']\n",
    "    return df\n",
    "\n",
    "def Column_SUM(df):\n",
    "    ''' Function sums the values calculated by the previous two functions for each position, or column, in the PWMs  '''\n",
    "    df['sum_0']=sum(df['F(aax)*0_log(x/y)'])+sum(df['F(aay)*0_log(y/x)'])\n",
    "    df['sum_1']=sum(df['F(aax)*1_log(x/y)'])+sum(df['F(aay)*1_log(y/x)'])\n",
    "    df['sum_2']=sum(df['F(aax)*2_log(x/y)'])+sum(df['F(aay)*2_log(y/x)'])\n",
    "    df['sum_3']=sum(df['F(aax)*3_log(x/y)'])+sum(df['F(aay)*3_log(y/x)'])\n",
    "    df['sum_4']=sum(df['F(aax)*4_log(x/y)'])+sum(df['F(aay)*4_log(y/x)'])\n",
    "    df['sum_5']=sum(df['F(aax)*5_log(x/y)'])+sum(df['F(aay)*5_log(y/x)'])\n",
    "    df['sum_6']=sum(df['F(aax)*6_log(x/y)'])+sum(df['F(aay)*6_log(y/x)'])\n",
    "    df['sum_7']=sum(df['F(aax)*7_log(x/y)'])+sum(df['F(aay)*7_log(y/x)'])\n",
    "    df['sum_8']=sum(df['F(aax)*8_log(x/y)'])+sum(df['F(aay)*8_log(y/x)'])\n",
    "    df['sum_9']=sum(df['F(aax)*9_log(x/y)'])+sum(df['F(aay)*9_log(y/x)'])\n",
    "    df['sum_10']=sum(df['F(aax)*10_log(x/y)'])+sum(df['F(aay)*10_log(y/x)'])\n",
    "    df['sum_11']=sum(df['F(aax)*11_log(x/y)'])+sum(df['F(aay)*11_log(y/x)'])\n",
    "    df['sum_12']=(sum(df['F(aax)*12_log(x/y)'])+sum(df['F(aay)*12_log(y/x)']))\n",
    "    return df\n",
    "\n",
    "def TotalScore(df):\n",
    "    ''' Function calculates the total score by summing the summed values for each position in the PWM (13 positions)'''\n",
    "    df['FinalScore']=df['sum_0']+df['sum_1']+df['sum_2']+df['sum_3']+df['sum_4']+df['sum_5']+df['sum_6']+df['sum_7']+df['sum_8']+df['sum_9']+df['sum_10']+df['sum_11']+df['sum_12']\n",
    "    Lst=df['FinalScore'].unique()\n",
    "    n=Lst[0]\n",
    "    return n\n",
    "\n",
    "# Import the Mok Kinases PWM .csv files individually and create dataframes\n",
    "path=r\"Mok_kinase_PWMs/\"\n",
    "filenames = glob.glob(path + \"*.csv\")\n",
    "\n",
    "dfs_lst = []\n",
    "for filename in filenames:\n",
    "    dfs_lst.append(pd.read_csv(filename, sep=\",\"))\n",
    "    \n",
    "ITER_NUM=1                                          # One iteration of the below function. \n",
    "dict_Final={}\n",
    "for df2 in dfs_lst:                                 # This is the dataframe that has Module PWMs\n",
    "    \n",
    "    subModule_name=df2['Motif'].unique()\n",
    "    for df in DF_CompareTo_lst:                     # select one of the compare to dataframes (Mok Kinase PWM)\n",
    "        Kinase_name=[]\n",
    "        Kinase_name=df['Motif'].unique()\n",
    " \n",
    "        for iteration in range (ITER_NUM):                                      # for the first iteration \n",
    "\n",
    "            Copied=Copy(df2)                                                    # Copy Dataframe\n",
    "            # Create a merged version of the dataframe for each Kinase PWM and each Module PWM\n",
    "            df_merged=mergeInputMotifFile_withDF_CompareTo(Copied, df)  \n",
    "\n",
    "            DF_1=Calculate_log_x_y(df_merged)                                   \n",
    "            DF_2=Calculate_log_y_x(DF_1)\n",
    "            DF_3=Calculate_Faax_times_log_x_y(DF_2)\n",
    "            DF_4=Calculate_Faay_times_log_y_x(DF_3)\n",
    "            DF_5=Column_SUM(DF_4)\n",
    "        \n",
    "            n=TotalScore(DF_5)                                                  \n",
    "            #print (n)\n",
    "            test_tup = (n, subModule_name[0])\n",
    "            if Kinase_name[0] in dict_Final:\n",
    "                dict_Final[Kinase_name[0]].append(test_tup)\n",
    "            else:\n",
    "                lst=[]\n",
    "                dict_Final[Kinase_name[0]] = lst\n",
    "                dict_Final[Kinase_name[0]].append(test_tup)\n",
    "\n",
    "\n",
    "# write out the final dictionary to a folder where each key and value pair is an independent csv file. \n",
    "if not os.path.exists('ClassA_NoShuffle_KL'):\n",
    "    os.mkdir('ClassA_NoShuffle_KL')\n",
    "    \n",
    "path=\"ClassA_NoShuffle_KL/\"              # this is the path to the folder where the output files will be housed\n",
    "\n",
    "for k, v in dict_Final.items():           # select each key and value pair in the dict \n",
    "    newFile=path+ k +'.csv'               # create newFile, that will have the path and name (the key, which is the kinase) associated with it\n",
    "    #print (newFile)\n",
    "    with open(newFile, 'w') as output:  \n",
    "        output.write(k)\n",
    "        output.write(\"\\n\")\n",
    "        for x in v:\n",
    "           \n",
    "            output.write(str(x))\n",
    "            output.write(\"\\n\")\n",
    "\n",
    "print('KLD complete')\n",
    "# OUTPUT: ClassA_NoShuffle_KL/*.csv    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kullback-Leibler Module to Each Kinase Shuffled 1000x\n",
    "\n",
    "\n",
    "Purpose:  The same algorithm as the last step is used but w/ 1000 Shuffles of the Mok Kinase PWMs are performed by the script, generating randomized PWMs that are compared against the Module PWMs, producing a distribution of scores.\n",
    "\n",
    "Output: A directory containing plain text .csv files named after each module. Within\n",
    "the .csv files are 63,000 KLD scores representing how well the 63 Mok et al kinases\n",
    "match the module motif after 1000 permutations of each Mok kinase.\n",
    "\n",
    "## Currently Iterations set to 1000.\n",
    "\n",
    "\n",
    "## This step can be slow.  For a faster alternative run _Shuffle-kullback-Leibler.py_ manually.  This will allow the user to access multiple cores. \n",
    "\n",
    ">usage: Shuffle_kullback-Leibler.py -f Position_weight_matrix.txt -i 1000 -p 10<br>  \n",
    "\n",
    ">Shuffle kullback-Leibler results for use w/ FDR function.<br>\n",
    "\n",
    ">optional arguments:<br>\n",
    "  -h, --help            show this help message and exit<br>\n",
    "  -f , --file           Position Weight Matrix file, from Matt's script<br>\n",
    "  -i ITER, --iterations Total number of iterations, this will be divided by number of processes.<br>\n",
    "  -p PROC, --processes  Number of processes to run, be smart don't use more than you have!<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Compare_To=pd.read_csv('position_weight_matrix.txt')\n",
    "Input=pd.read_csv('position_weight_matrix.txt')\n",
    "\n",
    "def SplitCompareTOMotifs_df():\n",
    "    ''' Function splits the Compare_To DF by Motif, which is listed in the \"Motif\" column, and puts the new dataframes into a list'''\n",
    "    DF_CompareTo_lst =[]\n",
    "    for Motif in Compare_To['Motif'].unique():\n",
    "        DF=Compare_To.loc[Compare_To['Motif']==Motif]\n",
    "        DF_CompareTo_lst.append(DF)\n",
    "    return DF_CompareTo_lst\n",
    "\n",
    "DF_CompareTo_lst=SplitCompareTOMotifs_df()\n",
    "\n",
    "\n",
    "def SplitInput_df_byMotif():\n",
    "    ''' Split the Input dataframe by Motif and create indpendent dataframes'''\n",
    "    DF_Input_lst =[]\n",
    "    for Motif in Input['Motif'].unique():\n",
    "        DF=Input.loc[Input['Motif']==Motif]\n",
    "        DF_Input_lst.append(DF)\n",
    "    return DF_Input_lst\n",
    "\n",
    "#DF_Input_lst=SplitInput_df_byMotif()\n",
    "#Unique_Input_Motif_Names_lst=Input['Motif'].unique()\n",
    "\n",
    "def Shuffle(df):\n",
    "    '''Shuffle each column by row, after first creating independent dataframes for each column (position within the PWM)''' \n",
    "\n",
    "    df_Index = df[['Motif','AA']]  \n",
    "    df_data_0 = df['0']#,'1','2','3','4','5','7','8','9','10','11','12']]\n",
    "    df_data_1 = df['1']#'2','3','4','5','7','8','9','10','11','12']]\n",
    "    df_data_2 = df['2']\n",
    "    df_data_3 = df['3']\n",
    "    df_data_4 = df['4']\n",
    "    df_data_5 = df['5']\n",
    "    df_data_6 = df['6']\n",
    "    df_data_7 = df['7']\n",
    "    df_data_8 = df['8']\n",
    "    df_data_9 = df['9']\n",
    "    df_data_10 = df['10']\n",
    "    df_data_11 = df['11']\n",
    "    df_data_12 = df['12']\n",
    "    Shuffled_Input_0=df_data_0.iloc[np.random.permutation(len(df_data_0))] \n",
    "    Shuffled_Input_1=df_data_1.iloc[np.random.permutation(len(df_data_1))]                                  # Shuffle the data by row\n",
    "    Shuffled_Input_2=df_data_2.iloc[np.random.permutation(len(df_data_2))] \n",
    "    Shuffled_Input_3=df_data_3.iloc[np.random.permutation(len(df_data_3))] \n",
    "    Shuffled_Input_4=df_data_4.iloc[np.random.permutation(len(df_data_4))] \n",
    "    Shuffled_Input_5=df_data_5.iloc[np.random.permutation(len(df_data_5))] \n",
    "    Shuffled_Input_6=df_data_6.iloc[np.random.permutation(len(df_data_6))] \n",
    "    Shuffled_Input_7=df_data_7.iloc[np.random.permutation(len(df_data_7))] \n",
    "    Shuffled_Input_8=df_data_8.iloc[np.random.permutation(len(df_data_8))]\n",
    "    Shuffled_Input_9=df_data_9.iloc[np.random.permutation(len(df_data_9))] \n",
    "    Shuffled_Input_10=df_data_10.iloc[np.random.permutation(len(df_data_10))] \n",
    "    Shuffled_Input_11=df_data_11.iloc[np.random.permutation(len(df_data_11))] \n",
    "    Shuffled_Input_12=df_data_12.iloc[np.random.permutation(len(df_data_12))] \n",
    "\n",
    "    Shuffled_Input_0_reset_Index= Shuffled_Input_0.reset_index(drop=True)                                   # reset the index for a later merge\n",
    "    Shuffled_Input_1_reset_Index= Shuffled_Input_1.reset_index(drop=True)\n",
    "    Shuffled_Input_2_reset_Index= Shuffled_Input_2.reset_index(drop=True) \n",
    "    Shuffled_Input_3_reset_Index= Shuffled_Input_3.reset_index(drop=True)\n",
    "    Shuffled_Input_4_reset_Index= Shuffled_Input_4.reset_index(drop=True) \n",
    "    Shuffled_Input_5_reset_Index= Shuffled_Input_5.reset_index(drop=True)\n",
    "    Shuffled_Input_6_reset_Index= Shuffled_Input_6.reset_index(drop=True) \n",
    "    Shuffled_Input_7_reset_Index= Shuffled_Input_7.reset_index(drop=True)\n",
    "    Shuffled_Input_8_reset_Index= Shuffled_Input_8.reset_index(drop=True) \n",
    "    Shuffled_Input_9_reset_Index= Shuffled_Input_9.reset_index(drop=True)\n",
    "    Shuffled_Input_10_reset_Index= Shuffled_Input_10.reset_index(drop=True)\n",
    "    Shuffled_Input_11_reset_Index= Shuffled_Input_11.reset_index(drop=True) \n",
    "    Shuffled_Input_12_reset_Index= Shuffled_Input_12.reset_index(drop=True)\n",
    "    \n",
    "    result = pd.concat([df_Index, Shuffled_Input_0_reset_Index, Shuffled_Input_1_reset_Index, Shuffled_Input_2_reset_Index, Shuffled_Input_3_reset_Index,\n",
    "                       Shuffled_Input_4_reset_Index, Shuffled_Input_5_reset_Index, Shuffled_Input_6_reset_Index, Shuffled_Input_7_reset_Index,\n",
    "                       Shuffled_Input_8_reset_Index, Shuffled_Input_9_reset_Index, Shuffled_Input_10_reset_Index, Shuffled_Input_11_reset_Index, Shuffled_Input_12_reset_Index], axis=1) # concatenate the dataframes - they will sit side by side since have the same index (numbering)\n",
    "    result_reordered=result[[ 'Motif', 'AA','0','1', '2','3','4','5','6','7','8','9','10','11','12']]       # reorder the columns so in the correct PWM order.\n",
    "    \n",
    "    result_reordered_Index=result_reordered[['Motif','AA']]\n",
    "    result_reordered_Frame=result_reordered[['0','1', '2','3','4','5','6','7','8','9','10','11','12']]\n",
    "    cols = result_reordered_Frame.columns.tolist()                                                          # send column headers to a list\n",
    "   \n",
    "    random.shuffle(cols)                                                                                    # shuffle the columns, which are in a list by name, and return a different order\n",
    " \n",
    "    \n",
    "    \n",
    "    FinalDF=result_reordered_Frame[cols]                                                                    # make a new dataframe with randomly shuffled columns\n",
    "    FinalDF.columns = ['0','1', '2','3','4','5','6','7','8','9','10','11','12']                             # reset the column names, so that they have the original names\n",
    "    FinalDataframe= pd.concat([result_reordered_Index, FinalDF],axis=1)\n",
    "    return FinalDataframe\n",
    "\n",
    "Shuffled=Shuffle(Input)\n",
    "\n",
    "def mergeInputMotifFile_withDF_CompareTo(df_Input,df_CompareTo):\n",
    "    ''' Merge the query and comparison PWMs so that KLD can be calculated by comparing column values'''\n",
    "    df_merged=df_Input.merge(df_CompareTo, on='AA')\n",
    "    #df_lst.append(df_merged)\n",
    "    return df_merged\n",
    "   \n",
    "df_merged=mergeInputMotifFile_withDF_CompareTo(Shuffled, Compare_To)\n",
    "\n",
    "def Calculate_log_x_y(df):\n",
    "        ''' Function takes the log2 value of the amino acid frequency at each position of the query/comparison motifs'''\n",
    "        df['0_log(x/y)'] = df.apply(lambda x: math.log(x['0_x'],2) - math.log(x['0_y'],2), axis=1)\n",
    "        df['1_log(x/y)'] = df.apply(lambda x: math.log(x['1_x'],2) - math.log(x['1_y'],2), axis=1)\n",
    "        df['2_log(x/y)'] = df.apply(lambda x: math.log(x['2_x'],2) - math.log(x['2_y'],2), axis=1)\n",
    "        df['3_log(x/y)'] = df.apply(lambda x: math.log(x['3_x'],2) - math.log(x['3_y'],2), axis=1)\n",
    "        df['4_log(x/y)'] = df.apply(lambda x: math.log(x['4_x'],2) - math.log(x['4_y'],2), axis=1)\n",
    "        df['5_log(x/y)'] = df.apply(lambda x: math.log(x['5_x'],2) - math.log(x['5_y'],2), axis=1)\n",
    "        df['6_log(x/y)'] = df.apply(lambda x: math.log(x['6_x'],2) - math.log(x['6_y'],2), axis=1)\n",
    "        df['7_log(x/y)'] = df.apply(lambda x: math.log(x['7_x'],2) - math.log(x['7_y'],2), axis=1)\n",
    "        df['8_log(x/y)'] = df.apply(lambda x: math.log(x['8_x'],2) - math.log(x['8_y'],2), axis=1)\n",
    "        df['9_log(x/y)'] = df.apply(lambda x: math.log(x['9_x'],2) - math.log(x['9_y'],2), axis=1)\n",
    "        df['10_log(x/y)'] = df.apply(lambda x: math.log(x['10_x'],2) - math.log(x['10_y'],2), axis=1)\n",
    "        df['11_log(x/y)'] = df.apply(lambda x: math.log(x['11_x'],2) - math.log(x['11_y'],2), axis=1)\n",
    "        df['12_log(x/y)'] = df.apply(lambda x: math.log(x['12_x'],2) - math.log(x['12_y'],2), axis=1)\n",
    "        return df\n",
    "\n",
    "DF_1=Calculate_log_x_y(df_merged)\n",
    "\n",
    "\n",
    "def Calculate_log_y_x(df):\n",
    "        ''' Function takes the log2 value of the amino acid frequency at each position of the comparison/query motifs'''\n",
    "        df['0_log(y/x)'] = df.apply(lambda x: math.log(x['0_y'],2) - math.log(x['0_x'],2), axis=1)\n",
    "        df['1_log(y/x)'] = df.apply(lambda x: math.log(x['1_y'],2) - math.log(x['1_x'],2), axis=1)\n",
    "        df['2_log(y/x)'] = df.apply(lambda x: math.log(x['2_y'],2) - math.log(x['2_x'],2), axis=1)\n",
    "        df['3_log(y/x)'] = df.apply(lambda x: math.log(x['3_y'],2) - math.log(x['3_x'],2), axis=1)\n",
    "        df['4_log(y/x)'] = df.apply(lambda x: math.log(x['4_y'],2) - math.log(x['4_x'],2), axis=1)\n",
    "        df['5_log(y/x)'] = df.apply(lambda x: math.log(x['5_y'],2) - math.log(x['5_x'],2), axis=1)\n",
    "        df['6_log(y/x)'] = df.apply(lambda x: math.log(x['6_y'],2) - math.log(x['6_x'],2), axis=1)\n",
    "        df['7_log(y/x)'] = df.apply(lambda x: math.log(x['7_y'],2) - math.log(x['7_x'],2), axis=1)\n",
    "        df['8_log(y/x)'] = df.apply(lambda x: math.log(x['8_y'],2) - math.log(x['8_x'],2), axis=1)\n",
    "        df['9_log(y/x)'] = df.apply(lambda x: math.log(x['9_y'],2) - math.log(x['9_x'],2), axis=1)\n",
    "        df['10_log(y/x)'] = df.apply(lambda x: math.log(x['10_y'],2) - math.log(x['10_x'],2), axis=1)\n",
    "        df['11_log(y/x)'] = df.apply(lambda x: math.log(x['11_y'],2) - math.log(x['11_x'],2), axis=1)\n",
    "        df['12_log(y/x)'] = df.apply(lambda x: math.log(x['12_y'],2) - math.log(x['12_x'],2), axis=1)\n",
    "        return df\n",
    "\n",
    "def Calculate_Faax_times_log_x_y(df):\n",
    "    ''' Function multiplies the frequency of an amino acid (Faax) \"Xa\" at a specific position in the query motif against the log(Xa/Ya) for that amino acid\n",
    "     It is calculating this part of the function  \"Xalog(Xa/Ya)\" '''\n",
    "    df['F(aax)*0_log(x/y)']=df['0_x']*df['0_log(x/y)']\n",
    "    df['F(aax)*1_log(x/y)']=df['1_x']*df['1_log(x/y)']\n",
    "    df['F(aax)*2_log(x/y)']=df['2_x']*df['2_log(x/y)']\n",
    "    df['F(aax)*3_log(x/y)']=df['3_x']*df['3_log(x/y)']\n",
    "    df['F(aax)*4_log(x/y)']=df['4_x']*df['4_log(x/y)']\n",
    "    df['F(aax)*5_log(x/y)']=df['5_x']*df['5_log(x/y)']\n",
    "    df['F(aax)*6_log(x/y)']=df['6_x']*df['6_log(x/y)']\n",
    "    df['F(aax)*7_log(x/y)']=df['7_x']*df['7_log(x/y)']\n",
    "    df['F(aax)*8_log(x/y)']=df['8_x']*df['8_log(x/y)']\n",
    "    df['F(aax)*9_log(x/y)']=df['9_x']*df['9_log(x/y)']\n",
    "    df['F(aax)*10_log(x/y)']=df['10_x']*df['10_log(x/y)']\n",
    "    df['F(aax)*11_log(x/y)']=df['11_x']*df['11_log(x/y)']\n",
    "    df['F(aax)*12_log(x/y)']=df['12_x']*df['12_log(x/y)']\n",
    "    return df\n",
    "\n",
    "def Calculate_Faay_times_log_y_x(df):\n",
    "    ''' Function multiplies the frequency of an amino acid (Faay) \"Ya\" at a specific position in the query motif against the log(Ya/Xa) for that amino acid\n",
    "     It is calculating this part of the function  \"Yalog(Ya/Xa)\" '''\n",
    "    df['F(aay)*0_log(y/x)']=df['0_y']*df['0_log(y/x)']\n",
    "    df['F(aay)*1_log(y/x)']=df['1_y']*df['1_log(y/x)']\n",
    "    df['F(aay)*2_log(y/x)']=df['2_y']*df['2_log(y/x)']\n",
    "    df['F(aay)*3_log(y/x)']=df['3_y']*df['3_log(y/x)']\n",
    "    df['F(aay)*4_log(y/x)']=df['4_y']*df['4_log(y/x)']\n",
    "    df['F(aay)*5_log(y/x)']=df['5_y']*df['5_log(y/x)']\n",
    "    df['F(aay)*6_log(y/x)']=df['6_y']*df['6_log(y/x)']\n",
    "    df['F(aay)*7_log(y/x)']=df['7_y']*df['7_log(y/x)']\n",
    "    df['F(aay)*8_log(y/x)']=df['8_y']*df['8_log(y/x)']\n",
    "    df['F(aay)*9_log(y/x)']=df['9_y']*df['9_log(y/x)']\n",
    "    df['F(aay)*10_log(y/x)']=df['10_y']*df['10_log(y/x)']\n",
    "    df['F(aay)*11_log(y/x)']=df['11_y']*df['11_log(y/x)']\n",
    "    df['F(aay)*12_log(y/x)']=df['12_y']*df['12_log(y/x)']\n",
    "    return df\n",
    "\n",
    "def Column_SUM(df):\n",
    "    ''' Function sums the values calculated by the previous two functions for each position, or column, in the PWMs  '''\n",
    "    df['sum_0']=sum(df['F(aax)*0_log(x/y)'])+sum(df['F(aay)*0_log(y/x)'])\n",
    "    df['sum_1']=sum(df['F(aax)*1_log(x/y)'])+sum(df['F(aay)*1_log(y/x)'])\n",
    "    df['sum_2']=sum(df['F(aax)*2_log(x/y)'])+sum(df['F(aay)*2_log(y/x)'])\n",
    "    df['sum_3']=sum(df['F(aax)*3_log(x/y)'])+sum(df['F(aay)*3_log(y/x)'])\n",
    "    df['sum_4']=sum(df['F(aax)*4_log(x/y)'])+sum(df['F(aay)*4_log(y/x)'])\n",
    "    df['sum_5']=sum(df['F(aax)*5_log(x/y)'])+sum(df['F(aay)*5_log(y/x)'])\n",
    "    df['sum_6']=sum(df['F(aax)*6_log(x/y)'])+sum(df['F(aay)*6_log(y/x)'])\n",
    "    df['sum_7']=sum(df['F(aax)*7_log(x/y)'])+sum(df['F(aay)*7_log(y/x)'])\n",
    "    df['sum_8']=sum(df['F(aax)*8_log(x/y)'])+sum(df['F(aay)*8_log(y/x)'])\n",
    "    df['sum_9']=sum(df['F(aax)*9_log(x/y)'])+sum(df['F(aay)*9_log(y/x)'])\n",
    "    df['sum_10']=sum(df['F(aax)*10_log(x/y)'])+sum(df['F(aay)*10_log(y/x)'])\n",
    "    df['sum_11']=sum(df['F(aax)*11_log(x/y)'])+sum(df['F(aay)*11_log(y/x)'])\n",
    "    df['sum_12']=(sum(df['F(aax)*12_log(x/y)'])+sum(df['F(aay)*12_log(y/x)']))\n",
    "    return df\n",
    "\n",
    "def TotalScore(df):\n",
    "    ''' Function calculates the total score by summing the summed values for each position in the PWM (13 positions)'''\n",
    "    df['FinalScore']=df['sum_0']+df['sum_1']+df['sum_2']+df['sum_3']+df['sum_4']+df['sum_5']+df['sum_6']+df['sum_7']+df['sum_8']+df['sum_9']+df['sum_10']+df['sum_11']+df['sum_12']\n",
    "    Lst=df['FinalScore'].unique()\n",
    "    n=Lst[0]\n",
    "    return n\n",
    "\n",
    "# Import the Mok Kinases PWM .csv files individually and create dataframes\n",
    "path=r\"Mok_kinase_PWMs/\"\n",
    "filenames = glob.glob(path + \"*.csv\")\n",
    "\n",
    "dfs_lst = []\n",
    "for filename in filenames:\n",
    "    dfs_lst.append(pd.read_csv(filename, sep=\",\"))\n",
    "    \n",
    "# CHANGE THE NUMBER OF ITERATIONS HERE IF DESIRED\n",
    "ITER_NUM=1000                                                                         # 1000 interations of this function\n",
    "dict_Final={}\n",
    "for df2 in dfs_lst:                                                                 # this is the dataframe that has Mok Kinase PWMs\n",
    "    \n",
    "    subModule_name=df2['Motif'].unique()\n",
    "    for df in DF_CompareTo_lst:                                                     # select one of the compare to dataframes (Modules)\n",
    "        Kinase_name=[]\n",
    "        Kinase_name=df['Motif'].unique()\n",
    "   \n",
    "        for iteration in range (ITER_NUM):                                          # for iteration x \n",
    "    \n",
    "            Shuffled=Shuffle(df2)                                                   # shuffle the dataframe by row and column\n",
    "            df_merged=mergeInputMotifFile_withDF_CompareTo(Shuffled, df)   \n",
    "            DF_1=Calculate_log_x_y(df_merged)\n",
    "            DF_2=Calculate_log_y_x(DF_1)\n",
    "            DF_3=Calculate_Faax_times_log_x_y(DF_2)\n",
    "            DF_4=Calculate_Faay_times_log_y_x(DF_3)\n",
    "            DF_5=Column_SUM(DF_4)\n",
    "        \n",
    "            n=TotalScore(DF_5)\n",
    "            #print (n)\n",
    "            test_tup = (n, subModule_name[0])\n",
    "            if Kinase_name[0] in dict_Final:\n",
    "                dict_Final[Kinase_name[0]].append(test_tup)\n",
    "            else:\n",
    "                lst=[]\n",
    "                dict_Final[Kinase_name[0]] = lst\n",
    "                dict_Final[Kinase_name[0]].append(test_tup)\n",
    "\n",
    "# write out the final dictionary to a folder where each key and value pair is an independent csv file. \n",
    "if not os.path.exists('Shuffle_KL'):\n",
    "    os.mkdir('Shuffle_KL')\n",
    "    \n",
    "path=r\"Shuffle_KL/\"  # path to output directory\n",
    "              \n",
    "for k, v in dict_Final.items():  \n",
    "    newFile=path+ k +'.csv'       \n",
    "    #print (newFile)\n",
    "    with open(newFile, 'w') as output:  \n",
    "        output.write(k)\n",
    "        output.write(\"\\n\")\n",
    "        for x in v:\n",
    "            #for subModule_name in filenames:\n",
    "            output.write(str(x))\n",
    "            output.write(\"\\n\")\n",
    "\n",
    "print('Kullback-Leibler Module to Each Kinase Shuffled 1000x complete')\n",
    "\n",
    "# OUTPUT: Shuffle_KL/*.csv \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate FDR Each Module \n",
    "Purpose: Identify FDR scores for each Mok et. al. kinase and each module by comparing\n",
    "the non-shuffled scores to the distribution of shuffled scores. The user can then manually\n",
    "define the FDR cutoff to call kinases \"motif-match\" or \"non-match\" for a given module.\n",
    "\n",
    "Input: Two directories and a single plain text .csv file, called \"Kinases_Not_In_Mok.csv\"\n",
    "that is provided in the required directory available in the git repository. \n",
    "The first directory contains plain text .csv files with KLD scores for non-shuffled Mok \n",
    "et. al. kinases and Modules. The second directory contains plain text .csv files containing\n",
    "KLD scores for shuffled Mok et. al. kinases and Modules.\n",
    "\n",
    "Csv format (For both Input Directories)\n",
    "\n",
    "Scores,Kinase,Module,\n",
    "13.25,cdc15,Induced.sP.\n",
    "\n",
    "Output: A table that contains for each module, all yeast kinases, including those found in\n",
    "the Mok et. al. dataset and those that were absent, and their FDR scores for each module.\n",
    "Kinases not found in the Mok et. al. dataset are given an FDR score of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' This script is calculates an FDR for each Mok Kinase to each Module. \n",
    "The script takes the 63,000 shuffled Mok et. al. kinase-module scores and determines for each kinase where \n",
    "that unshuffled kinase-module score falls in the shuffled distribution.  For example,if a Kinase has an \n",
    "shuffled score of 14.7 to a module and only 63 unshuffled kinase-module scores are below that value, then\n",
    "this kinase has has an FDR of 0.1% (63/63,000 scores).  We can then use the FDR values for all kinases to\n",
    "a module to determine an FDR cutoff for that module. Thus, we can say only these x kinases are a good match\n",
    "to the module. Calling an FDR threshold is done manually by the user.\n",
    "'''\n",
    "\n",
    "#Read in input files (which are the non-shuffled scores for all Mok kinases compared to each Module) \n",
    "path=r\"ClassA_NoShuffle_KL/\"\n",
    "filenames = glob.glob(path + \"*.csv\")\n",
    "\n",
    "labels = ['Scores','Kinase','Module']                # column header\n",
    "\n",
    "def load_data(filenames, dfs_input_lst):\n",
    "    ''' Read in files and parse to produce input data  '''\n",
    "    for i in filenames:\n",
    "        with open(i, 'r') as f:\n",
    "            mod_name = f.readline().rstrip()\n",
    "            for ln in f:\n",
    "                ln = ln.rstrip()\n",
    "                ln = re.sub('\\(', '', re.sub('\\)', '', re.sub('\\'','', re.sub('\\s', '', ln))))\n",
    "                row = (ln + ',' +mod_name).split(',')\n",
    "                dfs_input_lst.append(row)\n",
    "        f.close()\n",
    "    return dfs_input_lst\n",
    "\n",
    "dfs_input_lst = []\n",
    "Input = load_data(filenames, dfs_input_lst)\n",
    "Input = pd.DataFrame(dfs_input_lst, columns=labels)\n",
    "\n",
    "def SplitInput_df_byModule(data):\n",
    "    ''' Function splits the input dataframe, by Module, into independent dataframes'''\n",
    "    DF_Input_lst =[]\n",
    "    for Module in data['Module'].unique():\n",
    "        DF=data.loc[data['Module']==Module]\n",
    "        DF_Input_lst.append(DF)\n",
    "    return DF_Input_lst\n",
    "\n",
    "DF_Input_lst=SplitInput_df_byModule(Input)\n",
    "\n",
    "\n",
    "# Importing the Shuffled Mok kinase-Module Score csv files individually and creating dataframes\n",
    "path=r\"Shuffle_KL/\"\n",
    "filenames = glob.glob(path + \"*.csv\")\n",
    "\n",
    "dfs_lst = []\n",
    "shuffledData = load_data(filenames, dfs_lst)\n",
    "shuffledData = pd.DataFrame(dfs_lst, columns=labels)\n",
    "\n",
    "dfs_lst2= SplitInput_df_byModule(shuffledData)\n",
    "\n",
    "\n",
    "def CountScores_Below():\n",
    "    '''Function is calculating the number of scores in the shuffled distribution below a non-shuffled Kinase_Module score '''\n",
    "    Final_DFs_lst=[]   #\n",
    "    for DF_shuffled in dfs_lst2:  \n",
    "        DF_shuffled=DF_shuffled.copy()\n",
    "       \n",
    "        for df_noShuffle in DF_Input_lst:  \n",
    "            df_noShuffle=df_noShuffle.copy()\n",
    "       \n",
    "            if df_noShuffle['Module'].unique().all() == DF_shuffled['Module'].unique().all():      # if all of the values match in the module column for each df, then and only then, perform the below steps\n",
    "                Input_Score_lst=df_noShuffle['Scores'].tolist()  \n",
    "                \n",
    "                Scores_lst=[]  \n",
    "                for score in Input_Score_lst: \n",
    "                    Scores_lst_individual=[]\n",
    "                    num_smaller_items = (DF_shuffled['Scores']<score).sum()                        # create a variable that is the sum of all scores below the score in the Shuffled_Scores dataframe\n",
    "                 \n",
    "                    Scores_lst_individual.append(num_smaller_items)                                # append the number of scores below a given kinase-module score to the individual list.\n",
    "                    Scores_lst.append(Scores_lst_individual)                                       # append the individual scores to a list.\n",
    "                    merged = list(itertools.chain(*Scores_lst))                                    # Flatten the list of lists. \n",
    "                    \n",
    "                df_noShuffle['Counts_Less_Than']=merged\n",
    "                df_noShuffle['Number_of_Scores']=len(DF_shuffled)                                  # take all of the summed scores, one per kinase from the kinase-module no-shuffle dataframe, and create a new column.\n",
    "                Final_DFs_lst.append(df_noShuffle)  \n",
    "    return Final_DFs_lst\n",
    "\n",
    "DFs_with_CountsBelow_lst=CountScores_Below()\n",
    "   \n",
    "def Calculate_FDR():\n",
    "    ''' Function calculates an FDR value by dividing the number of shuffled scores for a kinase-module\n",
    "    that are smaller than the non-shuffled kinase-module score by all shuffled scores (63,000)  '''\n",
    "    DFs_with_CountsBelow_lst2=[]\n",
    "    for DF in DFs_with_CountsBelow_lst:\n",
    "        DF['FDR']=DF['Counts_Less_Than']/DF['Number_of_Scores']\n",
    "        DF=DF.sort_values(by=['FDR'], ascending=[True])\n",
    "        DFs_with_CountsBelow_lst2.append(DF)\n",
    "    return DFs_with_CountsBelow_lst2\n",
    " \n",
    "DFs_with_CountsBelow_lst2=Calculate_FDR()\n",
    "\n",
    "# Import the kinases not in the Mok et al dataset.\n",
    "Kinases_Not_In_Mok_DF=pd.read_csv('required/Kinases_Not_In_Mok.csv')\n",
    "\n",
    "\n",
    "def ConcatenateDFs_with_Kinases_Not_In_Mok():\n",
    "    '''Function adds the kinases not in the Mok et al dataset to the dataframes for each module'''\n",
    "    DFs_with_CountsBelow_lst3=[]\n",
    "    for DF in DFs_with_CountsBelow_lst2:\n",
    "        DF=DF.copy()\n",
    "        FinalDF=DF.append(Kinases_Not_In_Mok_DF)\n",
    "        DFs_with_CountsBelow_lst3.append(FinalDF)\n",
    "    return DFs_with_CountsBelow_lst3\n",
    "\n",
    "DFs_with_CountsBelow_lst3=ConcatenateDFs_with_Kinases_Not_In_Mok() \n",
    "  \n",
    "\n",
    "def ConcatenateDFs(): \n",
    "    '''Function appends all of the dataframes, for each module, together into one dataframe''' \n",
    "    EmptyDF = pd.DataFrame() \n",
    "    for df in DFs_with_CountsBelow_lst3: \n",
    "        df=df.copy() \n",
    "        EmptyDF=EmptyDF.append(df) \n",
    "    return EmptyDF\n",
    "\n",
    "Final=ConcatenateDFs()\n",
    "\n",
    "def DF_to_CSV(dataframe, NewFileName): \n",
    "    ''' Write out dataframe as a tab separated file.'''\n",
    "    dataframe.to_csv (NewFileName,sep='\\t') \n",
    "    \n",
    "DF_to_CSV(Final, 'FDR_Scores.csv')\n",
    "\n",
    "print('Done')\n",
    "# OUTPUT: FDR_Scores.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Merge_SI_subModule_relationships_with_FDR_Scores\n",
    "\n",
    "Input\n",
    "\n",
    "FDR Scores input file:\n",
    "\n",
    "> Scores,Kinase,Module,Counts_Less_Than,Number_of_Scores,FDR<br>\n",
    "> 11.44097292,pho85-pho80,Induced_......SP.....,0,63000,0<br>\n",
    "> 11.92968433,fus3,Induced_......SP.....,0,63000,0<br>\n",
    "> 11.02325264,cdc28,Induced_......SP.....,0,63000,0<br>\n",
    "\n",
    "\n",
    "SI (Shared Interactors file) Final_enriched.csv from __Identify Shared Interactors__ Step\n",
    "NOTE: the shared interactor name will which is last in the Final_enriched.csv file has to be moved to the\n",
    "start of the line.  This is done in the next cell.\n",
    "\n",
    "> Shared_Interactor,M,Motif_Containing_Proteins,Motif,n,N,m,p-value,Rank(i),m_(number_of_tests),Q_(FDR),>(i/m)Q,BH_significant<br>\n",
    "> YBR160W,304,YKL168C,Induced_......SP....._No_Phenotype_Exists,90,4638,24,1.37E-09,1,894,0.05,5.59E-05,1<br>\n",
    "> YNL293W,16,YLR319C,Induced_......SP....._mkk1_2_Induced_Defective,31,4638,4,2.81E-06,2,894,0.05,0.000111857,1<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prep input files\n",
    "# add group classification according to Mok to FDR_Scores.\n",
    "Mok = dict()\n",
    "with open('required/Mok_Kinase_Groups_Corrected.csv','r') as mk:\n",
    "    for kns in mk:\n",
    "        kns = kns.rstrip()\n",
    "        group = kns.split()\n",
    "        Mok[group[0]] = group[2]\n",
    "mk.close()\n",
    "\n",
    "with open('FDR_Scores.csv', 'r') as f, open('FDR_tmp.csv', 'w') as out:\n",
    "    out.write(f.readline())\n",
    "    for line in f:\n",
    "        dat = line.split()\n",
    "        if dat[3] == 'Not_in_Mok':\n",
    "            out.write(line)\n",
    "        else:\n",
    "            grp = dat[3].split('_')\n",
    "            dat[3] = grp[0] + '_' + grp[1]\n",
    "            row = '\\t'.join(dat) + '\\n'\n",
    "            out.write(row)\n",
    "f.close()\n",
    "out.close()\n",
    "\n",
    "shutil.move('FDR_tmp.csv', 'FDR_Scores.csv')                  # overwrite original fasta file \n",
    "\n",
    "with open('FDR_Scores_merged.csv', 'w') as outfile, open('FDR_Scores.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip()\n",
    "        dat = line.split()\n",
    "        if line.startswith('\\tScores'):               # setup header row\n",
    "            dat.insert(2,'Group (According to Mok)')\n",
    "            out = ','.join(dat) + '\\n'\n",
    "            outfile.write(out)\n",
    "            continue\n",
    "            \n",
    "        if dat[2] in Mok:\n",
    "            dat.insert(3,Mok[dat[2]])\n",
    "            dat.pop(0)                 # removes data frame column number\n",
    "            out = ','.join(dat) + '\\n'\n",
    "            outfile.write(out)\n",
    "        else:\n",
    "            dat.insert(3, '')\n",
    "            dat.pop(0)                 # removes data frame column number\n",
    "            out = ','.join(dat) + '\\n'\n",
    "            outfile.write(out)\n",
    "            \n",
    "outfile.close()\n",
    "f.close()\n",
    "\n",
    "# use FINAL_enriched.csv\n",
    "# Shared_Interactor\tM\tMotif_Containing_Proteins\tInteraction\tMotif\tn\tN\tm\tp-value\tRank(i)\tm_(number_of_tests)\tQ_(FDR)\t(i/m)Q\tBH_significant\n",
    "with open('Final_enriched.csv','r') as f, open('All_SIs.csv','w') as outfile:\n",
    "    for line in f:\n",
    "        dat = line.rstrip().split(',')\n",
    "        if line.startswith('M'):\n",
    "            dat[1] = 'Motif_Containing_Proteins'\n",
    "            dat[3] = 'Motif'\n",
    "        last = dat.pop(-1)\n",
    "        dat.insert(0,last)\n",
    "        out = ','.join(dat) + '\\n'\n",
    "        outfile.write(out)\n",
    "        \n",
    "outfile.close()\n",
    "\n",
    "print('Done')\n",
    "# OUTPUT: FDR_Scores_merged.csv\n",
    "#         All_SIs.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge_SI_subModule_relationships_with_FDR_Scores\n",
    "\n",
    "FDR_Scores_DF input file:\n",
    "> Scores,Kinase,Group (According to Mok),Module,Counts_Less_Than,Number_of_Scores,FDR<br>\n",
    "> 11.44097292,pho85-pho80,Proline_directed,Induced_......SP.....,0,63000,0,<br>\n",
    "> 11.92968433,fus3,Proline_directed,Induced_......SP.....,0,63000,0 <br>\n",
    " \n",
    "SIs_DF input file:\n",
    " > Shared_Interactor,M,Motif_Containing_Proteins,Interaction,Motif,n,N,m,p-value,Rank(i),m_(number_of_tests),Q_(FDR),(i/m)Q\tBH_significant<br>\n",
    ">YBR160W,304,YOR188W,kinase_substrate:Reversed,Induced_......SP.....,117,4638,29,1.65496910697932E-10,1,919,0.05,5.44069640914037E-05\t1\n",
    "\n",
    "\n",
    "Kinase_Names_DF file: ( does not change )\n",
    "\n",
    "> Kinase,Kinase_Pho85_renamed,Kinase_YORF,Mok <br>\n",
    "> yck3,yck3,YER123W,yes <br>\n",
    "> yck1,yck1,YHR135C,yes <br>\n",
    "> yck2,yck2,YNL154C,yes <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' This script takes the output of the KLD FDR script and adds these FDR scores to the identified SI-submodule pairs. \n",
    "'''\n",
    "# FDR_Scores_merged.csv\n",
    "FDR_Scores_DF=pd.read_csv('FDR_Scores_merged.csv') # All_SIs.csv  # All FDR scores for Mok kinase and module PWM comparison\n",
    "# All_SIs.csv\n",
    "SIs_DF=pd.read_csv('All_SIs.csv')  # SIs - All, enriched and not enriched\n",
    "# DO NOT CHANGE\n",
    "Kinase_Names_DF=pd.read_csv('required/Kinases_Mok_andNOT_In_Mok.csv') # Contains all kinases in Mok and not in Mok. Contains common names and YORFs, also contains modifications for Pho85 naming (ex: Pho85-Pcl is now Pho85 in one column)\n",
    "\n",
    "def DF_to_CSV(dataframe, NewFileName):  \n",
    "    dataframe.to_csv (current_dir + '/' + NewFileName,sep=',') \n",
    "####################################################################################################################################\n",
    "''' Merging the FDR_Scores_DF with the Kinase_Names_DF '''\n",
    "# This step is completed so that the correct Pho85 nomenclature is used for a subsquent merge with the SI dataframe. This is necessary because there are 3 pho85-cofactor variants in the Mok et al, dataset.\n",
    "\n",
    "FDR_Scores_DF_merged_left = pd.merge(left=FDR_Scores_DF,right=Kinase_Names_DF, how='left', left_on='Kinase', right_on='Kinase') # completing a merge so that all the kinase nomenclature from the Kinase_Names_DF\n",
    "\n",
    "####################################################################################################################################\n",
    "''' Merge the Kinase name (common name,ex. Hog1) with the Module name to create a new column called \"Candidate_Kinase_Regulators\" '''\n",
    "\n",
    "FDR_Scores_DF_merged_left['Candidate_Kinase_Regulators'] = FDR_Scores_DF_merged_left.Module.map(str) + \"_\" + FDR_Scores_DF_merged_left.Kinase_Pho85_renamed # creating a new column that is the result of a merge between Kinase_Pho85_renamed, and Module\n",
    "#print (FDR_Scores_DF_merged_left.head(1))\n",
    "####################################################################################################################################\n",
    "''' Filtering out non-enriched SIs'''\n",
    "\n",
    "SIs_Filtered=SIs_DF.loc[SIs_DF['BH_significant'] == 1] # Filter out non-significant shared interactors (anything with a \"0\") \n",
    "\n",
    "\n",
    "####################################################################################################################################\n",
    "''' Adding to the SIs file, the \"common\" name for the proteins, rather than just using the YORF designation'''\n",
    "SIs_Filtered_merged_left= pd.merge(left=SIs_Filtered,right=Kinase_Names_DF, how='left', left_on='Shared_Interactor', right_on='Kinase_YORF')\n",
    "\n",
    "\n",
    "####################################################################################################################################\n",
    "''' Splitting 'Motif' column and producing a new column, called \"Module\" that only lists the Induced/Repressed WT phenotype and the motif '''\n",
    "\n",
    "def Split_After_2nd_Occurence_In_A_String_Retaining_Beginning():\n",
    "    lst=[] # create an empty list \n",
    "    for string in SIs_Filtered_merged_left['Motif']: # Select the string from the \"Motif\" column \n",
    "        strip_character =\"_\"  # define character where strip will occur\n",
    "        lst.append(strip_character.join(string.split(strip_character)[:2])) # append to the list the text before the second occurence of the character \"_\"\n",
    "    Series_Object = pd.Series(lst) # put the list into a series \n",
    "    SIs_Filtered_merged_left['Module'] = Series_Object.values # append the series values to the already existing DF in a new column\n",
    "    return SIs_Filtered_merged_left\n",
    "        \n",
    "SIs_Filtered_merged_left_String_Split=Split_After_2nd_Occurence_In_A_String_Retaining_Beginning()\n",
    "#print (SIs_Filtered_merged_left_String_Split)\n",
    "\n",
    "####################################################################################################################################    \n",
    "####################################################################################################################################    \n",
    "'''Creating New Columns that can be used for a merge'''\n",
    "SIs_Filtered_merged_left_String_Split['Kinase_subModules'] = SIs_Filtered_merged_left_String_Split.Motif.map(str) + \"_\" + SIs_Filtered_merged_left_String_Split.Kinase_Pho85_renamed\n",
    "\n",
    "\n",
    "SIs_Filtered_merged_left_String_Split['Kinase_Modules'] = SIs_Filtered_merged_left_String_Split.Module.map(str) + \"_\" + SIs_Filtered_merged_left_String_Split.Kinase_Pho85_renamed\n",
    "#print(SIs_Filtered_merged_left_String_Split.head(4))\n",
    "#################################################################################################################################### \n",
    "''' Perform a merge where of the FDR Scores Dataframe with the SIs_Filtered_merged_left_String_Split DF. \n",
    "This will reveal if a kinase-Module relationship, from the FDR Score Dataframe, which contains all possible Kinase-Module relationships, exist in the users \n",
    "kinase-subModule file (so the SIs file)'''\n",
    "    \n",
    "merged_left= pd.merge(left=FDR_Scores_DF_merged_left,right=SIs_Filtered_merged_left_String_Split, how='left', left_on='Candidate_Kinase_Regulators', right_on='Kinase_Modules')\n",
    "\n",
    "#################################################################################################################################### \n",
    "''' Drop columns that are not needed or redundant '''\n",
    "\n",
    "merged_left=merged_left[['Scores', 'Kinase_x', 'Module_x', 'Candidate_Kinase_Regulators', 'Counts_Less_Than', 'Number_of_Scores', 'FDR', 'Kinase_subModules']]\n",
    "\n",
    "#################################################################################################################################### \n",
    "''' Drop NaN values '''\n",
    "merged_left=merged_left.dropna(subset=['Kinase_subModules']) # Drop the NaN values, so that the dataframe only contains Kinases that were connected to subModules.\n",
    "#print(merged_left.columns)\n",
    "####################################################################################################################################\n",
    "''' Drop any duplicates that occur in TWO columns - this is done only because of Pho85 being listed 3 times (because of it's co-factor interactions) and that affects the merge'''\n",
    "merged_left=merged_left.drop_duplicates(subset=['Kinase_x', 'Kinase_subModules']) # only drop duplicates that are found in BOTH columns\n",
    "\n",
    "DF_to_CSV(merged_left, 'All_FDR_Scores_and_their_Kinase_SI_subModules.csv')\n",
    "\n",
    "print('DONE')\n",
    "# OUTPUT: All_FDR_Scores_and_their_Kinase_SI_subModules.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Kullback_Leibler Shuffle results\n",
    "\n",
    "Input file:\n",
    "\n",
    "> Scores,Kinase_x,Module_x,Candidate_Kinase_Regulators,Counts_Less_Than,Number_of_Scores,FDR,Kinase_subModules <br>\n",
    "> 11.02325264,cdc28,Induced_......SP.....,Induced_......SP....._cdc28,0,63000,0,Induced_......SP....._No_Phenotype_Exists_cdc28\n",
    "> 11.02325264,cdc28,Induced_......SP.....,Induced_......SP....._cdc28,0,63000,0,Induced_......SP....._mkk1_2_Induced_Defective_cdc28\n",
    "> 12.51579799,slt2,Induced_......SP.....,Induced_......SP....._slt2,1,63000,1.59E-05, Induced_......SP....._mkk1_2_Induced_Defective_slt2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "''' This is a quick script that cleans up the Output of the Kullback_Leibler Shuffle Script.\n",
    "It removes unwanted names that trail the subModule name-these are leftovers from a previous\n",
    "script. It then also sorts each subModule by ascending for the SI scores\n",
    "'''\n",
    "# Input=pd.read_csv('All_DTT_T120_Kinase_Module_FDR_Scores_and_their_Kinase_SI_subModules_Sept2017.csv')\n",
    "Input=pd.read_csv('All_FDR_Scores_and_their_Kinase_SI_subModules.csv')\n",
    "#print (Input.dtypes)\n",
    "\n",
    "# Remove the last occurrence of a character, and the text that follows\n",
    "def Remove_Text_After_Last_Occurence_of_Character():\n",
    "    Value_lst=[]\n",
    "    for value in Input['Kinase_subModules']:\n",
    "        value=\"_\".join(value.split(\"_\")[:-1]) # return everything minus the last occurrence of the \"_\" and what trailed\n",
    "        Value_lst.append(value)\n",
    "    Input['Kinase_subModules']=Value_lst\n",
    "        #sep = '_'\n",
    "        #value = value.split(sep, 5)[-1]\n",
    "    return (Input)\n",
    "        \n",
    "        \n",
    "Input=Remove_Text_After_Last_Occurence_of_Character()\n",
    "#print (Input)\n",
    "\n",
    "#Split the dataframe into separate dataframes by the subModule name\n",
    "def SplitInput_df_by_subModule():\n",
    "    DF_Input_lst =[]\n",
    "    for subModule in Input['Kinase_subModules'].unique():\n",
    "        DF=Input.loc[Input['Kinase_subModules']==subModule]\n",
    "        DF_Input_lst.append(DF)\n",
    "    return DF_Input_lst\n",
    "\n",
    "DF_Input_lst=SplitInput_df_by_subModule()\n",
    "\n",
    "#Sort each dataframe within the list of dataframes by ascending for the FDR column\n",
    "def Sort_by_Ascending():\n",
    "    DF_Input_lst2=[]\n",
    "    for DF in DF_Input_lst:\n",
    "        DF=DF.copy()\n",
    "        DF=DF.sort_values(by=['FDR'], ascending=[True])\n",
    "        DF_Input_lst2.append(DF)\n",
    "    return DF_Input_lst2\n",
    "\n",
    "DF_Input_lst2=Sort_by_Ascending()\n",
    "\n",
    "\n",
    "\n",
    "#Concatenate the dataframes back together into one so they can be printed out as a single dataframe.\n",
    "def ConcatenateDFs():    #Concatenate the DFs together \n",
    "    EmptyDF = pd.DataFrame() # create an empty dataframe\n",
    "    for df in DF_Input_lst2:  # select a dataframe in the list \n",
    "        df=df.copy() # make a copy of that dataframe \n",
    "        EmptyDF=EmptyDF.append(df) # append to the empty DF the dataframe selected and overwrite the empty dataframe\n",
    "    return EmptyDF\n",
    "\n",
    "Final=ConcatenateDFs()\n",
    "#print (Final)\n",
    "\n",
    "\n",
    "def DF_to_CSV(dataframe, NewFileName): \n",
    "    dataframe.to_csv (NewFileName,sep='\\t')\n",
    "    \n",
    "DF_to_CSV(Final, 'All_Module_FDR_Scores__Kinase_SI_subModules_Sorted.csv')\n",
    "\n",
    "# OUTPUT: All_Module_FDR_Scores__Kinase_SI_subModules_Sorted.csv\n",
    "\n",
    "# clean up All_SIs.csv from Prep Merge_SI_subModule_relationships_with_FDR_Scores step\n",
    "with open('All_SIs.csv','r') as f, open('only_enriched.csv','w') as outfile :\n",
    "    for line in f:\n",
    "        dat = line.rstrip().split(',')\n",
    "        if line.startswith('Shared'):\n",
    "            dat.pop(4)\n",
    "            dat[3] = 'subModule'\n",
    "        else:\n",
    "            dat.pop(3)            \n",
    "        out = ','.join(dat) + '\\n'\n",
    "        outfile.write(out)\n",
    "\n",
    "f.close()\n",
    "outfile.close()\n",
    "        \n",
    "# OUTPUT: only_enriched.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remake the header for SI_Identification_SubmoduleS__SIs_and_Targets_FDR.csv (from Identify Shared Interactors step)\n",
    "# for the next script\n",
    "with open('SI_Identification_SubmoduleS__SIs_and_Targets_FDR.csv', 'r') as f, open('shared_Interactors.csv', 'w') as out:\n",
    "    f.readline()\n",
    "    newHeader = 'Motif_Containing_Proteins,Interaction,subModule,n,Possible_Shared_Interactors\\n'\n",
    "    out.write(newHeader)\n",
    "    for line in f:\n",
    "        out.write(line)\n",
    "f.close()\n",
    "out.close()    \n",
    "\n",
    "# OUTPUT: shared_Interactors.csv     \n",
    "\n",
    "''' \n",
    "The purpose of this script is to identify for each enriched SI (according to the hypergeometric and BH correction)\n",
    "what it's interacting proteins, and potential \"targets\"  are in the background network. Next, \n",
    "the script identifies the correct orientation for interactions between a Shared Interactor and it's \n",
    "partners, which are reversed in an earlier script. Thus, the script ensures all interactions are in \n",
    "the correct orientation. \n",
    "\n",
    "The output file from this script can then be used to determine if a Shared Interactor is acting as an input (all interactions going towards a subModule, or is an output (interactions go away), etc. An input is a potential regulator of a submodule whereas an output is unlikely to regulate a submodoule.\n",
    "'''\n",
    "##################################################################################################################################\n",
    "# FDR_Scores_merged.csv\n",
    "#Enriched_SIs_subModules_Only_Sig=pd.read_csv('DTT_T120_SIs_All_Sept2017_Enriched.csv')  # Import ONLY enriched SIs (drop non-enriched SIs)\n",
    "Enriched_SIs_subModules_Only_Sig=pd.read_csv('only_enriched.csv')  \n",
    "Enriched_SIs_subModules_Only_Sig[\"SI_subModule\"] = Enriched_SIs_subModules_Only_Sig[\"Shared_Interactor\"].map(str) + \"_\" + Enriched_SIs_subModules_Only_Sig[\"subModule\"] # creating a new column by merging two columns together.\n",
    "\n",
    "\n",
    "# I think this is from SI_Identification_SubmoduleS__SIs_and_Targets_FDR.csv  from step Identify Shared Interactors\n",
    "#All_enriched_and_not_SIs_andTargets=pd.read_csv('SI_Identification_T120_DTT_Sept2017_T120_Possible_SIs_and_Targets_Dashes_Removed_4638_proteins_BOTH_Reps_Ppeps_NORMALIZED.csv') # Import all identified Shared Interactors (both enriched and not) and their interacting partners from submodules.\n",
    "All_enriched_and_not_SIs_andTargets=pd.read_csv('shared_Interactors.csv')\n",
    "\n",
    "All_enriched_and_not_SIs_andTargets[\"SI_subModule\"] = All_enriched_and_not_SIs_andTargets[\"Possible_Shared_Interactors\"].map(str) + \"_\" + All_enriched_and_not_SIs_andTargets[\"subModule\"]\n",
    "\n",
    "Merged_left = pd.merge(left=Enriched_SIs_subModules_Only_Sig,right=All_enriched_and_not_SIs_andTargets, how='left', left_on='SI_subModule', right_on='SI_subModule') # \n",
    "\n",
    "Merged_left_FINAL=Merged_left[['SI_subModule', 'Shared_Interactor', 'Motif_Containing_Proteins_y', 'subModule_y']] # retain these columns only\n",
    "Merged_left_FINAL.columns=['SI_Module','Shared_Interactor', 'Motif_Containing_Proteins', 'subModule_Name'] # rename columns\n",
    "\n",
    "\n",
    "\n",
    "def DF_to_CSV(dataframe, NewFileName): \n",
    "    dataframe.to_csv (NewFileName,sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "''' Add the SI common name to the file  FILE DOES NOT CHANGE'''\n",
    "Annotation_File_DF=pd.read_csv('required/Annotation_file_dashes_remain_No_duplicate_Common_names.csv') \n",
    "\n",
    "Merged_left_Again = pd.merge(left=Merged_left_FINAL,right=Annotation_File_DF, how='left', left_on='Shared_Interactor', right_on='Protein_Name') # complete the merge toget the common names\n",
    "Merged_left_Again=Merged_left_Again[['SI_Module', 'Shared_Interactor', 'Common_Name', 'Motif_Containing_Proteins', 'subModule_Name']] # retain only these columns\n",
    "Merged_left_Again.columns=['SI_Module', 'Shared_Interactor', 'SI_Name', 'Motif_Containing_Proteins', 'subModule_Name'] # rename columns \n",
    "\n",
    "Merged_left_Again['Protein1:Protein2']=Merged_left_Again['Shared_Interactor'].map(str) + \":\" + Merged_left_Again['Motif_Containing_Proteins'] # adding column for merge section below.\n",
    "\n",
    "Merged_left_Again['Protein1:Protein2'] = Merged_left_Again['Protein1:Protein2'].str.replace('-', '') # Removing the dashes from the names because if they remain in this column, the merge below will fail, because the background network lacks dashes in the gene annotations. \n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "'''FILE DOES NOT CHANGE from Debbi Chasman'''\n",
    "Background_network_correct_Orientation=pd.read_csv('required/phospho_v4_bgnet_siflike_withdirections_fix_Matt_Modifications_ForPipeline.csv') # import the salt background network with the correct orientations (this file lacks dashes in gene annotations!)\n",
    "\n",
    "Merged_left_Again_Get_Correct_Protein_Orientiations=pd.merge(left=Merged_left_Again, right=Background_network_correct_Orientation, how='left', left_on='Protein1:Protein2', right_on='Protein1:Protein2') # merge based on the columns to the left\n",
    "\n",
    "Merged_left_Again_Get_Correct_Protein_Orientiations=Merged_left_Again_Get_Correct_Protein_Orientiations[['SI_Module', 'Shared_Interactor', 'SI_Name', 'Motif_Containing_Proteins', 'subModule_Name','Interaction']] # retain only these columns\n",
    "\n",
    "\n",
    "Merged_left_Again_Get_Correct_Protein_Orientiations.columns=['SI_Module', 'Shared_Interactor', 'SI_name', 'Motif_Containing_Proteins', 'subModule_Name','Interaction_Directionality'] # rename columns \n",
    "#print (Merged_left_Again_Get_Correct_Protein_Orientiations.head(2))\n",
    "\n",
    "DF_to_CSV(Merged_left_Again_Get_Correct_Protein_Orientiations, 'Orientation_Script.csv')\n",
    "\n",
    "# OUTPUT:  Orientation_Script.csv  this one of the inputs to the next script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' \n",
    "The function of this script is as follows: For each SI and it's interactions a submodules constituents,\n",
    "the script determines if the SI is a likely submodule regulator, the Shared Interactor has at least 1 \n",
    "directional interaction, or ppi interaction, with a subModule protein aimed from the SI to the submodule,\n",
    "or if the subModule proteins are act upon the SI , all interactions between the SI and subModule proteins\n",
    "have the 'Reverse' designation', indicating that the subModule proteins act upon the SI.  \n",
    "\n",
    "-If all of the interactions are reversed, then the script will define the relationship between the SI and the subModule\n",
    "as \"Output\", indicating that the SI is likely downstream of the submodule and is not likely regulating the submodule.\n",
    "\n",
    "-If there is at least one interaction that is NOT reverse (ie, kinase-substrate) or is and non-directed ppi, \n",
    "the relationship between the SI and the subMOdule is defined as \"Input\", suggesting there is a possibility \n",
    "that the SI can regulate the submodule protein phosphorylation state.\n",
    "\n",
    "\n",
    "This script takes an input file that contains the following:\n",
    "- All enriched Shared Interactors (SIs) (according to HyperG) and their connections to subModules.\n",
    "- All known protein interactions for each SI (ppi, kinase-substrate, etc)\n",
    "- Many of these interactions are directed (kinase-substrate, metabolic pathway, etc). PPI are not a directed interaction.\n",
    "\n",
    "'''\n",
    "Input_df=pd.read_csv('Orientation_Script.csv', delimiter='\\t')\n",
    "#Input_df=pd.read_csv('DTT_T120_Prep_for_Orientation_Script_Sept2017.csv', delimiter='\\t')\n",
    "#print(Input_df.head(n=5))\n",
    "\n",
    "# Split the input DF into independent DFs based on the term in the SI_Module column (this columns contains the SI and it's connection to each subModule). \n",
    "def Split_based_on_SI_Module_Column():\n",
    "    DF_lst =[]\n",
    "    for SI_Module in Input_df['SI_Module'].unique():\n",
    "        DF=Input_df.loc[Input_df['SI_Module']==SI_Module]\n",
    "        DF_lst.append(DF)\n",
    "    return DF_lst\n",
    "\n",
    "DF_lst=Split_based_on_SI_Module_Column()\n",
    "\n",
    "# This function counts, for each DF, how many of the interactions are reversed. It also counts the length of the dataframe, and then\n",
    "# subtracts the the length of the dataframe from the counts. If the resultant value is 0, then all of the interactions were reversed.\n",
    "def Count_Instances_of_Reverse_Interaction():\n",
    "    DF_Counts_lst=[]\n",
    "    for df in DF_lst:\n",
    "        df=df.copy()\n",
    "        df['Counts']=df.Interaction_Directionality.str.contains('Reversed').sum()  # Count the number of interactions that are \"Reversed\"\n",
    "        x=len(df)\n",
    "        df['Length']=x\n",
    "        df['Counts_Length']=df['Counts']-df['Length']\n",
    "        \n",
    "        DF_Counts_lst.append(df)\n",
    "    return DF_Counts_lst\n",
    "\n",
    "DF_Counts_lst=Count_Instances_of_Reverse_Interaction()\n",
    "\n",
    "# This function assigns 'Input' and 'Output' classifications based on the 'Counts_Length' column in the dataframe. \n",
    "def Only_Reverse_Interactions_Move_to_Outgoing_Columns():\n",
    "    df_Modified_Outgoing_lst=[]\n",
    "    for df in DF_Counts_lst:\n",
    "        #print (df.dtypes)\n",
    "        for value in df['Counts_Length'].unique():\n",
    "            #print (value)\n",
    "            if value == 0:\n",
    "                df['Shared_Interactor_subModule_Relationship']= 'Output'\n",
    "                df_Modified_Outgoing_lst.append(df)\n",
    "            else:\n",
    "                df['Shared_Interactor_subModule_Relationship']= 'Input'\n",
    "                df_Modified_Outgoing_lst.append(df)\n",
    "                \n",
    "    return df_Modified_Outgoing_lst\n",
    "    \n",
    "df_Modified_Outgoing_lst=Only_Reverse_Interactions_Move_to_Outgoing_Columns()\n",
    "\n",
    "# This function concatenates the dataframes back together, leaving a single DF. \n",
    "def ConcatenateDFs():   \n",
    "    EmptyDF = pd.DataFrame() # create an empty dataframe\n",
    "    for df in df_Modified_Outgoing_lst:  # select a dataframe in the list \n",
    "        df=df.copy() # make a copy of that dataframe \n",
    "        EmptyDF=EmptyDF.append(df) # append to the empty DF the dataframe selected and overwrite the empty dataframe\n",
    "    return EmptyDF\n",
    "\n",
    "Final=ConcatenateDFs()\n",
    "\n",
    "#print (Final)\n",
    "\n",
    "#Function writes out a Dataframe to a CSV file. \n",
    "def DF_to_CSV(dataframe, NewFileName): \n",
    "    dataframe.to_csv (NewFileName,sep='\\t') \n",
    "    \n",
    "\n",
    "Final_Keep_Columns_Needed_For_SIF=Final[['SI_Module', 'Shared_Interactor', 'subModule_Name', 'Shared_Interactor_subModule_Relationship']] \n",
    "Final_Keep_Columns_Needed_For_SIF=Final_Keep_Columns_Needed_For_SIF.drop_duplicates('SI_Module')\n",
    "\n",
    "\n",
    "DF_to_CSV(Final_Keep_Columns_Needed_For_SIF, 'SIs_subModule_Relationships_Defined_for_making_SIF.csv')\n",
    " \n",
    "print('Done')\n",
    "# OUTPUT: SIs_subModule_Relationships_Defined_for_making_SIF.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prep Modules_pPep.csv for next step\n",
    "with open('Modules_pPep.csv','r') as f, open('submodule_constituents-SIF.csv', 'w') as out:\n",
    "    for line in f:\n",
    "        dat = line.rstrip().split()\n",
    "        if line.startswith('Ppep'):\n",
    "            dat.insert(1,'Protein')\n",
    "            header = ','.join(dat) + '\\n'\n",
    "            out.write(header)\n",
    "        else:\n",
    "            protein = dat[0].split('_')\n",
    "            dat.insert(1,protein[0])\n",
    "            row = ','.join(dat) + '\\n'\n",
    "            out.write(row)\n",
    "\n",
    "f.close()\n",
    "out.close()        \n",
    "\n",
    "# OUTPUT: submodule_constituents-SIF.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prep FDR_Scores_merged.csv for next step\n",
    "kinase = dict()\n",
    "with open('required/Kinases_Mok_andNOT_In_Mok.csv') as k:  #load a dictonary w/ kinase and gene names\n",
    "    for kin in k:\n",
    "        dat = kin.split(',')\n",
    "        if kin.startswith('Kinase'):\n",
    "            continue\n",
    "        kinase[dat[0]] = dat[2]\n",
    "k.close()\n",
    "\n",
    "\n",
    "module = ''\n",
    "with open('FDR_Scores_merged.csv', 'r') as f, open('MotifMatch.csv','w') as out:\n",
    "    header ='Kinase_Module,Scores,Kinase,Name,Group (According to Mok),Module,Counts_Less_Than,Number_of_Scores,FDR,motif_match\\n'\n",
    "    out.write(header)\n",
    "    for line in f:\n",
    "        if line.startswith('Scores'):\n",
    "            continue\n",
    "        dat = line.rstrip().split(',')\n",
    "        if dat[3] != 'Not_in_Mok':\n",
    "            if dat[1] in kinase:\n",
    "                dat.insert(2, kinase[dat[1]])\n",
    "                module = dat[4].split('_')\n",
    "                kinaseModule = kinase[dat[1]] + '_' +  dat[4]\n",
    "                dat.insert(0, kinaseModule)\n",
    "                out.write(','.join(dat) + '\\n')\n",
    "        else:\n",
    "            if dat[1] in kinase:\n",
    "                dat.insert(2, kinase[dat[1]])\n",
    "                kinaseModule = kinase[dat[1]] + '_' + module[1]\n",
    "                dat.insert(0, kinaseModule)\n",
    "                dat[5] = '_'.join(module)\n",
    "                out.write(','.join(dat) + '\\n')\n",
    "            else:\n",
    "                print(dat)\n",
    "\n",
    "f.close()\n",
    "out.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "''' The function of this script is to produce a SIF file that Debbie can use to Infer\n",
    "a signaling network using an ILP progamming method. The output SIF file can also be opended with \n",
    "Cytoscape to view a signaling network that has NOT been inferred.  \n",
    "\n",
    "***Overview/Important Notes***\n",
    "-The final SIF file indicates directionality between interactions or information flow between entitites.\n",
    "For example, the protein in column \"A\" acts upon the protein/subModule in column \"B\".\n",
    "The submodule in column \"A\" contains the proteins listed in column \"B\"\n",
    "\n",
    "-There are 6 Interaction types in this file, listed below:\n",
    "\n",
    "A) Motif-matched: This is a Kinase-SI that recognizes the phosphorylation motif for a subModule at a\n",
    "predetermined FDR cutoff. This interaction type only exists for kinases that are Input for a subModule\n",
    "(and thus can potentially regulate them). If a kinase is a match to a subModule, but is an output,\n",
    "it is unlikely to regulate that subModule, since\n",
    "it is downstream of the subModule, and would be considered an Output (see below).\n",
    "\n",
    "B) Unknown-recognition-motif: A Kinase or Phoshphatase SI for which we have no information about the\n",
    "phosphorylation motif it recognizes (ie, not in the Mok et al Dataset)This interaction type is input only \n",
    "(Kinase/Phosphatase SI -> subModule) All of our SI-Phosphatase inputs fall into this group, since we \n",
    "do not know their recognized phosphorylation motif.\n",
    " \n",
    "C) Motif-unmatched: A Kinase SI which did NOT meet our FDR cutoff for a subModule.\n",
    "This is also only for Input Kinases.\n",
    "\n",
    "D) Output: subModule -> Kinase/Phosphatase SI. Key here is that all subModule-SI interactions face TOWARDS \n",
    "the SI, and thus the SI is an output and unlikely to regulate the submodule.\n",
    "\n",
    "E) Constituent: A subModule to it's protein constituents (proteins that are part of the subModule). For this group,\n",
    "many of the constituent proteins will NOT be SIs.\n",
    "   \n",
    "E) Shared_Interaction: Non-Kinase/Phosphatase SI connected to it's subModule. Can be either SI -> subModule or \n",
    "subModule -> SI (so an Input/Output based on interaction directionality)\n",
    "\n",
    "\n",
    "Files that will always be used to create the SIF file:\n",
    "# FILES DO NOT CHANGE\n",
    "-Annotation_kinases.csv = this file contains all kinases, including which kinases are in the Mok Dataset.\n",
    "-kinase_phosphatase_yeast.csv = this file contains all kinases and phosphatases in yeast\n",
    "(annotated as kinase/phosphatase catalytic-from Mike Tyers Kinome project)\n",
    "\n",
    "# User defined\n",
    "-List of enriched SIs and their subModules\n",
    "-List of subModules and their protein constituents \n",
    "-KL scoring system for kinases to their subModules (which is actually based on comparing Kinases to their Modules).\n",
    "\n",
    "\n",
    "'''\n",
    "# from previous step\n",
    "# All_SI_subModule_relationships_DF= pd.read_csv('SIs_subModule_Relationships_Defined_DTT_T120_Network_Input_for_making_SIF_Sept2017.csv') \n",
    "All_SI_subModule_relationships_DF= pd.read_csv('SIs_subModule_Relationships_Defined_for_making_SIF.csv', delimiter='\\t')\n",
    "\n",
    "\n",
    "subModule_Constituent_Proteins_DF= pd.read_csv('submodule_constituents-SIF.csv')\n",
    "#subModule_Constituent_Proteins_DF= pd.read_csv('DTT_submodule_constituents_Sept2017.csv')\n",
    "\n",
    "# File does not change\n",
    "#DF contains all kinases, including the 3 Pho85-Co-activator varieties, and includes whether a kinase is in the Mok dataset or not.\n",
    "Annotation_Kinases_DF=pd.read_csv('required/Annotation_kinases_Updated_Correct.csv') \n",
    "\n",
    "# File does not change\n",
    "#DF contains all protein annotated as kinase catalytic or phosphatase catalytic from Mike Tyers Kinome project_base\n",
    "Kinases_Phosphatases_yeast_DF=pd.read_csv('required/kinase_phosphatase_yeast.csv')\n",
    "\n",
    "KL_Matching_Kinases_Modules_DF=pd.read_csv('MotifMatch.csv')\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "def DF_to_CSV(dataframe, NewFileName): \n",
    "    dataframe.to_csv (NewFileName,sep='\\t')  \n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# Add a column to the SI_File\n",
    "All_SI_subModule_relationships_DF['SI']='Yes'\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# Generating Interaction type: Output (subModule -> subModule constituent proteins)\n",
    "\n",
    "subModule_Constituent_Proteins_DF=subModule_Constituent_Proteins_DF[['Protein', 'subModule']]  # Only retain the listed columns in the dataframe\n",
    "subModule_Constituent_Proteins_DF['Protein_Constituent_subModule']=subModule_Constituent_Proteins_DF.Protein.map(str) + \"_\" + subModule_Constituent_Proteins_DF.subModule  # create a new column that merges the protein name and subModule name together\n",
    "subModule_Constituent_Proteins_DF_dupes_removed=subModule_Constituent_Proteins_DF.drop_duplicates(subset='Protein_Constituent_subModule') # drop duplicates, so only a single occurrence is listed for each SI-subModule\n",
    "subModule_Constituent_Proteins_DF_dupes_removed_reorganized=subModule_Constituent_Proteins_DF_dupes_removed.rename(columns={'subModule':'Interactor_A', 'Protein':'Interactor_B'}) # Change column names\n",
    "subModule_Constituent_Proteins_DF_dupes_removed_reorganized['Edge_Type']='Constituent'  # Add the Edge type (Interaction type-column). This line has been modified since the original script was created.\n",
    "subModule_Constituent_Proteins_DF_dupes_removed_reorganized=subModule_Constituent_Proteins_DF_dupes_removed_reorganized[['Interactor_A','Edge_Type','Interactor_B','Protein_Constituent_subModule']] # reorganize columns\n",
    "subModule_Constituent_Proteins_merged_left_DF=pd.merge(left=subModule_Constituent_Proteins_DF_dupes_removed_reorganized,right=Kinases_Phosphatases_yeast_DF, how='left', left_on='Interactor_B', right_on='ORF')  # do a merge, so I get information about which proteins are kinases/phosphatases\n",
    "subModule_Constituent_Proteins_merged_left_DF=subModule_Constituent_Proteins_merged_left_DF[['Interactor_A', 'Edge_Type', 'Interactor_B', 'Annotation', 'Protein_Constituent_subModule']]  # drop the columns I don't want \n",
    "subModule_Constituent_Proteins_merged_left_DF_Add_SI_Information=pd.merge(left=subModule_Constituent_Proteins_merged_left_DF, right=All_SI_subModule_relationships_DF, how='left', left_on='Protein_Constituent_subModule', right_on='SI_Module')\n",
    "subModule_Constituent_Proteins_merged_left_DF_Add_SI_Information=subModule_Constituent_Proteins_merged_left_DF_Add_SI_Information[['Interactor_A', 'Edge_Type', 'Interactor_B', 'Annotation', 'SI']]\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# Split the SI_subModule_relationships_DF by Input/Output\n",
    "\n",
    "# Splitting the Dataframe based on if the SI is acting as an Input or Output\n",
    "# If a SI_subModule relationship is an output, then it should be listed that Interactor_A is the \n",
    "# subModule and Interactor_B is the SI. \n",
    "\n",
    "SIs_subModules_Output=All_SI_subModule_relationships_DF.loc[All_SI_subModule_relationships_DF['Shared_Interactor_subModule_Relationship']=='Output'] # All interactions that go from subModule -> SI\n",
    "SIs_subModules_Input=All_SI_subModule_relationships_DF.loc[All_SI_subModule_relationships_DF['Shared_Interactor_subModule_Relationship']=='Input']\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Generating Interaction type: Output (subModule -> SIs-Kinase (that have all Interactions facing from subModule to SI)    \n",
    "\n",
    "SIs_subModules_Output_renamed=SIs_subModules_Output.rename(columns={'subModule_Name':'Interactor_A','Shared_Interactor':'Interactor_B'}) #rename columns appropriately. \n",
    "#print (SIs_subModules_Output_renamed.head(5))\n",
    "SIs_subModules_Output_renamed['Edge_Type']='Output'  # Add the edge_type, which is output in this case \n",
    "\n",
    "SIs_subModules_Output_renamed=SIs_subModules_Output_renamed[['Interactor_A', 'Edge_Type', 'Interactor_B', 'SI_Module']] # drop unwanted columns\n",
    "SIs_subModules_Output_renamed_merge_left=pd.merge(left=SIs_subModules_Output_renamed,right=Kinases_Phosphatases_yeast_DF, how='left', left_on='Interactor_B', right_on='ORF')  # do a merge, so I get information about which proteins are kinases/phosphatases\n",
    "SIs_subModules_Output_renamed_merge_left=SIs_subModules_Output_renamed_merge_left[['Interactor_A', 'Edge_Type', 'Interactor_B', 'Annotation', 'SI_Module']] #Drop unwanted columns from the above merge \n",
    "\n",
    "SIs_subModules_Output_renamed_merge_left_left_again=pd.merge(left=SIs_subModules_Output_renamed_merge_left, right=All_SI_subModule_relationships_DF, how='left', left_on='SI_Module', right_on='SI_Module') # merge so we get SI information \n",
    "SIs_subModules_Output_renamed_merge_left_left_again=SIs_subModules_Output_renamed_merge_left_left_again[['Interactor_A', 'Edge_Type', 'Interactor_B', 'Annotation', 'SI']] # drop columns we don't want\n",
    "SIs_subModules_Output_renamed_merge_left_left_again_Kinase_SIs_Only=SIs_subModules_Output_renamed_merge_left_left_again.loc[SIs_subModules_Output_renamed_merge_left_left_again['Annotation']=='Kinase']\n",
    "SIs_subModules_Output_renamed_merge_left_left_again_Kinase_SIs_Only=SIs_subModules_Output_renamed_merge_left_left_again_Kinase_SIs_Only.loc[SIs_subModules_Output_renamed_merge_left_left_again_Kinase_SIs_Only['Annotation']=='Kinase'] # only keep annotations that are Kinases!\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Generating Interaction type: Output (subModule -> SIs_Phosphatase (that have all interactions facing from submodule to SI)\n",
    "SIs_subModules_Output_renamed_merge_left_left_Twice=pd.merge(left=SIs_subModules_Output_renamed_merge_left, right=All_SI_subModule_relationships_DF, how='left', left_on='SI_Module', right_on='SI_Module')\n",
    "SIs_subModules_Output_renamed_merge_left_left_Twice=SIs_subModules_Output_renamed_merge_left_left_Twice[['Interactor_A', 'Edge_Type', 'Interactor_B', 'Annotation', 'SI']] # drop columns we don't want\n",
    "SIs_subModules_Output_renamed_merge_left_left_Twice_Phosphatase_SIs_Only = SIs_subModules_Output_renamed_merge_left_left_Twice.loc[SIs_subModules_Output_renamed_merge_left_left_Twice['Annotation']=='Phosphatase']\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Generating Interaction type: Shared_Interaction: Non-Kinase/Phosphatase Shared Interactors and their Module associations. can be directed as follows: subModule -> non-kinase/phosphatase SI  or non-kinase/phosphatase SI -> subModule\n",
    "# Note: These are All Shared Interactors, so we can simply add a SI column to this file. \n",
    "\n",
    "# This section of code is making the subModule -> SI direction \n",
    "SIs_subModules_Output_renamed_merge_left=SIs_subModules_Output_renamed_merge_left[['Interactor_A', 'Edge_Type', 'Interactor_B', 'Annotation', 'SI_Module']] \n",
    "SIs_subModules_Output_renamed_merge_left_non_Kin_Phos=SIs_subModules_Output_renamed_merge_left.loc[SIs_subModules_Output_renamed_merge_left['Annotation']!='Kinase']\n",
    "SIs_subModules_Output_renamed_merge_left_non_Kin_Phos=SIs_subModules_Output_renamed_merge_left_non_Kin_Phos.loc[SIs_subModules_Output_renamed_merge_left_non_Kin_Phos['Annotation']!='Phosphatase'] # Return all proteins with annotations that are NOT kinases!\n",
    "SIs_subModules_Output_renamed_merge_left_non_Kin_Phos['SI']='Yes' # Since all of these proteins are SIs, add a column that indicates they are SIs\n",
    "SIs_subModules_Output_renamed_merge_left_non_Kin_Phos=SIs_subModules_Output_renamed_merge_left_non_Kin_Phos[['Interactor_A', 'Edge_Type', 'Interactor_B', 'Annotation', 'SI']]\n",
    "SIs_subModules_Output_renamed_merge_left_non_Kin_Phos['Edge_Type']='Shared_Interaction'\n",
    "#print (SIs_subModules_Output_renamed_merge_left_non_Kin_Phos)\n",
    "\n",
    "#This section of code is making the SI -> subModule direction \n",
    "\n",
    "#print (len(SIs_subModules_Input))\n",
    "SIs_subModules_Input_merge_left_left_again=pd.merge(left=SIs_subModules_Input,right=Kinases_Phosphatases_yeast_DF, how='left', left_on='Shared_Interactor', right_on='ORF') # Merge so we get Kinae/phosphatase information for SIs\n",
    "SIs_subModules_Input_merge_left_left_again_non_Kin_Phos=SIs_subModules_Input_merge_left_left_again.loc[SIs_subModules_Input_merge_left_left_again['Annotation']!='Kinase']  # Drop kinases\n",
    "SIs_subModules_Input_merge_left_left_again_non_Kin_Phos=SIs_subModules_Input_merge_left_left_again_non_Kin_Phos.loc[SIs_subModules_Input_merge_left_left_again_non_Kin_Phos['Annotation']!='Phosphatase'] # drop phosphatases \n",
    "SIs_subModules_Input_merge_left_left_again_non_Kin_Phos['Edge_Type']='Shared_Interaction' # add the edge type \n",
    "\n",
    "SIs_subModules_Input_merge_left_left_again_non_Kin_Phos=SIs_subModules_Input_merge_left_left_again_non_Kin_Phos[['Shared_Interactor', 'Edge_Type', 'subModule_Name', 'Annotation', 'SI']] # drop unwanted columns\n",
    "SIs_subModules_Input_merge_left_left_again_non_Kin_Phos_renamed=SIs_subModules_Input_merge_left_left_again_non_Kin_Phos.rename(columns={'Shared_Interactor':'Interactor_A', 'subModule_Name':'Interactor_B'}) # rename columns so I can merge all dataframes later on.\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Generating Interaction Type: Motif-Matched  (SI-Kinase -> subModule)\n",
    "\n",
    "SIs_subModules_Input_merge_left_left_again_Kinase_Only=SIs_subModules_Input_merge_left_left_again.loc[SIs_subModules_Input_merge_left_left_again['Annotation']== 'Kinase'] # subset the dataframe so we are only working with Kinases.['a'] = df['a'].apply(lambda x: x.split('-')[0])\n",
    "SIs_subModules_Input_merge_left_left_again_Kinase_Only['SI_V2']=SIs_subModules_Input_merge_left_left_again_Kinase_Only['SI_Module'].apply(lambda x: x.split('_')[0])  # Get term before the first \"_\"\n",
    "SIs_subModules_Input_merge_left_left_again_Kinase_Only['Cluster']=SIs_subModules_Input_merge_left_left_again_Kinase_Only['SI_Module'].apply(lambda x: x.split('_')[1]) # Get term after the first \"_\"\n",
    "SIs_subModules_Input_merge_left_left_again_Kinase_Only['motif']=SIs_subModules_Input_merge_left_left_again_Kinase_Only['SI_Module'].apply(lambda x: x.split('_')[2]) # Get term after the second \"_\"\n",
    "SIs_subModules_Input_merge_left_left_again_Kinase_Only['SI_MODULE']=SIs_subModules_Input_merge_left_left_again_Kinase_Only.SI_V2.map(str) + \"_\" + SIs_subModules_Input_merge_left_left_again_Kinase_Only.Cluster + \"_\" + SIs_subModules_Input_merge_left_left_again_Kinase_Only.motif\n",
    "#DF_to_CSV(SIs_subModules_Input_merge_left_left_again_Kinase_Only, \"Test1.csv\")\n",
    "SIs_subModules_Input_merge_left_left_again_Kinase_Only_merge_left_KL=pd.merge(left=SIs_subModules_Input_merge_left_left_again_Kinase_Only, right=KL_Matching_Kinases_Modules_DF, how='left', left_on='SI_MODULE', right_on='Kinase_Module') # Perform a merge so I get information about matching Kinases to subModule motifs (KL script output).\n",
    "#print (SIs_subModules_Input_merge_left_left_again_Kinase_Only_merge_left_KL)\n",
    "#DF_to_CSV(SIs_subModules_Input_merge_left_left_again_Kinase_Only_merge_left_KL, 'Test2.csv')\n",
    "SIs_subModules_Input_merge_left_left_again_Kinase_Only_merge_left_KL=SIs_subModules_Input_merge_left_left_again_Kinase_Only_merge_left_KL[['Shared_Interactor', 'motif_match', 'subModule_Name', 'Annotation', 'SI', 'FDR']] # drop columns I don't want in the final version\n",
    "SIs_subModules_Input_merge_left_left_again_Kinase_Only_merge_left_KL_renamed=SIs_subModules_Input_merge_left_left_again_Kinase_Only_merge_left_KL.rename(columns={'Shared_Interactor':'Interactor_A', 'motif_match':'Edge_Type', 'subModule_Name':'Interactor_B'}) # rename columns so I can merge all dataframes in the future.\n",
    "SIs_subModules_Input_merge_left_left_again_Kinase_Only_merge_left_KL_renamed['Edge_Type']=SIs_subModules_Input_merge_left_left_again_Kinase_Only_merge_left_KL_renamed['Edge_Type'].map({'yes':'motif_match', 'no':'motif_unmatched', 'unknown_recognition_motif':'unknown_recognition_motif'})\n",
    "#print (SIs_subModules_Input_merge_left_left_again_Kinase_Only_merge_left_KL_renamed)\n",
    "DF_to_CSV(SIs_subModules_Input_merge_left_left_again_Kinase_Only_merge_left_KL_renamed, 'New_Script_file_for_merge.csv')\n",
    "#print (KL_Matching_Kinases_Modules_DF.head(5))\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "# Generating Interaction Type: unknown-recognition motif  (SI-Phosphatase -> subModule)\n",
    "\n",
    "SIs_subModules_Input_merge_left_left_again_Phosphatase_Only_unknown_recogntion_motif=SIs_subModules_Input_merge_left_left_again.loc[SIs_subModules_Input_merge_left_left_again['Annotation']== 'Phosphatase'] # subset the dataframe so we are only working with Phosphatases.\n",
    "SIs_subModules_Input_merge_left_left_again_Phosphatase_Only_unknown_recogntion_motif['Edge_Type']='unknown_recognition_motif'  # add the edge type \n",
    "SIs_subModules_Input_merge_left_left_again_Phosphatase_Only_unknown_recogntion_motif=SIs_subModules_Input_merge_left_left_again_Phosphatase_Only_unknown_recogntion_motif[['Shared_Interactor', 'Edge_Type','subModule_Name', 'Annotation', 'SI']]\n",
    "SIs_subModules_Input_merge_left_left_again_Phosphatase_Only_unknown_recogntion_motif_renamed=SIs_subModules_Input_merge_left_left_again_Phosphatase_Only_unknown_recogntion_motif.rename(columns={'Shared_Interactor':'Interactor_A', 'subModule_Name':'Interactor_B'})\n",
    "#print (SIs_subModules_Input_merge_left_left_again_Phosphatase_Only_unknown_recogntion_motif_renamed)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "#Adding empty columns to a few of the dataframes so that the final append will work (requires that all files have the same column names,otherwise you'll end up with Column_X, Column_Y)\n",
    "\n",
    "subModule_Constituent_Proteins_merged_left_DF_Add_SI_Information['FDR_Score']=\"\"\n",
    "subModule_Constituent_Proteins_merged_left_DF_Add_SI_Information['Match_FDR']=\"\"\n",
    "#print (subModule_Constituent_Proteins_merged_left_DF_Add_SI_Information.head(5))\n",
    "\n",
    "\n",
    "SIs_subModules_Output_renamed_merge_left_left_again_Kinase_SIs_Only['FDR_Score']=\"\"\n",
    "#print (SIs_subModules_Output_renamed_merge_left_left_again_Kinase_SIs_Only.head(10))\n",
    "SIs_subModules_Output_renamed_merge_left_left_again_Kinase_SIs_Only['Match_FDR']=\"\"\n",
    "#print (SIs_subModules_Output_renamed_merge_left_left_again_Kinase_SIs_Only.head(2))\n",
    "\n",
    "SIs_subModules_Output_renamed_merge_left_non_Kin_Phos['FDR_Score']=\"\"\n",
    "SIs_subModules_Output_renamed_merge_left_non_Kin_Phos['Match_FDR']=\"\"\n",
    "#print (SIs_subModules_Output_renamed_merge_left_non_Kin_Phos.head(2))\n",
    "\n",
    "SIs_subModules_Input_merge_left_left_again_non_Kin_Phos_renamed['FDR_Score']=\"\"\n",
    "SIs_subModules_Input_merge_left_left_again_non_Kin_Phos_renamed['Match_FDR']=\"\"\n",
    "#print (SIs_subModules_Input_merge_left_left_again_non_Kin_Phos_renamed.head(4))\n",
    "\n",
    "\n",
    "SIs_subModules_Input_merge_left_left_again_Kinase_Only_merge_left_KL_renamed=SIs_subModules_Input_merge_left_left_again_Kinase_Only_merge_left_KL_renamed.rename(columns={'FDR':'FDR_Score'})\n",
    "SIs_subModules_Input_merge_left_left_again_Kinase_Only_merge_left_KL_renamed['Match_FDR']=\"\"\n",
    "#DF_to_CSV(SIs_subModules_Input_merge_left_left_again_Kinase_Only_merge_left_KL_renamed, 'Test101.csv')\n",
    "#print (SIs_subModules_Input_merge_left_left_again_Kinase_Only_merge_left_KL_renamed.head(4))\n",
    "\n",
    "SIs_subModules_Input_merge_left_left_again_Phosphatase_Only_unknown_recogntion_motif_renamed['FDR_Score']=\"\"\n",
    "SIs_subModules_Input_merge_left_left_again_Phosphatase_Only_unknown_recogntion_motif_renamed['Match_FDR']=\"\"\n",
    "#print (SIs_subModules_Input_merge_left_left_again_Phosphatase_Only_unknown_recogntion_motif_renamed.head(4))\n",
    "\n",
    "SIs_subModules_Output_renamed_merge_left_left_Twice_Phosphatase_SIs_Only['FDR_Score']=\"\"\n",
    "SIs_subModules_Output_renamed_merge_left_left_Twice_Phosphatase_SIs_Only['Match_FDR']=\"\"\n",
    "\n",
    "FinalDF=subModule_Constituent_Proteins_merged_left_DF_Add_SI_Information.append(SIs_subModules_Output_renamed_merge_left_left_again_Kinase_SIs_Only)\n",
    "#DF_to_CSV(FinalDF, \"Test.csv\")\n",
    "FinalDF_2=FinalDF.append(SIs_subModules_Output_renamed_merge_left_non_Kin_Phos)\n",
    "FinalDF_3=FinalDF_2.append(SIs_subModules_Input_merge_left_left_again_non_Kin_Phos_renamed)\n",
    "FinalDF_4=FinalDF_3.append(SIs_subModules_Input_merge_left_left_again_Kinase_Only_merge_left_KL_renamed)\n",
    "FinalDF_5=FinalDF_4.append(SIs_subModules_Input_merge_left_left_again_Phosphatase_Only_unknown_recogntion_motif_renamed)\n",
    "FinalDF_6=FinalDF_5.append(SIs_subModules_Output_renamed_merge_left_left_Twice_Phosphatase_SIs_Only)\n",
    "DF_to_CSV(FinalDF_6, 'FINAL_SIF.csv')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
