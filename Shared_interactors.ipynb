{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Shared Interactors  for Kevin\n",
    "\n",
    "This script identifies proteins enriched for interactions with Submodule constituent proteins, based on known interactions in the background network. We call these proteins 'Shared Interactors'. The background network is a protein\n",
    "interaction network curated in yeast under mostly nutrient replete conditions that contains 4638 proteins and ~ 25,000 interactions, including directed (ex; kinase-substrate), and \n",
    "non-directed. \n",
    "\n",
    "Proteins enriched for interactions with Submodule proteins at a 5% FDR, determined by a hypergeometric test and BH correction, are considered shared interactors.\n",
    "\n",
    "Shared Interactors represent numerous functional classes, including kinases and phosphatases. Kinase and phosphatase shared interactors represent potential Submodule regulators.\n",
    " \n",
    "HyperG function:\n",
    "distrib=hypergeom(N,M,n)\n",
    "distrib.pmf(m)\n",
    "\n",
    "* N - population size (4638 unique proteins in Background network file - phospho_v4_bgnet_siflike_withdirections_Matt_Modified.csv)\n",
    "\n",
    "* M - total number of successes  (# of interactions for a given protein. ie. Protein A has 200 known interactions in the background network).\n",
    "\n",
    "* n - the number of trials (also called sample size) -  ie. (Number of proteins that reside within a submdoule)\n",
    "\n",
    "* m - the number of successes - for example: Protein A, a shared interactor, has 35 interactions with proteins in Submodule B. \n",
    " \n",
    " \n",
    " Final shared interactor file:   __Final_enriched.csv__  , this contains the significant Shared Interactors based on the\n",
    " BH_significance test.\n",
    " \n",
    " A list of all shared interactors can be found:  __Network_Submodule_Nodes_background_Network.csv__\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directory = '/home/mplace/projects/forMatt/Phospho_Network/Kevins_data/Like127and128/'\n",
    "\n",
    "group = '/Like_128_Inc_v127/'\n",
    "\n",
    "Submodule_DF   = pd.read_csv(directory + group + 'Like_128_Inc_v127.csv')   # File that contains Submodule names and their protein constituents\n",
    "BgNet          = pd.read_csv(current_dir + '/SI_Identification_Input_Files/Background_Network.csv')                                                                                   # Background network of protein interactions\n",
    "Num_Prot_Inter = pd.read_csv(current_dir + '/SI_Identification_Input_Files/Number_Interactions_Each_Protein.csv')                                              # Number of protein interactions for each protein in the background network\n",
    "Annotation_DF  = pd.read_csv(current_dir + '/SI_Identification_Input_Files/Annotation.csv')                                                   # Yeast protein annotation file\n",
    " \n",
    "Submodule_List=Submodule_DF['Submodule'].unique().tolist()                                                                                                  # Send the Submodules to a list, but filter out duplicates, which there will be many, since the Submodules will have been found in many proteins.\n",
    "\n",
    "dicOrfs={}\n",
    "for Submodule in Submodule_List:                                                                                                                            # Key (Submodule), Value (Yeast ORFs that are Submodule constituents). Filter ORFs found twice to single occurence (important for enrichment analysis)\n",
    "    dicOrfs[Submodule]=(Submodule_DF.loc[Submodule_DF['Submodule'] == Submodule])['ORF'].unique().tolist()\n",
    "        \n",
    "\n",
    "dicOrfsCounts={}  \n",
    "for k,v in dicOrfs.items():  \n",
    "    if k not in dicOrfsCounts:  \n",
    "        value=len(v)            \n",
    "        dicOrfsCounts[k]=value\n",
    "        \n",
    "df_Submodule_Size=pd.DataFrame(list(dicOrfsCounts.items()),                                                                                                  # convert dict to dataframe.\n",
    "                      columns=['Submodule','n'])\n",
    "\n",
    "def SliceDataframe():\n",
    "    ''' For each Submodule identify all proteins that interact with the Submodule proteins in the backgroudn network '''\n",
    "    lst = []\n",
    "    for key in dicOrfs.keys():                                                                                                                             #Select the key, which is a Submodule, from the dict\n",
    "        CurrentDF=BgNet.copy() \n",
    "        x=CurrentDF[CurrentDF['Protein1'].isin(dicOrfs[key])].rename(columns={'Protein1':'Submodule_Containing_Proteins', 'Protein2':'Possible_Shared_Interactors'})                              #Create a new dataframe that is a slice of the salt background network, and only contains proteins that were passed in \"dicOrfs[key]\". At the same time, rename the columns                                \n",
    "        x['Submodule']=key \n",
    "        lst.append(x)\n",
    "        \n",
    "    return lst\n",
    "\n",
    "Sliced_dataframe_list= SliceDataframe()\n",
    "      \n",
    "def Add_n():    \n",
    "    ''' Function adds 'n', the number of proteins in the Submodule, to each dataframe'''\n",
    "    lst= []\n",
    "    for df in Sliced_dataframe_list:\n",
    "        NewDF=df.merge(df_Submodule_Size)\n",
    "        lst.append(NewDF)\n",
    "        \n",
    "    return lst\n",
    "\n",
    "Sliced_dataframe_list= Add_n()\n",
    "\n",
    "def Identify_Shared_Interactors():\n",
    "    ''' Function identifies proteins that interact with at least 2 protein constituents of each submodule'''\n",
    "    \n",
    "    lst=[] \n",
    "    for df in Sliced_dataframe_list: \n",
    "        NewDF=df.copy()\n",
    "        NewDF2=NewDF[NewDF.duplicated(['Possible_Shared_Interactors'], keep = 'last')| NewDF.duplicated(['Possible_Shared_Interactors'])]                  # Only retain proteins that interact with at least 2 submodule protein constituents\n",
    "        x=NewDF2.sort_values(by='Possible_Shared_Interactors', ascending=True) \n",
    "        lst.append(x)\n",
    "       \n",
    "    return lst\n",
    "\n",
    "Shared_Interactors_lst=Identify_Shared_Interactors()\n",
    "\n",
    "def AppendDFs_that_Contain_AllSharedInteractors_and_their_targets():\n",
    "    ''' Function appends all submodules and their shared interactors together into a single file'''\n",
    "    EmptyDF = pd.DataFrame() \n",
    "    for df in Shared_Interactors_lst:  \n",
    "        df=df.copy() \n",
    "        EmptyDF=EmptyDF.append(df)\n",
    "    return EmptyDF\n",
    "\n",
    "SI_andTargets=AppendDFs_that_Contain_AllSharedInteractors_and_their_targets()\n",
    "\n",
    "SI_andTargets_FINAL=pd.merge(left=SI_andTargets, right=Annotation_DF, how='left',\n",
    "                              left_on='Possible_Shared_Interactors', right_on='systematic_name_dash_removed')                                               # complete a merge so I can get the dashes back in the names, which are not included in the background network\n",
    "del SI_andTargets_FINAL['Possible_Shared_Interactors']                                                                                                      # drop because  lacks the dashes which are needed for the correct naming convention\n",
    "del SI_andTargets_FINAL['systematic_name_dash_removed']                                                                                                     # drop because carried over from the merge\n",
    "del SI_andTargets_FINAL['Directed']\n",
    "\n",
    "SI_andTargets_FINAL.columns = ['Submodule_Containing_Proteins', 'Interaction', 'Submodule', 'n','Possible_Shared_Interactors']                        # rename columns\n",
    "\n",
    "myDF = pd.DataFrame(SI_andTargets_FINAL)\n",
    "# OUTPUT NAME FOR SHARED INTERACTORS\n",
    "filename = 'SI_Identification_SubmoduleS__SIs_and_Targets_FDR.csv'\n",
    "myDF.to_csv( directory +  group + filename, index=False, encoding='utf-8' )              # All interactions between SIs and their submodule constituent proteins. No enrichment at this step.\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' Preparing dataframe for Hypergeometric test'''\n",
    "\n",
    "def Add_N_and_m():\n",
    "    ''' Function adds 'N' and calculates 'm' values, which are inputs for the hypergeometric test, to the datframe'''\n",
    "    lst=[]\n",
    "    for df in Shared_Interactors_lst:\n",
    "        NewDF=df.copy()\n",
    "        NewDF['N'] = 4638          # THIS IS THE LENGTH OF THE DATA FRAME, *******************************                                                                                                                         # of proteins in the background network\n",
    "        NewDF['m'] = NewDF.groupby('Possible_Shared_Interactors')['Possible_Shared_Interactors'].transform('count')\n",
    "        lst.append(NewDF)\n",
    "    \n",
    "    return lst\n",
    "\n",
    "Dataframes_list_with_n_N_m=Add_N_and_m()\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def Drop_dups():\n",
    "    ''' For each dataframe, which contains a single submodule, it's protein constituents, and shared interactors, drop duplicate entries for identified SI proteins\n",
    "    . This leaves a single entry for each shared interactor protein. '''\n",
    "    lst=[]\n",
    "    for df in Dataframes_list_with_n_N_m:\n",
    "        NewDF=df.copy()\n",
    "        Final_DF=NewDF.drop_duplicates('Possible_Shared_Interactors')\n",
    "        Final_DF=Final_DF.rename(columns={'Possible_Shared_Interactors':'Shared_Interactor'})\n",
    "        lst.append(Final_DF)\n",
    "        \n",
    "    return lst\n",
    "\n",
    "Drop_Dups_lst=Drop_dups()\n",
    "\n",
    "\n",
    "def Return_M():\n",
    "    ''' Function identifies 'M' (the total number of interactions for each Shared Interactor protein in the background network) and adds that number\n",
    "    to the dataframe'''\n",
    "    lst=[]\n",
    "    for df in Drop_Dups_lst:\n",
    "        NewDF=df.copy()\n",
    "        NewDF2=df.copy()\n",
    "        NewDF_lst=NewDF['Shared_Interactor'].tolist()                                                                                                            # place all proteins in the 'Shared_Interactor' column in a list \n",
    "        Shared_Interactors=Num_Prot_Inter[Num_Prot_Inter['Protein'].isin(NewDF_lst)].rename(columns={'Protein':'Shared_Interactor', 'Total':'M'})\n",
    "        Shared_Interactor_merge=Shared_Interactors.merge(NewDF2, on='Shared_Interactor')\n",
    "        Shared_Interactor_merge=Shared_Interactor_merge.sort_values(by='Shared_Interactor', ascending=True)\n",
    "        lst.append(Shared_Interactor_merge)\n",
    "        \n",
    "    return lst\n",
    "\n",
    "Return_M_lst=Return_M()\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------\n",
    "def hyper(N,M,n,m): \n",
    "    ''' Function defines the parameters for a hypergeometric test that returns a p-value representing the chances of identifying >= x, where x is the number of successes '''  \n",
    "    frozendist=hypergeom(N,M,n)\n",
    "    ms=np.arange(m, min(n+1, M+1))\n",
    "    rv=0;\n",
    "    for single_m in ms: rv=rv+frozendist.pmf(single_m)\n",
    "    return rv\n",
    "\n",
    "def run_hyper():\n",
    "    ''' Function calls the hypergeometric function above  on each shared interactor for each submodule'''\n",
    "    lst=[]\n",
    "    for df in Return_M_lst:\n",
    "        if not df.empty:\n",
    "            NewDF=df.copy()\n",
    "            NewDF['p-value'] = NewDF.apply(lambda row: hyper(row['N'], row['M'], row['n'], row['m']), axis=1)\n",
    "            lst.append(NewDF)\n",
    "        \n",
    "    return lst \n",
    "\n",
    "run_hyper_lst=run_hyper()\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def AppendDFs():\n",
    "    ''' Append DFs for each submodule and it's SIs together into a single DF'''   \n",
    "    EmptyDF = pd.DataFrame() #\n",
    "    for df in run_hyper_lst: \n",
    "        df=df.copy() \n",
    "        EmptyDF=EmptyDF.append(df)\n",
    "    return EmptyDF\n",
    "\n",
    "Final=AppendDFs()\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "''' Prepping for Benjamini Hochberg procedure. Below code is ranking p-values from 1 to n based on lowest to highest p-value score'''\n",
    "\n",
    "Final=Final.sort_values(by=['p-value'],ascending=[True])                                                                                              # Sort p-values from lowest to highest\n",
    "Final_resetIndex=Final.reset_index()                                                                                                        # Reset the index after the sort\n",
    "Final_resetIndex.index +=1                                                                                                                  # start numbering at 1 for index\n",
    "       \n",
    "NewDF=Final_resetIndex\n",
    "NewDF_Allp_values=Final_resetIndex\n",
    "NewDF=NewDF[['p-value']]                                                                                                                    # select only the p-value column of the dataframe \n",
    "NewDF_dropdups=NewDF.drop_duplicates('p-value')                                                                                             # drop duplicate p-values\n",
    "NewDF_dropdups=NewDF_dropdups.reset_index()                                                                                                 # reset the index\n",
    "NewDF_dropdups.index +=1                                                                                                                    # start numbering at 1 for index\n",
    "NewDF_dropdups['Rank(i)'] = NewDF_dropdups.index                                                                                            # #Add a rank column that will be filled with index values. \n",
    "NewDF_dropdups=NewDF_dropdups.drop('index', 1)                                                                                              # Drop the additional column 'index' that is not sorted.\n",
    "NewDF_merge=NewDF_Allp_values.merge(NewDF_dropdups, on='p-value')                                                                           # create a new dataframe that is a merge of the dataframe with all p-values, and the dataframe with unique p-values and their ranks. \n",
    "NewDF_merge=NewDF_merge.drop('index',1)                                                                                                     # drop the index that was added from the merge. This leaves all p-values ordered from lowest to highest with their ranking.\n",
    "\n",
    "'''Add parameters necessary for completing Benjamini-Hochberg procedure '''\n",
    "\n",
    "NewDF=NewDF_merge\n",
    "NewDF['m_(number_of_tests)']=(len(NewDF))                                                                                                   # Add 'm (number of tests)' column \n",
    "NewDF['Q_(FDR)']=0.05      # THIS IS THE FDR VALUE, USER CAN CHANGE **************************************                                                                                                                 # Add Q (FDR) column. This can be changed manually.\n",
    "NewDF['(i/m)Q']=((NewDF['Rank(i)']/NewDF['m_(number_of_tests)'])*NewDF['Q_(FDR)'])                                                          # add the (i/m)Q column \n",
    "NewDF['BH_significant']=NewDF.apply(lambda x: 1 if x['p-value']<x['(i/m)Q'] else 0, axis=1)                                                 # Identify which proteins are  significant. \n",
    "NewDF=pd.merge(left=NewDF, right=Annotation_DF, how='left', left_on='Shared_Interactor', right_on='systematic_name_dash_removed')           # complete a merge to recover dashed version of YORFs\n",
    "del NewDF['Shared_Interactor'] \n",
    "del NewDF['systematic_name_dash_removed']\n",
    "del NewDF['Directed']\n",
    "NewDF.columns = ['M','Submodule_Containing_Proteins', 'Interaction', 'Submodule', 'n','N','m','p-value','Rank(i)', 'm_(number_of_tests)', 'Q_(FDR)','(i/m)Q','BH_significant', 'Shared_Interactor'] # rename columns\n",
    "\n",
    "myDF = pd.DataFrame(NewDF)\n",
    "filename = 'Network_Submodule_Nodes_background_Network.csv'\n",
    "myDF.to_csv( directory + group + filename, index=False, encoding='utf-8' )       # Write out final file with enriched shared interactors for each submodule\n",
    "\n",
    "\n",
    "# FILTER FOR THE FINAL Shared Interactors.\n",
    "# Open and parse Network_Submodule_Nodes_background_Network.csv \n",
    "# Only keep the identified Shared Interactors about the first zero that appears in the BH_Significant column.\n",
    "with open(directory + group +'Final_enriched.csv', 'w') as outfile, open(directory + group + 'Network_Submodule_Nodes_background_Network.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('M,'):\n",
    "            outfile.write(line)\n",
    "            continue\n",
    "        dat = line.split(',')\n",
    "        if dat[12] == '0':\n",
    "            break\n",
    "        else:\n",
    "            outfile.write(line)\n",
    "\n",
    "f.close()\n",
    "outfile.close()\n",
    "\n",
    "# OUTPUT: SI_Identification_SubmoduleS__SIs_and_Targets_FDR.csv, \n",
    "#         Network_Submodule_Nodes_background_Network.csv\n",
    "#         Final_enriched.csv"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
